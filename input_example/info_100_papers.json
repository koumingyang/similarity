{"researcher_id": [8531, 12650], "researcher_name_in_nsf_list": "Ming  Xu", "researcher_paper_title_in_json_file": "Maintaining Connectivity of MANETs through Multiple Unmanned Aerial Vehicles", "projects_cnt": 2, "year": 2015, "paper_citation": 0, "score_lda": ["0.000982033", "0.110548"], "field": ["Simulation", "Engineering", "Distributed computing", "Computer security"], "researcher_nsf_project_abstract": ["1605202 (Xu)\n\nThis project will advance the science and engineering of Food-Energy-Water (FEW) nexus modeling by developing and applying an integrated systems modeling framework. The modeling framework will enable quantitative characterization of urban FEW nexus, identify areas for efficiency improvement, and evaluate the consequences of policy and technology scenarios. Results from the case studies in Detroit and Beijing will guide policy and technology development for better managing FEW resources for these two testbeds as well as other similar cities in the U.S., in China, and around the world. The collaboration between the U.S. and China on this project provides opportunities for a group of researchers with diverse background to share data, expertise, and experience on modeling urban FEW nexus. The project will also engage stakeholders for seeking inputs to improve the modeling framework and scenarios. Research results will also be provided to stakeholders to support their decision making relevant to FEW nexus. Through the U.S.-China collaboration, unique opportunities will be created for a diverse group of graduate and undergraduate STEM students to conduct cutting-edge research in an international, interdisciplinary environment.\n\nThe research employs the latest knowledge from multiple disciplines for developing an integrated systems modeling framework to understand urban FEW nexus. Dynamic material flow analysis will characterize urban FEW resource stocks and flows. The structure and importance of network components will be examined using network-based metrics and methods from different but related fields (ecological network analysis and complex network analysis). Policy and technology scenarios will be evaluated using these network-based metrics and methods to identify co-benefits and avoid unintended consequences. The resulting urban FEW nexus modeling framework will address multiple dimensions and scales: systems (integrated FEW systems), spatial (key system processes that are both within and outside the city boundary), and temporal (short-term flows and long-term stocks). The modeling framework will also be applied to two distinct testbeds (Detroit and Beijing) for demonstration. While the case study results are specific for the testbeds, the integrated systems modeling framework will be generally applicable for understand the FEW nexus for other urban areas.", "1554349 (Xu)\n\nThis research aims to advance the current practice of developing Life Cycle Inventory (LCI) databases into a faster, less expensive process that still generates reliable LCI data. The research will (1) create a framework for modeling and analyzing LCI networks, (2) develop computational models for estimating missing LCI data, and (3) apply these models to evaluate LCI data quality and predict LCI data for emerging technologies. The education plan will (1) engage a diverse group of LCA practitioners during the course of the project, (2) deliver open source software add-ons for LCA practitioners to easily use the computational models developed in the proposed research, (3) develop an education theory grounded curriculum module incorporating research outcomes for broader dissemination, and (4) train undergraduate and graduate students with diverse background in STEM fields by engaging them in the research program and other education activities. \n\nThis research will develop computational approaches for estimating missing data in Life Cycle Inventory (LCI) databases based solely on limited known data, without relying on time-consuming, expensive empirical data collection. The approach transfers the latest knowledge from network science to LCI database development. An LCI database represents the interdependence of unit processes and environmental interventions. The ensemble of such interdependence characterizes the structure of the underlying technology network (or LCI network). If sufficient enough, observed LCI data, although limited, can be used to extract structural features of the underlying LCI network. Such structural features, in turn, can be used to predict the structure of the unknown area of the LCI network, which is equivalent to estimating the unknown data in the LCI database. This research will first create a framework for modeling and analyzing LCI networks. This framework will then be used to develop and validate a variety of link prediction models to estimate missing data for LCI databases. Finally the validated link prediction models will be used to evaluate LCI data quality and predict LCI data for emerging technologies for testbed databases selected in consultation with stakeholders."], "researcher_paper_abstract_in_json_file": "Recently, Unmanned Aerial Vehicles (UAVs) have emerged as relay platforms to maintain the connectivity of ground mobile ad hoc networks (MANETs). However, when deploying UAVs, existing methods have not consider one situation that there are already some UAVs deployed in the field. In this paper, we study a problem jointing the motion control of existing UAVs and the deployment of new UAVs so that the number of new deployed UAVs to maintain the connectivity of ground MANETs can be minimized. We firstly formulate the problem as a Minimum Steiner Tree problem with Existing Mobile Steiner points under Edge Length Bound constraints (MST-EMSELB) and prove the NP completeness of this problem. Then we propose three Existing UAVs Aware (EUA) approximate algorithms for the MST-EMSELB problem: Deploy-Before-Movement (DBM), Move-Before-Deployment (MBD), and Deploy-Across-Movement (DAM) algorithms. Both DBM and MBD algorithm decouple the joint problem and solve the deployment and movement problem one after another, while DAM algorithm optimizes the deployment and motion control problem crosswise and solves these two problems simultaneously. Simulation results demonstrate that all EUA algorithms have better performance than non-EUA algorithm. The DAM algorithm has better performance in all scenarios than MBD and DBM ones. Compared with DBM algorithm, the DAM algorithm can reduce at most 70&#x25; of the new UAVs number.", "paper_keywords": "", "score_lsi": ["0.119706", "0.0795219"]}
{"researcher_id": [7367], "researcher_name_in_nsf_list": "David  Anderson", "researcher_paper_title_in_json_file": "Adding Realism to Inverse Simulation of Helicopters in Aggressive Maneuvering Flight Using a Receding Horizon Approach", "projects_cnt": 1, "year": 2007, "paper_citation": 0, "score_lda": ["0.547999"], "field": ["Control engineering", "Simulation", "Engineering", "Control theory"], "researcher_nsf_project_abstract": ["Millions of archaeological sites have been recorded across the United States over the past century. However, this vast record of archaeological data currently remains fragmented in a variety of incompatible formats and database structures scattered across largely inaccessible public and private data stores. The DINAA (Digital Index of North American Archaeology) project will obtain, index, and integrate nonsensitive aspects of this record in coordination with state, tribal, and federal personnel from across the country. The effort is led by  David G. Anderson of the University of Tennessee, Eric C. Kansa of Open Context, Dr. Sarah Whitcher Kansa of the Alexandria Archive Institute, and Dr. Joshua Wells of Indiana University South Bend. The resulting databases and maps have a low spatial resolution to protect site integrity, but will otherwise permit, for the first time, the visualization and exploration of human responses to change in the natural and social environment at a continental scale over the entire period of human settlement in North America. By removing access, legal and technical barriers, this project will offer researchers, land managers, and the public truly \"Open Data\" to facilitate interdisciplinary research, powerful new approaches to computational modeling, data intensive instruction, and archaeological resource management at local, regional and national scales. The ready availability of online maps, datasets, and links to an ecosystem of similar products and analysis tools will enhance public awareness, use, and appreciation for scientific research in general and archaeology in particular. \n\nThe DINAA project addresses head-on a major challenge facing archaeological informatics: how to connect currently incompatible and fragmentary legacy information systems together so our community can engage in cutting-edge science. The DINAA team has already processed information on over 500,000 sites in 15 states, and this project will expand this effort to the remaining states in North America. DINAA will greatly improve the scientific research value and land management potential of the US archaeological site, report, collections, and other datasets, by broadly employing shared and open data formats, analyses, and dissemination procedures. Anyone using it can download maps, citation records, and other information from the combined site records (no coordinates or other sensitive data are present online) free of charge, and free of intellectual property restrictions. DINAA does this while maintaining strict security. Site locational data is not published or stored; instead, software allocates them to a 400 square kilometer grid (20km on a side) for online visualization. The demonstration that primary archaeological data can be integrated and used to address fundamental questions of human settlement at vast and varying scales will stimulate similar efforts worldwide, and serve as a catalyst to strengthen professional commitments to digital data collection, management, and publication throughout American archaeology, as well as foster public support for scientific research."], "researcher_paper_abstract_in_json_file": "Abstract A conventional inverse simulation does not accommodate control constraints; therefore for aggressive maneuvering flight conditions, where control inputs are close to the limits, this algorithm becomes ineffective. A modification of the conventional inverse simulation technique which accommodates the onset of physical limits or constraints is proposed in this paper. In this way a process of constraints handling is incorporated into the inverse simulation algorithm. Therefore, the aim of this paper is to show that conventional inverse simulation can be improved in terms of the realism of the results by applying a predictive capability for applications involving maneuvering flight. The paper gives details of the development of the predictive inverse simulation algorithm, the mathematical model used and presents examples of results calculated for a pop-up helicopter maneuver. Notation 1 H p prediction horizon h height of pop-up maneuver s distance covered in pop-up maneuver mt total time to complete pop-up maneuver", "paper_keywords": "", "score_lsi": ["0.175769"]}
{"researcher_id": [3260], "researcher_name_in_nsf_list": "Gregory D Hager", "researcher_paper_title_in_json_file": "Multi-Environment Model Estimation for Motility Analysis of Caernorhabditis elegans", "projects_cnt": 1, "year": 2010, "paper_citation": 50, "score_lda": ["0.983392"], "field": ["Biology", "Simulation", "Bioinformatics"], "researcher_nsf_project_abstract": ["Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts. This project addresses this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.\n\n    The proposed experiential learning framework will build on recent advances in deep neural networks. The key problem is to learn the mappings between raw perceptual and control data via a low-dimensional implicit physics space representing a perception-based physical model of how an object acts in the environment. Three directions will be investigated: 1) the development of experiential physics models for object interaction and fluid flow that have strong predictive capabilities, 2) creating mappings directly from experiential models to control of actions such as pouring or moving an object, 3) the assembly of local experience-based controllers into complex tasks from interactive demonstration. Additionally, the project will develop unique data sets that include physical models, simulations, data components, and learned components that other groups can access and build on to enable comparative research similar to what has emerged in machine perception."], "researcher_paper_abstract_in_json_file": "The nematode Caenorhabditis elegans is a well-known model organism used to investigate fundamental questions in biology. Motility assays of this small roundworm are designed to study the relationships between genes and behavior. Commonly, motility analysis is used to classify nematode movements and characterize them quantitatively. Over the past years, C. elegans' motility has been studied across a wide range of environments, including crawling on substrates, swimming in fluids, and locomoting through microfluidic substrates. However, each environment often requires customized image processing tools relying on heuristic parameter tuning. In the present study, we propose a novel Multi-Environment Model Estimation (MEME) framework for automated image segmentation that is versatile across various environments. The MEME platform is constructed around the concept of Mixture of Gaussian (MOG) models, where statistical models for both the background environment and the nematode appearance are explicitly learned and used to accurately segment a target nematode. Our method is designed to simplify the burden often imposed on users; here, only a single image which includes a nematode in its environment must be provided for model learning. In addition, our platform enables the extraction of nematode \u2018skeletons\u2019 for straightforward motility quantification. We test our algorithm on various locomotive environments and compare performances with an intensity-based thresholding method. Overall, MEME outperforms the threshold-based approach for the overwhelming majority of cases examined. Ultimately, MEME provides researchers with an attractive platform for C. elegans' segmentation and \u2018skeletonizing\u2019 across a wide range of motility assays.", "paper_keywords": ["620 engineering", "000 computer science knowledge systems"], "score_lsi": ["0.687595"]}
{"researcher_id": [11615], "researcher_name_in_nsf_list": "WEI  LIU", "researcher_paper_title_in_json_file": "Multisource Traffic Data Fusion with Entropy Based Method", "projects_cnt": 1, "year": 2009, "paper_citation": 50, "score_lda": ["0.0"], "field": ["Simulation", "Computer Science", "Data mining", "Sensor fusion", "Computer security", "Statistics"], "researcher_nsf_project_abstract": ["QFPS ARE IMPORTANT BECAUSE THEY ARE CORRELATED WITH QUASI-PERIODIC PULSATIONS OF SOLAR FLARES, WHICH ARE MAJOR\\nMANIFESTATIONS OF SOLAR ACTIVITY AND DRIVERS OF SPACE-WEATHER DISTURBANCES. QFPS CAN PROVIDE CRITICAL NEW CLUES TO FLARE ENERGY RELEASE, A LONG"], "researcher_paper_abstract_in_json_file": "It is a crucial part for ATMS to accurately identify and forecast traffic state from real-time traffic data. To improve the identification rate of traffic state, multisource information should be used. The multisource information fusion method is important. Information fusion is divided into three levels, i.e. data level, feature level, and decision level. In traffic congestion identification, many means collected traffic data source can be used, such as induce loop vehicle detector, video detector, GPS floating car and so on. The traffic state can be identified according to each data source. For improving the identification rate, we develop a decision level multisource fusion method. In the method, Bayesian inference is used to obtain the traffic state in probability style according to each data source, and entropy based weighted method is used to fuse the result in decision level to improve the identification rate. The entropy based fusion model and algorithm is introduced and presented in this paper. Field data collected through loop vehicle detector and GPS floating car are analyzed with the proposed method.", "paper_keywords": ["traffic state", "entropy based method", "detectors", "gps floating car", "intelligent transport system", "bayes methods", "road traffic", "intelligent transportation systems", "bayesian inference", "real time traffic", "bayesian methods", "data fusion", "entropy intelligent transportation systems data fusion traffic data", "decision level multisource fusion method", "bayesian inference multisource traffic data fusion entropy based method traffic state multisource information fusion method traffic congestion identification induce loop vehicle detector video detector gps floating car decision level multisource fusion method", "traffic congestion", "roads", "global positioning system", "multisource traffic data fusion", "traffic congestion identification", "mutual information", "field data", "entropy vehicle detection telecommunication traffic detectors bayesian methods traffic control radar detection global positioning system mutual information intelligent transportation systems", "traffic engineering computing", "video detector", "information fusion", "entropy", "induce loop vehicle detector", "multisource information fusion method", "vehicles", "traffic data", "sensor fusion", "traffic engineering computing bayes methods road traffic sensor fusion"], "score_lsi": ["0.215216"]}
{"researcher_id": [12361, 13864], "researcher_name_in_nsf_list": "JIA  ZHANG", "researcher_paper_title_in_json_file": "Application-Aware Dynamic Fine-Grained Resource Provisioning in a Virtualized Cloud Data Center", "projects_cnt": 2, "year": 2017, "paper_citation": 50, "score_lda": ["0.976962", "0.843996"], "field": ["Real-time computing", "Simulation", "Cloud computing", "Dynamic priority scheduling", "Computer Science", "Resource management", "Operating system", "Bismuth", "Distributed computing", "Computational model"], "researcher_nsf_project_abstract": ["DATA-CENTRIC RECOMMENDATION HAS BECOME A CRITICAL ISSUE IN THE MODEM SCIENTIFIC RESEARCH. TAKE HURRICANE SCIENCE AS AN EXAMPLE. IT ENCOMPASSES A WIDE ARRAY OF RESEARCH WORKING TO IMPROVE OBSERVATION METHODS, MODELING AND PREDICTION, DATA ASSIMILATION, AND", "OUR PLAN IS DELIBERATED IN FOUR SYNERGISTIC OBJECTIVES. (1) WE PROPOSE TO STUDY HOW EARTH SCIENTISTS CONDUCT DATA ANALYTICS RESEARCH IN THEIR DAILY WORK, DEVELOP A PROVENANCE\\nMODEL TO RECORD THEIR ACTIVITIES, AND TO DEVELOP A TECHNOLOGY TO AUTOMATICALLY"], "researcher_paper_abstract_in_json_file": "A key factor of win\u2013win cloud economy is how to trade off between the application performance from customers and the profit of cloud providers. Current researches on cloud resource allocation do not sufficiently address the issues of minimizing energy cost and maximizing revenue for various applications running in virtualized cloud data centers (VCDCs). This paper presents a new approach to optimize the profit of VCDC based on the service-level agreements (SLAs) between service providers and customers. A precise model of the external and internal request arrival rates is proposed for virtual machines at different service classes. An analytic probabilistic model is then developed for non-steady VCDC states. In addition, a smart controller is developed for fine-grained resource provisioning and sharing among multiple applications. Furthermore, a novel dynamic hybrid metaheuristic algorithm is developed for the formulated profit maximization problem, based on simulated annealing and particle swarm optimization. The proposed algorithm can guarantee that differentiated service qualities can be provided with higher overall performance and lower energy cost. The advantage of the proposed approach is validated with trace-driven simulations.", "paper_keywords": ["optimization data center dynamic resource provisioning heuristic algorithm", "bismuth", "resource management", "computational modeling", "heuristic algorithms", "optimization", "resource management cloud computing dynamic scheduling heuristic algorithms optimization computational modeling bismuth", "dynamic scheduling", "cloud computing"], "score_lsi": ["0.375053", "0.439143"]}
{"researcher_id": [4898], "researcher_name_in_nsf_list": "Kristin A Persson", "researcher_paper_title_in_json_file": "Att rimma och ramsa i f\u00f6rskolan : F\u00f6rskoll\u00e4rares erfarenheter kring arbete med barns spr\u00e5kutveckling", "projects_cnt": 1, "year": 2010, "paper_citation": 0, "score_lda": ["0.937623"], "field": ["Simulation", "Engineering", "Communication"], "researcher_nsf_project_abstract": ["Traditional empirical and 'one-at-a-time' materials testing is unlikely to meet the innovation needs in chemical and materials research, to enable emerging industries to address challenges in energy, national security, healthcare, and other areas in a timely manner.  Historically, novel materials exploration has been slow and expensive, taking on average 18 years from concept to commercialization.  This project has identified a major scientific challenge - characterization of materials and chemical systems via spectroscopy - that can greatly enhance and expand materials research through accumulation, organization, and automation of both experimental and computational resources and data.  Currently, a large amount of time is invested in the interpretation and understanding of spectroscopic data, since no resource for efficiently accomplishing these tasks is available. This project allows materials researchers and chemists working in the spectroscopic field to access a searchable database of existing parameters and spectra for comparative, automated identification, and to address the full range of data elements -- production, curation, analysis, dissemination and sharing.  The resulting data resource contributes to the cyberinfrastructure of the broader materials, chemistry, and engineering community, and has the potential to catalyze the discovery of new materials and the innovative use of materials and chemical systems in science and industry. \n\nThe goal of the Local Spectroscopy Data Infrastructure (LSDI) project is to establish the first computational local atomic environment spectroscopy database, based on well-benchmarked computational spectra, to enable a publicly available, online resource for rapid material characterization, to accelerate materials development and optimization. Through novel technological advancements involving nanoscale engineering of defects, interfaces and surfaces, it has become increasingly important to determine the local atomic environments in materials. Spectroscopic techniques - including X-Ray Absorption Near Edge Spectroscopy (XANES), Extended X-Ray Absorption Fine Structure (EXAFS), Electron Energy Loss Spectroscopy (EELS), and Nuclear Magnetic Resonance (NMR) - have become essential characterization tools in elucidating atomic-scale chemical structure, electronic properties, and quantum phenomena in materials. There is a growing need for a general-use resource to help make spectral assignments for all researchers, including non- specialists, by capitalizing on recent advances in computational methods to populate an interactive database consisting of solid-state X-ray absorption and NMR spectra and associated parameters. This project includes: (i) creation of robust, benchmarked workflows for first-principles calculation of XAS/NMR spectra; (ii) data generation, curation and storage; (iii) development of automated spectral analysis algorithms; (iv) dissemination through the Materials Project; and (v) dynamic interaction with the community through the new Materials Data Cloud (MDCloud) environment. The data infrastructure developed by this project will allow a researcher who has recorded an experimental spectrum, such as by NMR, XANES or XAFS, of a solid-state material - even one with a disordered or non-crystalline structure - to access through the internet a searchable database of existing parameters and spectra for comparative, automated identification, along with a computational resource for simulating the spectra associated with various structural and chemical hypotheses.  The LSDI contributes to the cyberinfrastructure of the materials, chemistry, and engineering communities, and supports advances in the fundamental understanding of spectroscopic methods, materials and chemical systems. The system catalyzes the discovery of new materials, and supports innovative use of materials and chemical systems in science and industry, consistent with the goals of the Materials Genome Initiative.\n\nThis award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Engineering Directorate (Division of Civil, Mechanical & Manufacturing Innovation) and the NSF Directorate for Mathematical & Physical Sciences (Division of Chemistry and Division of Materials Research)."], "researcher_paper_abstract_in_json_file": "The aim of this paper was to contribute to knowledge about how preschool-teachers work with the development of children\u2019s spoken language in preschool.The study was conducted through interviews wit ...", "paper_keywords": ["pedagogy", "pedagogik"], "score_lsi": ["0.31837"]}
{"researcher_id": [13928], "researcher_name_in_nsf_list": "JULIE A ADAMS", "researcher_paper_title_in_json_file": "Multi-touch interaction for tasking robots", "projects_cnt": 1, "year": 2010, "paper_citation": 50, "score_lda": ["0.766782"], "field": ["Human\u2013robot interaction", "Mobile robot", "Computer vision", "10-foot user interface", "Simulation", "Mobile telephony", "Human\u2013computer interaction", "Computer Science", "Artificial intelligence", "Natural user interface"], "researcher_nsf_project_abstract": ["THE OBJECTIVE OF THE PROPOSED PROJECT WILL BE TO IDENTIFY OPERATOR WORKLOAD METRICS, DEVELOP WORKLOAD DETECTION METHODS, CREATE WORKLOAD PREDICTION MODELS, AND PERFORM A CASE STUDY OF REMOTE DRIVING/PILOTING OF AN UNMANNED VEHICLE.  MORE SPECIFICALLY, THE"], "researcher_paper_abstract_in_json_file": "The objective is to develop a mobile human-robot interface that is optimized for multi-touch input. Our existing interface was designed for mouse and keyboard input and was later adopted for voice and touch interaction. A new multi-touch interface permits multi-touch gestures, for example zooming and panning a map, and robot task specific touch interactions. An initial user evaluation found that the multi-touch interface is preferred and yields superior performance.", "paper_keywords": ["keyboards", "user evaluation", "mice", "legged locomotion", "multi touch interaction", "multi touch interaction human robot interaction", "mobile human robot interface", "mobile robots", "human robot interaction", "tasking robots", "mobile communication", "mobile human robot interface multitouch interaction tasking robots", "keyboards mice displays fingers mobile robots legged locomotion reconnaissance human robot interaction mobile computing shape", "reconnaissance", "nasa", "human robot interface", "mobile robots human robot interaction", "multitouch interaction"], "score_lsi": ["0.414881"]}
{"researcher_id": [2207, 2709], "researcher_name_in_nsf_list": "Natalia A Schmid", "researcher_paper_title_in_json_file": "Development of NOx Predictive Model Using Multiple Adaptive Regression Splines", "projects_cnt": 2, "year": 2007, "paper_citation": 0, "score_lda": ["0.91546", "0.974557"], "field": ["Simulation", "Engineering", "Operations management", "Forensic engineering"], "researcher_nsf_project_abstract": ["This Research Experiences for Teachers (RET) in Engineering and Computer Science Site, entitled Digital Signal Processing in Radio Astronomy (DSPIRA) at West Virginia University (WVU) Lane Department of Computer Sciences and Electrical Engineering, the WVU Center for Gravitational Waves and Cosmology, and the National Radio Astronomy Observatory, (NRAO) in Green Bank, WV, will expose high school STEM teachers from West Virginia school districts, to hands-on research experiences in the engineering method, via involvement in the research, design, development, and prototyping of digital signal processing (DSP) techniques and applications targeted for the next generation of radio telescopes.  Radio Astronomy is undergoing a revolution as major new telescopes come on line. This next generation of telescopes requires exceptionally sophisticated signal processing algorithms running in high throughput, heterogeneous computing environments. Implementation of these algorithms and hardware is pushing the state of the art of current DSP techniques.   Advanced DSP algorithms running in commodity devices are a fundamental part of modern life. The signal processing techniques being developed here are also becoming vital across a wide range of areas, including vision-based navigation, remote sensing, robotics, mechatronics, computerized tomography, biomedical engineering, radar and sonar, and signal processing for security.  The experiences gained by the teachers will give them, and through them their own students, insight into the design, development, and implementation of such devices. DSPIRA addresses a confluence of three needs: an industry need for greater public understanding of a widely used technology; science and industry's need to have cross-curricular problem solvers; and the K-12 world's need to integrate engineering principles into their new science standards.    \n\nOver a three-year period this RET Site will offer an intensive six week summer research program and academic year follow-up to a total of 30 STEM high school teachers who will join research teams led by engineers at WVU and NRAO who will provide the RET teachers the opportunity to make meaningful contributions, with authentic involvement, in these engineering research areas. With the recent advances in open source Software Defined Radio (SDR) tools, teachers will be able to learn core concepts and explore implementation strategies in an extremely accessible software/hardware environment (a laptop and RTL-SDR device running GNU Radio software) and take these back to their institutions. The research experience will also include RET teachers working with project staff to develop classroom projects that involve an entire classroom of students in DSP activities.  in addition to dissemination of the results of RET participants' research projects, through poster sessions and national conferences such as NSTA and ASEE, the PIs will share all educational and research material developed over the period of this project both in state and nationally through the Teach Engineering Digital Library.", "\"Fast Radio Bursts\" (FRBs) are linked to exotic astronomical objects in distant galaxies that produce a sudden and very dramatic burst of radio waves. The burst lasts a fraction of a second. Yet modern radio telescopes like the Green Bank Telescope (GBT) in West Virginia can detect these remarkable events - if they are equipped with sensitive equipment and sophisticated computer programs to analyze the data. A collaboration of scientists and students (both graduate and undergraduate) from the University of West Virginia and Virginia Tech plan to develop these programs for the GBT. Astronomers do not yet know what produces FRBs. They could be the result of colliding stars, or perhaps black holes swallowing neighboring stars. Either way, telescopes like the GBT should lead astronomers much closer to an answer.\n\nThe detection of FRBs, believed to be extra-galactic in origin, and other potentially exotic radio transits, has in recent years been enabled by a revolution in radio instrumentation and computing power. The latter has lead to the development of sophisticated signal processing algorithms. The principal investigator, working with her graduate and undergraduate students and a collaborator at Virginia Tech., aims to develop such algorithms with a view to deploying them on the GBT and detecting radio transients in real time. The techniques will be prototyped on radio receivers already in place and in operation at the GBT; the software will run in parallel with existing data processing systems. The algorithms will enable sensitive, wide-field surveys of pulsars and other radio transients, in additions to the FRBs mentioned above. The algorithms and source code will be made freely available to the astronomical community once commissioned on the GBT. The PI also aims to develop a \"Statistical Signal Processing for Sensor Arrays\" undergraduate course to help make the technology more accessible to a wider audience and to educate the next generation of scientists and engineers."], "researcher_paper_abstract_in_json_file": "Emissions models currently employed by Environmental Protection Agency and California Air Resources Board do not account for the variations in engine operation and their effect on emissions. Most of these models simply relate average emission values to vehicle densities and speeds on a traffic network. This study demonstrates the feasibility of using Engine Control Module (ECM) broadcast parameters such as Engine Speed, Engine Torque, Injection Timing, Fueling Rate, etc. as inputs to predict engine-out exhaust NOx emissions. This paper taps into the in-use emissions measurement capabilities and the vast databases that reside at the National Research Center for Alternative Fuels Engine and Emissions, and combines them with an advanced statistical modeling technique, Multivariate Adaptive Regression Splines (MARS), to predict NOx emissions. The MARS technique is an adaptive piece-wise regression approach that can automatically fit models with terms that represent nonlinear effects and interactions among input variables. In this study, an on-board portable emission measurement tool called the Mobile Emissions Measurement System, developed at West Virginia University was used to record in-use, continuous NOx emissions along with ECM broadcast parameters from 3 heavy-duty diesel-powered vehicles. The vehicles were tested over different routes, which included a mix of urban and highway driving to represent real-world conditions. Data collected from the on-road tests of a vehicle(s) were used to create a predictive model using MARS. Results indicate that these predictive values were highly successful with the standard deviation of error 5%. The new in-use emissions measurement tools provide an excellent opportunity to improve upon the existing emissions inventory models, and serve as a tool for predicting in-use emissions in the engine development process.", "paper_keywords": ["torque", "environmental impacts", "multivariate adaptive regression splines mars", "standard deviation", "pollutants", "environmental protection", "engine efficiency", "california", "engines", "air quality management", "u s environmental protection agency", "engine speed", "california air resources board", "emission control devices", "engine control module ecm"], "score_lsi": ["0.270822", "0.349377"]}
{"researcher_id": [3347], "researcher_name_in_nsf_list": "Dawei  Wang", "researcher_paper_title_in_json_file": "Towards contactless skid resistance measurement", "projects_cnt": 1, "year": 2014, "paper_citation": 0, "score_lda": ["0.632022"], "field": ["Simulation", "Engineering", "Geotechnical engineering", "Forensic engineering"], "researcher_nsf_project_abstract": ["The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project can be found across the semiconductor industry with its initial market in wireless communications.  Due to the extraordinary properties of carbon nanotubes (CNTs), applications include low noise amplifiers (LNAs), mixers, and RF power amplifiers (PAs). Looking forward, carbon nanotube transistors (CFETs) can reshape analog radio frequency electronics, enabling the higher data rates and improved capacity demanded by next generation wireless systems due to their intrinsic linearity and associated low out of band interference . CFETs are highly efficient, dissipating less unwanted power than current state of the art technologies while handling high power levels. This translates into more battery life for mobile devices with lower cooling costs. With more linear RF transistors, many billions of dollars of spending on additional base stations, larger batteries, and radio spectrum can be avoided, at great savings to consumers and industry. More speculatively, the creation of reliable grwoth techniques for CFETs and associated manufacturing processes may offer an excellent sensor platform or better ways to form on chip interconnects. The key problems being investigating of in situ growth of high performance nanotubes are applicable to the fabrication of CNT based devices for many electronic applications.\n\nThis Small Business Innovation Research (SBIR) Phase II project will develop electronic devices for radio frequency applications using carbon nanotubes (CNTs).  CNTs are a one dimensional material with diameters in the nanometer range. CNTs have unique and highly desirable properties ranging from superior mobility to current carrying capability to thermal stability.  Calculations show CNT amplifiers will be inherently linear with noise suppressed to the lowest possible quantum limit. These properties allow for electronic devices that will perform better than existing technologies, such as silicon and gallium arsenide.  Just as importantly, the cost for making these devices will be dramatically lower due to the relatively simple method for material synthesis and device fabrication. This work will enable wafer scale arrays of high density in-situ tubes to be grown on silicon enabling the development of carbon electronic components a manner comparable to silicon devices. This work will enable cost effective wafer scale growth of devices which exploit the groundbreaking linearity that CNTs can deliver."], "researcher_paper_abstract_in_json_file": "Monitoring of skid resistance is an important component of maintaining road networks. Over the past decades a wide range of routine measurement devices has been developed, all of them measuring the friction force between a rubber wheel and the (wetted) road surface. At the same time many efforts have been undertaken on a variety of grounds to predict skid resistance solely from texture measurements. We present a concept of contactless skid resistance measurement which is based on optical texture measurement and consists of two components: measurement of the pavement texture by means of an optical measuring system; and calculation of the skid resistance based on the measured texture by means of a rubber friction model. We describe the basic assumptions underlying the theoretical approach and present the model based on the theory of Persson. Two skid resistance measuring devices are chosen to prove the theoretical approach: one laboratory device called Wehner/Schulze (W/S) machine which corresponds to a blocked-wheel braking test and the ViaFriction device of ViaTech AS which measures the skid resistance under controlled longitudinal slip and corresponds to ABS braking conditions. We describe the measurement devices and experiments conducted. The results are very promising although in the case of the ViaFriction device only a few surfaces could be tested. A close relation between measured and predicted friction coefficients could be found. The 95 % prediction interval is +/- 0.04 and the variance 0.02. Thus, a strong indication could be provided that skid resistance could be measured without contact in the future.", "paper_keywords": ["highway design", "test procedures", "road design", "braking", "skidding resistance", "test method", "friction", "skid resistance", "pavement technology", "pavement performance"], "score_lsi": ["0.576147"]}
{"researcher_id": [3872], "researcher_name_in_nsf_list": "Mark  Yim", "researcher_paper_title_in_json_file": "Special Issue on the Grand Challenges of Robotics", "projects_cnt": 1, "year": 2007, "paper_citation": 0, "score_lda": ["0.972985"], "field": ["Simulation", "Engineering", "Nanotechnology"], "researcher_nsf_project_abstract": ["The objective of this award is to organize the annual Principal Investigators (PI) meeting for the National Robotics Initiative (NRI), which was launched in 2011.  The PI meeting brings together the community of researchers, companies, and program officers who are actively engaged in the NRI to provide cross-project coordination in terms of common intellectual challenges, methods for education and training, best practices in terms of transition of results, and a centralized and lasting repository illustrating the research ideas explored and milestones achieved by the NRI projects.  The meeting will be two days during late fall 2016 in the vicinity of Washington DC. The format will include short presentations by all the attending PIs, a poster session, keynote speeches, and panel discussions.  Invitations to the meeting will include all PIs with active NRI grants, program managers with robotics-related programs, and members of the press."], "researcher_paper_abstract_in_json_file": "Inspired by the hugely successful DARPA Grand Challenge, the workshop organizers of the Robotics Science and Systems 2006 (RSS\u201906) conference decided to use Grand Challenges of Robotics as a theme for the RSS\u201906 workshops. Besides including the normal presentations of recent developments in a select area of robotics, each workshop was charged with defining the grand challenges for their research areas. This involved identifying the main problems that were still to be resolved, discussing which challenges held the most promise for moving the field forward, and selecting representative challenge tasks or demonstrations that could be used to serve as tests for progress being made toward solving these challenges. Grand challenges such as these can serve as concrete targets for which multiple groups can focus their research efforts in order to make tangible, and measurable, progress. The workshops and their organizers at the conference were:", "paper_keywords": "", "score_lsi": ["0.715861"]}
{"researcher_id": [5205], "researcher_name_in_nsf_list": "STEVEN  FEINER", "researcher_paper_title_in_json_file": "Collaboration in Mediated and Augmented Reality (CiMAR) Summary", "projects_cnt": 1, "year": 2015, "paper_citation": 0, "score_lda": ["0.927579"], "field": ["Simulation", "Human\u2013computer interaction", "Computer Science", "Multimedia"], "researcher_nsf_project_abstract": ["COLLABORATIVE AUGMENTED REALITY WITH HANDS-FREE GESTURE CONTROL FOR REMOTE ASTRONAUT TRAINING AND MENTORING\\nAUGMENTED REALITY (AR), WHERE 3D AND 2D IMAGES ARE OVERLAID ON A USER S NATURAL FIELD OF VIEW LIKE A HEADS UP DISPLAY, IS BEING DEVELOPED FOR USE"], "researcher_paper_abstract_in_json_file": "The world is becoming more complex everyday, so problem solving often requires global teams of experts working together. To do this effectively there is a need for collaborative tools, and so a variety of teleconferencing and telepresence technologies have been developed. However, most of them involve some variation of traditional video conferencing, which has limitations, such as not being able to effectively convey spatial cues. This workshop focuses on how Augmented Reality (AR) [1] and Mediated Reality (MR) [2] technology can be used to overcome these limitations and develop radically new types of collaborative experiences. In combination, AR and MR technologies could be used to merge the shared perceived realities of different users, as well as enrich each user's own individual experience in a collaborative task.", "paper_keywords": ["collaboration awareness collaboration in mediated and augmented reality cimar collaborative tools teleconferencing technologies telepresence technologies video conferencing spatial cues ar technologies mr technologies collaborative task assembly tasks mr systems collaborative design tasks shared visual feedback collaborative ar applications studierstube system face to face presentations virtual 3d scientific data wearcom remote users virtual avatars indoor ar users shared annotations crime scene investigation distributed situational awareness cross organisational teams security domain tablet based system touchscreen interface remote maintenance human computer interaction social interaction affective computing task domain analysis collaborative mr ar systems", "groupware augmented reality", "collaboration augmented reality conferences australia security manuals"], "score_lsi": ["0.44316"]}
{"researcher_id": [3085], "researcher_name_in_nsf_list": "Stergios I Roumeliotis", "researcher_paper_title_in_json_file": "Vision-aided inertial navigation", "projects_cnt": 1, "year": 2009, "paper_citation": 0, "score_lda": ["0.463228"], "field": ["Computer vision", "Simulation", "Computer Science", "Machine learning"], "researcher_nsf_project_abstract": ["This project develops technologies to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure for civil and industrial infrastructure such as bridges, power plants, and refineries.  It also develops technologies for online processing including localization, path planning and obstacle avoidance. The project builds a system that employs quadrotors to assist their human co-workers in visual inspections of the outdoor infrastructure to enhance efficiency and effectiveness of such operations.  The research advances the current state of the art in key areas of sensing, estimation, and control necessary for enabling small-size quadrotors to assist humans in visual inspections. In addition to improving the reliability of the nation's infrastructure, the project benefits researchers, developers, educators, and end-users in robotics by developing open-source, modular algorithms for quadrotors. The project offers educational and community outreach activities aligned with local efforts and state-wide initiatives, and seeks to increase diversity and attract underrepresented groups to Science, Technology, Engineering, and Mathematics (STEM) via a partnership with local high schools. \n\nThis research addresses the fundamental challenges stemming from sensing and processing limitations that prevent the use of low-cost, small-size quadrotors in visual-inspection tasks. It focuses on a four-step process, where initially a quadrotor is tele-operated at a safe distance from the structure of interest to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure. These maps are then used, by the inspection engineer, to designate areas of interest. Lastly, the quadrotor employs its onboard sensors to precisely localize with respect to the structure and navigate along the inspection route, while collecting additional data for increasing the accuracy and improving the reliability of future inspections. A key innovation is making information available in multiple forms and levels of abstraction so as to meet the often-conflicting needs of offline (e.g., visualization of inspection areas and planning information-rich paths) and online (e.g., map-based localization and obstacle avoidance) uses. Also critical is an information-driven approach for making maximum use of the limited sensing and processing resources available to the quadrotor. Lastly, a key advantage of the proposed approach is that it provides the foundation for continual improvement in accuracy and efficiency after each inspection flight."], "researcher_paper_abstract_in_json_file": "This document discloses, among other things, a system and method for implementing an algorithm to determine pose, velocity, acceleration or other navigation information using feature tracking data. The algorithm has computational complexity that is linear with the number of features tracked.", "paper_keywords": "", "score_lsi": ["0.443008"]}
{"researcher_id": [5182, 11656], "researcher_name_in_nsf_list": "RALPH D LORENZ", "researcher_paper_title_in_json_file": "Energy Cost of Acquiring and Transmitting Science Data on Deep-Space Missions", "projects_cnt": 2, "year": 2015, "paper_citation": 50, "score_lda": ["0.489748", "0.914149"], "field": ["Simulation", "Engineering", "Electrical engineering", "Remote sensing"], "researcher_nsf_project_abstract": ["VOLCANIC ERUPTIONS ARE OFTEN OBSCURED FROM OPTICAL SPACE OBSERVATION BY CLOUDS AND ASH. THE HEAT SIGNATURE OF ERUPTED MATERIALS HAVE NOT BEEN STUDIED IN THE PAST IN MICROWAVE DATA BECAUSE THE SMALL WARM AREA IS DILUTED BY LARGE INSTRUMENT FOOTPRINT. SMAP", "I PROPOSE TO CONTRIBUTE AS A PARTICIPATING SCIENTIST (PS) IN THE PLANNING AND ANALYSIS OF OBSERVATIONS WITH THE AKATSUKI LIGHTNING AND AIRGLOW CAMERA (LAC) WITH AN EMPHASIS ON STUDYING THE TIME-HISTORY OF LIGHTNING FLASHES, A UNIQUE AND NOVEL WINDOW ON LI"], "researcher_paper_abstract_in_json_file": "An empirical correlation is drawn between the science data returned from deep-space missions and the energy required to operate the spacecraft and its payload. This correlation serves to give a simple early estimate of power requirements for planetary missions. Surprisingly, missions follow a linear dependence of the specific energy (joules per bit) with distance from Earth, which is an emergent law that reflects the engineering effort to partially defeat the inverse square law. Proportionalities of 5\u2009\u2009mJ/bit/astronomical unit for missions with high-gain antennas and 10\u2009\u2009J/bit/astronomical unit for omnidirectional antennas are found. The payload energy costs to acquire the data are examined briefly: except for in situ missions with chemical analyses using relay spacecraft, the acquisition costs are small compared with the spacecraft operation and transmission costs.", "paper_keywords": "", "score_lsi": ["0.443394", "0.501253"]}
{"researcher_id": [1362], "researcher_name_in_nsf_list": "Wei  Cai", "researcher_paper_title_in_json_file": "Effect of the Reaction Field on Molecular Forces and Torques Revealed by an Image-Charge Solvation Model", "projects_cnt": 1, "year": 2013, "paper_citation": 0, "score_lda": ["0.765297"], "field": ["Text mining", "Medical research", "Simulation", "Computer Science", "Nanotechnology", "Physics"], "researcher_nsf_project_abstract": ["New materials with special properties are necessary in the search for new clean energy sources and advanced medical devices. Electromagnetic phenomena play a key role in the design of new materials such as meta-materials and conducting materials. Meta-materials, assembled with blocks of meta-atoms of naturally available components, have provided a wide range of new possibilities to design man-made materials with special properties. Novel devices using meta-materials have been proposed including perfect lens and sub-diffraction-limited imaging for medical applications, light harvest in clear energy solar cells. In addition, understanding the conducting flow of a charged system is essential for studying confined nuclear thermal reactions for the exploration of new clean energy sources.\n\nThe computational simulation of electromagnetic phenomena is challenging, owing to the demand of highly accurate and efficient numerical methods that not only represent the correct physics in the magnetic induction equation but also resolve the multiple scattering and local field enhancements from random objects in meta-materials. To meet these requirements, the PIs will accomplish the following two tasks in this project: (1) to develop a highly efficient volume integral equation method for Maxwell equations for very accurate computation of multiple scatterings of large number of regular or random objects employed in the construction of meta-materials; (2) to devise a high order constrained transport finite element method for the magnetic induction equations in the magneto-hydrodynamics  problem so the global divergence free condition on the magnetic field is preserved. The research findings will be disseminated through journal publications and software tool development."], "researcher_paper_abstract_in_json_file": "We recently developed the Image-Charge Solvation Model (ICSM), which is an explicit/implicit hybrid model to accurately account for long-range electrostatic forces in molecular dynamics simulations (Lin et al., J. Chem. Phys., 131, 154103,2009). The ICSM has a productive spherical volume within the simulation cell for which key physical properties of bulk water are reproduced, such as density, radial distribution function, diffusion constants and dielectric properties. Although the reaction field (RF) is essential, it typically accounts for less than 2% of the total electrostatic force on a water molecule. This observation motivates investigating further the role of the RF within the ICSM. In this report we focus on distributions of forces and torques on wa- ter molecules as a function of distance from the origin and make extensive tests over a range of model parameters where Coulomb forces are decomposed into direct inter- actions from waters modeled explicitly and the RF. Molecular torques due to the RF typically account for 20% of the total torque, revealing why the RF plays an important role in the dielectric properties of simulated water. Moreover, it becomes clear that the buffer layer in the ICSM is essential to mitigate artifacts caused by the discontinuous change in dielectric constants at the explicit/implicit interface.", "paper_keywords": ["biological patents", "biomedical journals", "text mining", "europe pubmed central", "citation search", "citation networks", "research articles", "abstracts", "open access", "life sciences", "clinical guidelines", "full text", "rest apis", "orcids", "europe pmc", "biomedical research", "bioinformatics", "literature search"], "score_lsi": ["0.452977"]}
{"researcher_id": [13439], "researcher_name_in_nsf_list": "Chandra  Krintz", "researcher_paper_title_in_json_file": "Language Support for Highly Resource- Constrained Microcontroller Applications", "projects_cnt": 1, "year": 2010, "paper_citation": 0, "score_lda": ["0.865602"], "field": ["Embedded system", "Real-time computing", "Simulation", "Computer Science"], "researcher_nsf_project_abstract": ["The ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI) is a premier forum for sharing advanced academic and industrial research focused on all areas of programming language research, including the design, implementation, theory, and efficient use of languages. PLDI emphases include innovative and creative approaches to compile-time and runtime technology, novel language designs and features, and results from implementations. The conference will be held in Santa Barbara, CA on June 13-17, 2016. \n\nPLDI seeks to increase student participation in conference and the field. The proposed funding would support the travel of 12 eligible US students to the conference. Recipients would be able to attend the main conference, workshops, and tutorials. A special effort will be made to reach out to women and under-represented minorities. These efforts are anticipated to broaden the participation in the conference and, by extension, the research field."], "researcher_paper_abstract_in_json_file": "Modern embedded systems (i.e. highly resource-constrained, microcontroller-based devices) are increasingly availab le at very low cost and consist of a variety of hardware components. As a result, these systems have become increasingly ubiquitous and are emerging as an important computing platform. Advances in support for easy program development across heterogeneous devices, by a broad developer base (with a range of expertise and backgrounds) for such devices is vital, but unfortunately has not kept pace. Current ly, only very skilled developers are able to develop even simple applications or extant software development frameworks only support a small set of similar devices or a particular application domain. Toward this end, we present a new programming language, called Em, for the development of highly resourceconstrained device applications. Em is an extension to the C language that integrates high-level language and software engineering features that include modularity, encapsulat ion and data hiding, interface separation from implementation, inheritance, support for key design patterns, and reduced syntax verbosity, among others. Em facilitates code reuse, portability across platforms and device components, interchangeability (substitutability) while still achieving t he footprint and code efficiency of C. Em also integrates novel features such as unifying the configuration (build time execution) and target (run time execution) code development. We demonstrate the efficacy of Em using multiple embedded systems building blocks and applications.", "paper_keywords": "", "score_lsi": ["0.380148"]}
{"researcher_id": [2459, 10695], "researcher_name_in_nsf_list": "Xintao  Wu", "researcher_paper_title_in_json_file": "On Burst Detection and Prediction in Retweeting Sequence", "projects_cnt": 2, "year": 2015, "paper_citation": 50, "score_lda": ["0.435052", "0.674034"], "field": ["Simulation", "Data mining", "Computer security"], "researcher_nsf_project_abstract": ["Various business models have been built around the collection and use of customer data to make important decisions like employment, credit, and insurance. There are increasing worries of discrimination as data analytics technologies could be used to unfairly treat individuals based on their demographic information such as gender, age, marital status, race, religion or belief, membership in a national minority, disability, or illness. It is imperative to develop predictive decision models, such that the data that goes into them and the decisions made with their assistance are not subject to discrimination. This EAGER research designs practical techniques to accurately detect and remove discrimination from the datasets used to build decision models. A primary outcome of this research is a unifying framework and a prototype system for discrimination discovery and removal. This system can help individuals from disadvantaged groups determine whether they are fairly treated and help decision makers from organizations ensure their predictive decision models are discrimination free. \n\nExisting discrimination discovery approaches are mainly based on correlation or association and cannot accurately discover the true discrimination. In addition, each of them targets on one or two types of discrimination only. This research categorizes discrimination based on whether discrimination is across the whole system, occurs in one subsystem, or happens to one individual, and whether discrimination is a direct effect or an indirect effect on the decision. This research then develops a unifying causal Bayesian network based framework that takes into consideration the distinctions between discrimination and general causalities and models both direct discrimination and indirect discrimination as causal effects via different paths between protected attributes and the decision. It can accurately capture and measure various types of discrimination at system, group, and individual levels. The research then develops novel discrimination discovery and prevention models and algorithms. The research also builds a testing framework for simulating different types of discrimination and evaluating the approaches based on various metrics, and integrates the discrimination discovery and prevention algorithms into an open source data mining and machine learning software system.", "Online social networks (OSNs) face various forms of fraud and attacks, such as spam, denial of service, Sybil attacks, and viral marketing.  In order to build trustworthy and secure OSNs, it has become critical to develop techniques to analyze and detect OSN fraud and attacks.  Existing OSN security approaches usually target a specific type of OSN fraud or attack and often fall short of detecting more complex attacks such as collusive attacks that involve many fraudulent OSN accounts, or dynamic attacks that encompass multiple attack phases over time.  This research, dubbed oSAFARI (Online SociAl network Fraud and Attack Research and Identification), models, analyzes and characterizes OSN frauds and attacks; designs, develops, and evaluates a new approach to detecting static OSN frauds and attacks; and further enhances the approach to handle dynamic attacks with multiple phases.  The research team plans to develop a new course focused on OSN attacks and defenses, which has the potential to be offered across many institutions.  To increase public security awareness, the team also plans to develop tutorial courses on typical OSN attacks and their defense and offer them at popular public events and in freshman classes.  The research team will broadly disseminate their results, tools, software, and documents to the research community, IT industries, and to OSN companies.\n \nThis project embraces a systematic, comprehensive study of OSN frauds and attacks.  It models OSN threats by viewing an OSN as a graph embedded with attacker nodes and edges, identifies and analyzes specific forms of frauds and attacks, and evaluates state-of-the-art attack analysis and defense approaches.  It develops a spectral-analysis-based framework for OSN fraud and attack detection.  The framework transforms topological information of an OSN graph into patterns formed by spectral coordinates in the spectral space, and introduces the use of the spectral graph perturbation theory to more easily model and capture changes of spectral coordinates for attacker, victim, and regular nodes.  Further, this research develops spectral-analysis-based detection approaches for complex networks where nodes can carry attributes and edges can be negative, weighted, or asymmetric.  Through a novel combination of the network dynamics and the vector autoregressive model, it develops an automatic spectral-analysis-based approach to detecting dynamic attacks while avoiding the high cost and low accuracy of traditional approaches.  It also transforms attack characteristics from high-dimensional spectral spaces into distinctive visual patterns, and develops interactive mechanisms for analysts to incorporate domain knowledge and flexibly handle attacks.  The research team will build a simulation framework to evaluate the detection approaches against different types of OSN attacks, where one can plug in different OSN datasets to evaluate and compare different detection approaches.  Moreover, the research team will build a prototype oSAFARI on top of an OSN, and evaluate how oSAFARI can withstand various attacks in a real setting."], "researcher_paper_abstract_in_json_file": "Message propagation via retweet chain can be regarded as a social contagion process. In this paper, we examine burst patterns in retweet activities. A burst is a large number of retweets of a partic- ular tweet occurring within a certain short time window. The occur- ring of a burst indicates the original tweet receives abnormally high attentions during the burst period. It will be imperative to character- ize burst patterns and develop algorithms to detect and predict bursts. We propose the use of the Cantelli's inequality to identify bursts from retweet sequence data. We conduct a comprehensive empirical analysis of a large microblogging dataset collected from the Sina Weibo and report our observations of burst patterns. Based on our empirical findings, we extract various features from users' profiles, followship topology, and message topics and investigate whether and how accurate we can predict bursts using classifiers based on the extracted features. Our empirical study of the Sina Weibo data shows the feasibility of burst prediction using appropriately extracted features and classic classifiers.", "paper_keywords": "", "score_lsi": ["0.734011", "0.495282"]}
{"researcher_id": [10285, 12112], "researcher_name_in_nsf_list": "WILLIAM  LAU", "researcher_paper_title_in_json_file": "Calibration apparatus for a medical tool", "projects_cnt": 2, "year": 2014, "paper_citation": 0, "score_lda": ["0.407647", "0.200461"], "field": ["Computer vision", "Simulation", "Computer Science", "Computer graphics (images)"], "researcher_nsf_project_abstract": ["THE GOAL OF THIS PROPOSAL IS TO FILL THIS GAP BY BRINGING TOGETHER EXPERTISE IN CLIMATE DIAGNOSTICS, BIOMASS BURNING, AND CLIMATE MODELING, WITH THE OBJECTIVES TO: A) IDENTIFY AND QUANTIFY THE CLIMATE INDICATORS THAT DRIVE BIOMASS BURNING CHARACTERISTICS", "THE OBJECTIVES OF THE PROPOSED RESEARCH ARE TO IMPROVE UNDERSTANDING OF A) THE ORGANIZATION OF TROPICAL CONVECTION, IN TERMS OF PRECIPITATION AND CLOUD SYSTEMS CHARACTERISTICS AND RELATIONSHIPS WITH EXTREME PRECIPITATION EVENTS (EPES) IN THE TROPICS, AND"], "researcher_paper_abstract_in_json_file": "A calibration apparatus is provided for calibrating a medical tool having a tool tracking marker. The medical tool and the calibration apparatus are for use with a medical navigation system. The calibration apparatus comprises a frame, a frame tracking marker attached to the frame, and a reference point formed on the frame. The reference point provides a known spatial reference point relative to the frame tracking marker.", "paper_keywords": "", "score_lsi": ["0.167186", "0.401632"]}
{"researcher_id": [2368], "researcher_name_in_nsf_list": "YANG  LIU", "researcher_paper_title_in_json_file": "Project Application in the Field of Composite Materials in Badminton", "projects_cnt": 1, "year": 2014, "paper_citation": 0, "score_lda": ["0.762377"], "field": ["Structural engineering", "Application software", "Simulation", "Computer Science", "Engineering", "Textile", "Mechanical Engineering"], "researcher_nsf_project_abstract": ["NUMEROUS EPIDEMIOLOGICAL STUDIES LINKS AMBIENT AIR POLLUTION TO EXCESS MORBIDITY AND MORTALITY. HISTORICALLY THESE STUDIES RELIED ON GROUND MONITORING STATIONS, SUCH AS THE U.S. EPA REGULATORY MONITORING NETWORK, TO ESTIMATE POPULATION EXPOSURE. THE GEOGR"], "researcher_paper_abstract_in_json_file": "In this article, through the rapid development of badminton, detailed introduces the evolution process of badminton racket, through the use of the badminton racket composites badminton flying speed, also become more and more wonderful and exciting game.", "paper_keywords": ["badminton", "material", "application"], "score_lsi": ["0.238667"]}
{"researcher_id": [2800, 4830, 10537], "researcher_name_in_nsf_list": "Maurizio  Porfiri", "researcher_paper_title_in_json_file": "Spatial memory training in a citizen science context", "projects_cnt": 3, "year": 2017, "paper_citation": 0, "score_lda": ["0.976251", "0.974489", "0.97805"], "field": ["Psychology", "Cognitive psychology", "Simulation", "Citizen science", "Multimedia", "Communication", "Social psychology", "Intellectual disability"], "researcher_nsf_project_abstract": ["1604355 - Porfiri\n\nRobotic devices have been playing an increasingly central role in physical rehabilitation due to their capability to support an increasing range of therapeutic treatments, minimize therapist time commitment, and record the performance of patients. To further extend rehabilitation treatments beyond clinical settings and enhance rehabilitation progress, considerable effort has been devoted toward robot-mediated telerehabilitation, which allows a physical therapist to remotely monitor and supervise several patients simultaneously. However, the cost of these devices and the repetitive nature of the prescribed exercises have significantly hampered the practicality of robot-mediated telerehabilitation. This project will open new directions for transforming robot-mediated telerehabilitation, through the integration of therapeutic treatments with low-cost haptic devices and interactive online citizen science activities. In the envisioned paradigm, participants will contribute to citizen science by performing activities that are part of their therapeutic regiment, and consequently increase the time spent on otherwise-boring rehabilitation activities. This integration is expected to not only contribute to scientific research but also increase patient self-esteem. \n\nThis research program seeks to advance upper limb robot-mediated telerehabilitation for patients recovering from stroke by empowering them through active science participation. The envisioned system comprises a low-cost haptic joystick interfaced to a PC, which affords online social interactions in a citizen science research project. Patients will contribute to an environmental monitoring citizen science project developed by the research team, by analyzing images acquired by an aquatic mobile robot in a polluted canal in Brooklyn, NY, while interacting with other patients online. By harnessing the motivation and interests of patients in science and the environment, the proposed approach aims to maximize retention and efforts towards rehabilitation. The analysis will be performed using a haptic joystick, which provides a force-feedback to the patient while recording salient rehabilitation performance indices for upper limb rehabilitation. The system will be tested on both healthy subjects and patients undergoing rehabilitation for post-stroke hemiparesis through a series of experimental studies that elucidates the combined effects of force feedback and social interactions on patient performance and satisfaction.", "Citizen science involves the general public in research activities that are conducted in collaboration with professional scientists. Citizens' participation shortens the duration and lowers the costs of certain research activities. A key challenge inhibiting the widespread adoption of citizen science is guaranteeing the reliability of contributions submitted by volunteers. Traditional approaches have relied on redundant distribution of tasks, whereby multiple volunteers are indiscriminately assigned identical tasks. However, most citizen science projects suffer from a scarcity of long term contributors and an abundance of casual, short term volunteers. Drawing inspiration from species across every phylum of life where physical and behavioral heterogeneities are evolutionarily selected, this EArly-concept Grant for Exploratory Research (EAGER) project posits that heterogeneities in citizen scientists will improve the reliability of data gathered. The envisioned paradigm will promote the progress of science, by enabling researchers to quickly gather large quantities of reliable data with minimal changes to existing infrastructure. Outcomes of this project will be mutually beneficial to researchers and society at large: researchers will have more confidence in citizen science and put forward more exciting projects which will contrive to enhance the scientific literacy of the public.\n\nThis research program seeks to demonstrate a novel methodology to cogently distribute tasks among volunteers based on prior performance, affinity to the project, and technical potential. Specifically, the project hypothesizes that data obtained from subsamples of participants that are highly heterogeneous in terms of individual attributes will lead to more reliable data, thereby enabling a significant reduction in the degree of task redundancy and an improvement in data quality. This hypothesis will be tested within Brooklyn Atlantis, an online citizen science project for monitoring the environmental health of the Gowanus Canal - a highly polluted Superfund site. In Brooklyn Atlantis, citizen scientists identify objects of interest in images taken from the surface of the canal through an aquatic robot. A series of studies will be performed to: i) elucidate the relationship between data reliability and individual attributes; ii) quantify the potential of data fusion to enhance quality and accuracy of contributions; and iii) understand the role of group heterogeneity on data reliability. Rigorous statistics and constrained optimization will drive the implementation of an optimal task allocation engine for use in distributed citizen science applications.", "Mathematical models of infectious disease spread are potent tools for the management of dangerous outbreaks. These models can form a basis for planning and implementing vaccination strategies, evaluating the risks and benefits of travel bans, and improving the effectiveness of prophylaxis campaigns. However traditional modeling approaches do not fully capture the national and international mobility characteristic of modern society, where contacts do not remain geographically confined to the area of the initial outbreak, and an infection may jump thousands of miles in a single day. This project will advance fundamental understanding of dynamical systems evolving on reconfigurable networks, in which the subsystems and the network connections change on comparable time-scales. The resulting mathematical framework will enable a new class of predictive models of infectious disease spread. These models will aid in safeguarding uninfected populations and in mitigating impact on afflicted nations, even when, as in the case of Ebola Virus Disease, no therapeutic protocol is available. More broadly, the underlying theoretical advances are expected to transform the analysis, design, and control of dynamical systems on rapidly reconfiguring networks. Complementing the research component of this project is outreach to promote the education of underprivileged students and to serve economically-disadvantaged communities.\n\nThis research program seeks to advance the field of dynamical systems and complex networks toward tractable mathematical models of infectious disease epidemics. Specifically, this project will establish a theoretical framework for the study of the concurrent evolution of the dynamics of infectious diseases and the formation of the network of contacts through which they spread. The framework will be based on the notion of activity-driven networks, which can be effectively utilized to model contact processes that evolve over time-varying networks across a range of time-scales. This modeling paradigm contrasts that of traditional connectivity-driven networks, where links between nodes have a long life span, resulting in the separation between the time-scales of the dynamics of the network connections and the process evolution. The research team will seek to understand the effect of non-ideal containment procedures on the spread of infectious disease through the systematic analysis of global and local network features; devise strategies for community detection in time-varying networks, toward identifying untraced contacts that are critical for disease spreading and of great public concern; and establish model-based optimization strategies to prioritize contact tracing procedures toward improving the effectiveness and outcomes of control interventions."], "researcher_paper_abstract_in_json_file": "Memory deficit is one of the primary effects of intellectual disability, and has a great impact on daily life. Here, we propose a novel spatial memory training system based on a citizen science virtual environment, in which users navigate an aquatic robot in a polluted canal and identify specific objects from images acquired by the robot. A portable low-cost electroencephalography device is utilized to enhance the degree of interactivity and enable real-time estimation of the affective state of the user. We involved a cohort of 60 healthy adult subjects to evaluate users' interest, memory performance, and affective variables as a function of navigation modality (active versus passive) and interface (a traditional computer mouse versus the headset). Despite offering a higher level of difficulty, the headset was preferred over a traditional mouse control by the users, whose spatial memory performance did not vary with the navigation modality or the interface. Active navigation was found to lead to a higher level of engagement, as measured by the headset. These findings suggest the possibility of a new, effective, and entertaining form of intellectual rehabilitation with potential impact on fetal alcohol syndrome.", "paper_keywords": ["spatial memory training", "intellectual disability", "affective state evaluation", "citizen science", "fetal alcohol syndrome"], "score_lsi": ["0.557199", "0.43717", "0.354412"]}
{"researcher_id": [10115, 12076], "researcher_name_in_nsf_list": "Lijun  Liu", "researcher_paper_title_in_json_file": "Research on Computer Network Data Transfer Methods", "projects_cnt": 2, "year": 2014, "paper_citation": 0, "score_lda": ["0.764947", "0.796984"], "field": ["Distribution", "Grid file", "Simulation", "Semantic grid", "Computer Science", "Theoretical computer science", "Data grid", "Distributed computing", "Utility computing", "Grid computing"], "researcher_nsf_project_abstract": ["The physics behind several fundamental aspects of the South American tectonics remains elusive. For example, various geological records suggest that the Andes Mountains started to shorten significantly since at least 40 million years ago. However, paleo-altimetry proxy data reveal that most of the surface elevation of the Andes remained relatively low until as late as 20 or 15 million years ago, a time frame that is inconsistent with the shortening history. Furthermore, the early Cenozoic arc volcanism along the central Andes suddenly shifted more than 500 km inland around 30 million years ago and formed widespread silicic volcanic activity that continues to the present. Consequently, a clear relationship among these tectonic events is lacking. An important reason is that both the mantle structures beneath South America and their dynamic evolution remain poorly understood. In this proposal, the team plans to carry out a multidisciplinary research project to attempt to improve our knowledge about the observational records of surface tectonics, deep mantle seismic properties, and their geodynamic relationship with the temporal evolution of subduction beneath the continent. By collecting more data related to the tectonic history of South America and to the present-day mantle interior, they will build a sophisticated 4-dimentional geodynamic model using data assimilation, in order to quantitatively reproduce the past subduction history. Ultimately, the team hopes to better understand the Cenozoic evolution of the South American continent.  The team plans to achieve this goal through a collaborative research effort that involves seismology (led by PI Beck), geology (led by PI DeCelles), and geodynamics (led by PI Liu). They propose to reproduce the Cenozoic subduction history beneath South America and associated continental deformation using geodynamic models constrained by geophysical and geological observations. This project includes interdisciplinary training of several graduate students at two institutions, and also involves undergraduates at both institutions who will be involved in the research and will all students will participate in a summer field trip to western U.S. Cordillera as an analog for the South American Cordillera.   This multi-disciplinary combination will provide a unique opportunity for the students to understand orogenic systems at plate scale.\n\nMultiple fundamental questions exist about the Cenozoic tectonic history of South America, including the asynchronous crustal shortening and surface uplift/subsidence history of the Andes, and the enigmatic Central Andean flare-up magmatism occurring during the late Cenozoic. None of the earlier proposed geodynamic models for the Cenozoic evolution of South America could simultaneously explain all these tectonic records and it is unclear whether these different physical processes could co-exist and interact in reality. An important reason for the existence of these alternative models is the uncertain subduction and mantle dynamics due to our imperfect knowledge of deep mantle structures beneath South America. Key observational constraints on the geodynamic and tectonic evolution include an improved present-day mantle seismic structure, especially in the lower mantle, a better identified relationship among structural deformation, surface uplift and magmatic history within western South America, and time-dependent geodynamic models that are consistent with these observational records.", "How plate tectonics have shaped the Earth's surface geology (such as mountain building, basin formation, landscape evolution, volcanic activities and earthquakes) remains a fundamental question in geosciences. Key to this question is the uncertain variation in the style and dynamics of subduction, a process when cold oceanic plates recycle into the Earth's warm interior. In this proposal, the PI plans to study the causes and consequences of flat-slab subduction (i.e., down-going plates travel sub-horizontally beneath the lithosphere before sinking into the mantle) that has found to be greatly affecting the evolution of continents. This problem has been traditionally difficult to understand due to the many complexities and unknowns involved. Fortunately, the recent progress in geophysical data acquisition and high performance computing make it possible to tackle this important geodynamic problem by building sophisticated physical models using various techniques of data assimilation. Using the PI's previous experience on constructing both forward and inverse data-oriented models (similar to how weather prediction works), the investigator will explore the subduction history in South America, North America and East Asia, where multiple flat-slab epochs have occurred, resulting in the unique geology surrounding the Pacific Ocean. Results from this project will help to better understand not only basic earth evolution but also formation of natural hazards and resources.  This project also aims at enhancing communication and education of geodynamic research to both researchers and students. By designing an online interactive geodynamics forum, the PI hopes to promote geodynamic research to the broad Earth science community, and to help interested users to learn geodynamic modeling. The PI plans to develop this geodynamics group into a community hub for doing cross-disciplinary and collaborative research. With multiple research-based flipped-classroom activities, the PI also\naims to build a modern education culture, with a key focus on nurturing high quality geoscientists for the new century. Finally, this proposal will support and train two PhD students and several undergraduates.\n\nIt is widely accepted that subduction processes, especially flat-slab subduction, strongly influence the geological evolution of continents and mantle dynamics. However, both the causes and consequences of flat-slab subduction remain elusive. This is largely due to the difficulty to realistically simulate the complex behavior of subduction and its interaction with the overriding continents. In this proposal, the PI plan's to improve our understanding on the physical mechanisms and tectonic consequences of flat-slab subduction by combining advanced data-oriented physical models with geological and geophysical constraints. In practice, he will combine well-established subduction modeling tools (both forward and adjoint methods) with data constraints from multiple disciplines. This comprehensive modeling effort will be applied to three different geographic locations (South America, North America, and East Asia) where flat subduction occurred in the past or is still going on now, and where several different physical mechanisms for slab flattening (subduction of various buoyancy features, fast plate motions, and hydrodynamic suction from the thick overriding plate) could be evaluated. The multidisciplinary nature of this work will make the results highly relevant to many other research fields."], "researcher_paper_abstract_in_json_file": "In order to spread across different locations, sharing of computer resources, and ease of use of idle CPU or storage space Resources, there is the concept of grid and grid computing. Data - intensive scientific and engineering applications ( such as seismic data Numerical Simulation of physics, computational mechanics, weather forecast ) needed in a wide area, quick and safe transmission in distributed computing environments Huge amounts of data. So how in a grid environment efficient, reliable, and secure transfer massive files are in the grid computing A study on the key issue. Design and realization of dynamic task assignment algorithm and Performance experiment of the system.", "paper_keywords": ["distribution", "computer network", "algorithm ga"], "score_lsi": ["0.385248", "0.499048"]}
{"researcher_id": [5713], "researcher_name_in_nsf_list": "Ward  Whitt", "researcher_paper_title_in_json_file": "Fluid Models for Overloaded Multi-Class Many-Server Queueing Systems with FCFS Routing", "projects_cnt": 1, "year": 2007, "paper_citation": 0, "score_lda": ["0.952865"], "field": ["Static routing", "Simulation", "Equal-cost multi-path routing", "Computer Science", "Operations management", "Destination-Sequenced Distance Vector routing", "Layered queueing network", "Backpressure routing", "Distributed computing"], "researcher_nsf_project_abstract": ["Some emergency department (ED) patients experience long delays in being transferred from the ED to a bed within the main hospital, a phenomenon called ED boarding. While ED boarding can be caused by surges of demand in the ED, it also can be caused by operating policies in the hospital wards where the patient needs a bed. To properly understand ED boarding, it is necessary to take a broader view, looking at the rest of the hospital in addition to the ED itself.  The problem of ED boarding and other patient flows in hospitals can be modeled as multi-class networks of queues. Accordingly, this research will develop new ways to analyze complex data-driven queueing network models in healthcare.  This research will draw on the extensive experience with queueing network models to reduce congestion and improve the efficiency of manufacturing, computer and communication systems.  The queueing network models needed in healthcare are more complicated because they require (i) classifying patients and resources, with priorities, and (ii) accounting for time-varying arrival rates and complicated stochastic dependence in the patient flows.  Healthcare presents a new opportunity for fruitful applications of these queueing network models because the models can be fit to healthcare patient flow data, which are rapidly becoming available.  In undertaking this research, the PI remains committed to helping to develop new researchers from traditionally under-represented groups.\n\nThis project will develop new tractable data-driven analytical approximations and simulation methods for time-varying multi-class queueing network models.  New approximation methods will combine the recently developed robust optimization with established approximations for stochastic processes based on heavy-traffic limits and partial characterizations of stochastic dependence, such as indices of dispersion.  A new robust queueing (RQ) formulation is proposed for exposing the performance impact of the time dependence and stochastic dependence in the flows. The new RQ formulation is based on the cumulative rate and variance of each flow for each class, represented as the total input of work as functions of time. The research will investigate if the new RQ optimization is effective and tractable; i.e., if it can indeed expose the impact of the time dependence and stochastic dependence on the performance of the queue.  The impact of priorities also will be studied. Computer simulation will be used to evaluate the approximations and also directly as a performance analysis tool.  This research will investigate new simulation methods for these time-varying models, including a new rare-event simulation method for the time-varying single-server queue. The methods will be tested by experiments with simulation and system data."], "researcher_paper_abstract_in_json_file": "Motivated by models of tenant assignment in public housing, we study approximating deterministic fluid models for overloaded queueing systems having multiple customer classes (classes of tenants) and multiple service pools (housing authorities), each with many servers (housing units). Customer abandonment acts to keep the system stable, yielding a proper steady-state description. Motivated by fairness considerations, we assume that customers are selected for service by newly available servers on a first-come first-served (FCFS) basis from all classes the corresponding service pools are allowed to serve. In this context, it is challenging to determine stationary routing flow rates between customer classes and service pools. Given those routing flow rates, each single fluid queue can be analyzed separately using previously established methods. Our ability to determine the routing flow rates depends on the structure of the network routing graph. We obtain the desired routing flow rates in three cases: when the routing graph is (i) a tree (sparsely connected), (ii) complete bipartite (fully connected), and (iii) an appropriate combination of the previous two cases. Other cases remain unsolved. In the last two solved cases, the routing flow rates are actually not uniquely determined by the fluid model, but become so once we make stochastic assumptions about the queueing models that the fluid model approximates.", "paper_keywords": ["first come first serve", "fluid model", "network routing", "flow rate", "queueing model", "queueing system", "public housing", "fluid queue", "steady state"], "score_lsi": ["0.752044"]}
{"researcher_id": [13515], "researcher_name_in_nsf_list": "Jing  Wang", "researcher_paper_title_in_json_file": "Electrostatic Charged Two-Phase Turbulent Flow Model", "projects_cnt": 1, "year": 2008, "paper_citation": 50, "score_lda": ["0.322049"], "field": ["Classical mechanics", "Simulation", "Chemistry", "Analytical chemistry"], "researcher_nsf_project_abstract": ["Like every novel technology, piezoelectricity has contributed to important scientific achievements to Acoustic Emission (AE) and ultrasonic sensor technology, but there still are technical limitations that need to be resolved. The concept of the technology proposed originated from the feedbacks from professionals expertizing on structural integrity monitoring in Non-destructive Testing (NDT) industry as well as several discussion with former doctoral students dedicated to relevant research areas. The proposed piezoelectric-nanocompostie based acoustic/ultrasonic transducer array technology has great potential to resolve several of the performance limiting issues. In addition, due to the high cost of traditional sensors and the requirements in terms of sensor volumes, an alternative and more affordable transducer technology is urgently needed for design teams in relevant technology companies. Modern acoustic and ultrasonic sensing technologies are able to supply an enormous amount of information of health conditions of aging civil infrastructure to avoid a catastrophic failure, thereby making an important impact on the public safety and economical investments. The technology proposed will expedite a wider employment of acoustic emission or ultrasonic transducers on objects of unique geometries or in special working conditions, while allowing substantial cost reduction and shortening the design-cycle due to superb adaptability of the piezo-composites processes. Therefore, with the new piezo-nanocomposite technology introduced, it is reasonable to anticipate a marketing expansion in different categories of business, such as NDT provider and wearable medical electronics companies. \n\nThe key objective of this program is to develop piezoelectric elastomer or thermoplastic nanocomposites that are amenable to additive manufacturing of flexible or 3D conformal transducer array prototypes for detection of acoustic emission or ultrasonic signals. One of the most popular applications relying on the utilization of acoustic emission is to determine if cracks are growing at the interior of a structure or to monitor its degree of deterioration. However, oftentimes, it is very difficult and costly to achieve an accurate or even accessible measurement readings on tested objects because of some certain special technical requirement, e.g. unique geometry or frequency selection. The proposed technology facilitates  low-cost volume production of acoustic emission or ultrasonic transducer arrays that are well tailored for structural health monitoring of concrete, steel, and composite structures and rotating machinery. In this I-Corps program a commercial assessment of will be conducted on a new piezo-nanocomposite material technology that enables customized design, molding or additive manufacturing (3D printing) and ease of deployment of a new class of flexible and/or 3D conformal acoustic and ultrasonic transducer arrays."], "researcher_paper_abstract_in_json_file": "Two-phase electrostatic charged flows have been applied in electrostatic spray, crop- dusting, fuel spray and so on. Electrostatic charged spray can improve desulfurization effi- ciency, decrease water usage. There exist interactions between non-uniform electric and flow fields, and phase interaction between charged particle and continuous phases, which makes the flow more complex. Based on the Reynolds transport equation, equations for the volume aver- aging and instantaneous state of the electrostatic charged two-phase flow have been obtained in this study, thus the k i \" i kp model is closed.", "paper_keywords": "", "score_lsi": ["0.0643203"]}
{"researcher_id": [7665], "researcher_name_in_nsf_list": "Jie  Shen", "researcher_paper_title_in_json_file": "Simulating the Compositional Distribution Evolution of Drop with the Flux Model", "projects_cnt": 1, "year": 2015, "paper_citation": 0, "score_lda": ["0.731543"], "field": ["Mathematical optimization", "Simulation", "Thermodynamics"], "researcher_nsf_project_abstract": ["Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations. Solutions for many problems of interest exhibit local singular behaviors which contaminate the accuracy of usual spectral/spectral-element methods. Many complex systems exhibiting anomalous diffusion can be better modeled with fractional partial differential equations, which are numerically challenging due to their nonlocal nature. Spectral/spectral-element methods usually lead to dense or block dense and ill-conditioned matrices that are difficult and expensive to solve. The focus of this project is to design, analyze, and implement fast and robust spectral methods for a class of numerically challenging problems. The numerical simulations will enable us to handle challenging problems having stringent accuracy and/or memory requirements with a reasonable cost in CPU and turn-around time, and will contribute to a better understanding of some fundamental issues in materials science and fluid dynamics through fast and accurate numerical simulations. Another important goal of this project is to engage graduate students in learning necessary skills of computational and applied mathematics so that they can pursue a successful career in sciences and engineering.\n\nThe PI will address these issues with the following tasks: (i) develop effective Muntz Galerkin method to deal with problems with singular solutions; (ii) develop efficient and accurate spectral methods for solving a class of fractional differential equations by constructing special basis functions with generalized Jacobi and Laguerre functions, and derive corresponding error estimates; (iii) develop direct structured solvers with optimal computational complexity for dense or block dense linear systems arising from spectral/spectral-element discretization; and (iv) develop efficient spectral algorithms for solving the phase-field model of electro-magnetic couplings in ferroelectric and multiferroic nanostructures. The research will result in fast and stable direct spectral/spectral-element solvers for a class of partial differential equations as well as fast Jacobi/spherical harmonic transforms. This project will also result in a set of computational modules to efficiently and accurately solve the coupled nonlinear system for the phase-field model of electro-magnetic couplings in ferroelectric and multiferroic nanostructures."], "researcher_paper_abstract_in_json_file": "Abstract   The compositional distribution of aerosol-cloud drop particles is an important factor to assess their climatic impact, since it governs their chemical reactivity, cloud condensation nuclei activity, and radiative properties. To investigate the compositional distribution evolution of two-component aerosol-cloud drop, the two-component flux method (TFM) of Bott A.[1] is employed to simulate the coagulation. To illustrate its effectiveness, with this method, we compared analytical and numerical results for the constant kernel with exponential initial condition. The results of simulating the Golovin kernel and the hydrodynamic kernel with exponential initial condition showed the compositional distribution tends to be the fixed pattern without the initial influence.", "paper_keywords": "", "score_lsi": ["0.483608"]}
{"researcher_id": [4579], "researcher_name_in_nsf_list": "Raymond J Mooney", "researcher_paper_title_in_json_file": "On Evaluation and Training-Set Construction for Duplicate Detection", "projects_cnt": 1, "year": 2003, "paper_citation": 105, "score_lda": ["0.48856"], "field": ["Simulation", "Computer Science", "Machine learning", "Data mining", "Active learning"], "researcher_nsf_project_abstract": ["Robots are increasingly capable and are on the threshold of becoming a ubiquitous technology. For robots to be truly useful, people must be able to effectively communicate their needs in everyday human language. Although there is a growing body of research on natural-language processing for human-robot interaction, it typically requires some form of explicit supervision provided by an engineering expert and involves unnatural, laborious training to obtain robustness and coverage. This project involves the development of human-robot dialog systems that learn to communicate with users through natural dialog, learning from repeated normal user interactions to become more robust and capable. The project supports the education of students in the areas of natural-language processing, human-robot interaction, and machine learning, where there is significant demand for educated personnel.  It is integrated with the university's  Freshman Research Initiative, which gets undergraduate students involved in research in their first year.\n\nIn order to develop human-robot dialog systems that learn to improve their communication skills through normal user interactions, the project integrates and adapts learning techniques from three currently disparate technical areas: semantic parsing, spoken dialog management, and perceptual language grounding.  The project adapts and integrates techniques for semantic-parser learning using combinatory categorial grammar (CCG), dialog management using Partially Observable Markov Decision Processes (POMDPs), and multi-modal language grounding using both visual and haptic sensors, in order to develop a dialog system for communicating with robots that comprise the Building Wide Intelligence (BWI) system being developed at the University of Texas at Austin. The research integrates the PI's expertise in semantic parsing and language grounding with the co-PI's expertise in robotics and reinforcement learning, forming a unique interdisciplinary team for developing novel and effective systems for human-robot interaction. The project includes rigorous evaluations using controlled experiments on a range of tasks using both on-line simulations with crowdsourced users, and natural user interaction with a mobile robot platform consisting of a wheeled Segway base and a Kinova robot arm being developed for the BWI system."], "researcher_paper_abstract_in_json_file": "A variety of experimental methodologies have been used to evaluate the accuracy of duplicate-detection systems. We advocate presenting precision-recall curves as the most informative evaluation methodology. We also discuss a number of issues that arise when evaluating and assembling training data for adaptive systems that use machine learning to tune themselves to specific applications. We consider several different application scenarios and experimentally examine the effectiveness of alternative methods of collecting training data under each scenario. We propose two new approaches to collecting training data called static-active learning and weaklylabeled non-duplicates, and present experimental results on their effectiveness.", "paper_keywords": ["duplicate detection", "active learning", "machine learning", "evaluation methodology", "adaptive system", "experimental methodology"], "score_lsi": ["0.551743"]}
{"researcher_id": [13585], "researcher_name_in_nsf_list": "Feng  Li", "researcher_paper_title_in_json_file": "Simulation of Facet Dendritic Shape of Isothermal Alloy in a Forced Flow by Phase Field Method", "projects_cnt": 1, "year": 2011, "paper_citation": 0, "score_lda": ["0.0875128"], "field": ["Materials Science", "Simulation", "Optics", "Engineering drawing"], "researcher_nsf_project_abstract": ["This CISE Research Experiences for Undergraduates (REU) Site award funds the renewal of an outstanding REU site at Indiana University - Purdue University Indianapolis.   The site will recruit undergraduates from across the nation to participate in research topics related to mobile cloud security, which focuses on issues related to the use of mobile devices such as smart phones and the secure use of mobile clouds to access, manage, store, and secure digital assets. Mobile cloud security is an area of current interest that is well-suited to undergraduate research productivity.   The students will use controlled and competitive test environments to experiment with cyber attack and defense techniques needed to secure the mobile devices that are pervasive in our society.  This site should help develop a group of computing professionals who can develop the systems of the future that impact society and enhance our quality of life. The REU experience provides students with the foundations and inspiration to pursue computing careers and research in areas that are rapidly evolving and impacting all of our citizens. This site is co-funded by the Secure and Trustworthy Cyberspace program.\n\nThe is project is led by an outstanding team offering state-of-the art facilities and professional mentors to guide undergraduates in explorations of problems related to mobile cloud security. Students will learn how to use current tools and techniques to solve those problems that have direct impact on people.  The team will use proven strategies to recruit undergraduate students from groups traditionally under-represented in computer science including African American, Hispanic and female students.   The students will participate in research and professional development activities all designed to achieve the goals of retaining and graduating undergraduate students in computer science and engineering, recruiting students from groups traditionally under-represented in computing fields, and increasing recruitment of students into graduate programs."], "researcher_paper_abstract_in_json_file": "Numerical simulation based on a new regularized phase field model was performed to describe the dendritic growth of an isothermal alloy with a strong anisotropy in the presence of a forced flow. These results indicate that a crystal grow into an equiaxial facet dendritic in the absence of a forced flow and into an asymmetrical facet dendritic in the presence of a forced flow. With increasing a flow velocity, the tip steady velocity of upstream dendritic arm increases, that of the downstream arm decreases, and that of the perpendicular arms increases at first, and then decreases, the perpendicular arms gradually grow toward the incoming flow direction. In the certain range of anisotropy parameter, when \u03b3 is larger than 0.14, dendritic tip steady velocities in all direction are expected to reach their own saturation values. In addition, the effect of a compound forced flow on an isothermal facet dendritic is similar to experimental results.", "paper_keywords": ["phase field method", "forced flow", "isothermal alloy", "facet dendritic growth", "strong anisotropy", "steady state tip velocity"], "score_lsi": ["0.123598"]}
{"researcher_id": [7627], "researcher_name_in_nsf_list": "Michael  Brogioli", "researcher_paper_title_in_json_file": "Location agnostic platform for medical condition monitoring and prediction and method of use thereof", "projects_cnt": 1, "year": 2015, "paper_citation": 0, "score_lda": ["0.996216"], "field": ["Simulation", "Medicine", "Biological engineering"], "researcher_nsf_project_abstract": ["Today, over 56 million people in the United States report some degree of disability, a number which will likely grow in coming years as the so-called \"baby boomer\" generation ages.  Yet measuring the outcomes of interventions for persons with disabilities has challenged the disability and rehabilitation research community for decades.  Because of the extreme variability and unique circumstances of each individual, the gold standard of randomized control trials (RCT) becomes infeasible when trying to measure the true effects of using assistive technologies (AT).  This leaves the research community and those who rely on evidence of the efficacy of interventions (e.g., funding agencies and insurance companies) in a quandary.  This is funding to support a Summit (workshop) on New Strategies for Measuring Assistive Technology Outcomes, which will take place as part of and immediately following the 2016 annual RESNA conference to be held July 10-14 at the Hyatt Crystal City in Arlington, Virginia.  RESNA, the Rehabilitation Engineering and Assistive Technology Society of North America, is the one organization with an international focus that is devoted solely to technology for individuals with disabilities.  RESNA's membership consists of individuals and institutions covering a range of disciplines (including researchers, clinicians, suppliers, manufacturers, consumers and educators who work in both non-profit and for-profit settings), all of whom are dedicated to promoting the exchange of ideas and information for the advancement of assistive technology.  More information about RESNA and its annual conference is available online at http://www.resna.org.  The PI plans to leverage the fact that the upcoming RESNA conference will bring together national organizations and professionals with expertise related to assistive technology research and services, to organize a Summit on this critical topic at much less cost than would otherwise be required.  With this in mind, the conference organizers have woven an AT outcomes theme (consisting of a full day course, plenary and platform sessions, workshops, etc.) throughout the conference program, which will provide a backdrop and foundation for a full day of advanced discussions on ways to improve measurement of AT outcomes immediately following the end of the conference.  The consensus building and documentation of new methodologies resulting from the Summit will contribute key steps in transforming evidence based practice and its acceptance in new methodologies for outcomes data collection, analysis, and decision making.\n\nThe full-day invitational Summit at the end of the conference will aggregate current scientific thinking to lay the foundation for a transformative shift in rationale toward a new assessment methodology, to delineate steps that are required to create a new evidence based practice schema, and to begin sketching the key framework of variables required for such a new methodology.  Summit outcomes will include a report that will:\n\n1) Summarize the state of the science of AT outcomes research.\n2) Summarize the state of the science of AT outcomes research methodologies.\n3) Delineate the current issues that restrain the current state of the science.\n4) Generate actionable ideas for new AT outcomes research methods.\n5) Articulate specific AT outcomes approaches for consideration by research and funding agencies.\n\nIt is anticipated that these future methodologies will require broad data collection from practitioners, consumers, and assistive technology distributors throughout their practices, and that these data from varied sources will need to be aggregated into a central database for inquiries and analysis."], "researcher_paper_abstract_in_json_file": "A system and method of real-time monitoring of medical patient information, both within a medical facility, as well as during in home care. The system and method can include collection of substantial amounts of longitudinal data used to make deductions in trends across a single patients care, care across multiple patients within a facility, and quality of care across given practitioners. The system and method can also include providing information and feedback regarding a patient's perceived quality of care within a facility, as normalized to a given patient's prior experiences. The system and method can also include providing interactive feedback stimuli and patient care experiences to the patient, as well as real time care monitoring systems for practitioners. In embodiments, the present invention can be a system for holistic pain monitoring and prediction, a system for prevention of narcotic diversion, or a magnetometer sensor system for respiratory measurement.", "paper_keywords": "", "score_lsi": ["0.482941"]}
{"researcher_id": [8317], "researcher_name_in_nsf_list": "Jun  Yang", "researcher_paper_title_in_json_file": "Chain Stores Location Problem with Bounded Linear Consumption Expansion Function on Paths", "projects_cnt": 1, "year": 2008, "paper_citation": 50, "score_lda": ["0.95602"], "field": ["Mathematical optimization", "Simulation", "Operations management", "Business"], "researcher_nsf_project_abstract": ["Driven by prevailing applications that process extreme volume of data, the quest for large memory capacity has become increasingly strong. Conventional DRAM-based memory is facing severe technology scaling limitations such as process variations, which hinders the growth in memory density at reasonable cost. Such challenges inspire the search for alternative memory technologies such as the emerging non-volatile Resistive RAMs (ReRAM). ReRAM exploits resistance of Metal-Oxide-Metal structure to represent stored information. It has shorter read and write latency, and better write endurance when compared with other non-volatile memories. ReRAM exhibits superior scalability and can be architected to build high-density memories using a crossbar structure, or 3D stacking. Such features make ReRAM a competitive technology as a DRAM replacement to achieve significant large memory capacity for modern data intensive applications. Education objectives will be achieved through broad dissemination of results via publications, research seminars, tutorials, software demonstrations, conference participation, and technology transfer initiatives. Continuous student training will be carried through involving graduate, undergraduate students, especially underrepresented students in this research. \n\nThere are major difficulties in building a large memory using the crossbar ReRAM architecture. The reliability of the memory is challenged by its large sneak leakage, operation disturbance and endurance. This research aims to tackle those challenges by investigating novel cell-array organizations and management techniques to reduce sneak leakage, minimize disturbance, prolong the lifetime and improve the overall reliability of the new memory structure. This research will ensure that future resistive memories can be developed to become reliable at high density, which helps to fuel the continuation of Moore?s Law in memory advancement."], "researcher_paper_abstract_in_json_file": "Many customers purchased service as part of routine pre-planned trips nowadays, instead of making a special-purpose trip to obtain service. With the assumption that the customer flows at most be serviced for one time, the decision objective for service providers is to find the optimal locations for such service facilities to maximize the number of customers \"captured\". This sort of problem is called flow interception problem (FIP). This paper proposes utility of each facility and concerns the FIP with bounded linear consumption on the path which is proportional to total utility of same path. Two FIP models with linear bounded expenditures under competitive and monopolistic cases are formulated. Two heuristic algorithms for two cases are developed and computational experiment is described.", "paper_keywords": ["bounded linear consumption expansion function", "location problem", "service provider", "stochastic processes heuristic algorithms telecommunication traffic technology management information management gaussian distribution traffic control banking relays greedy algorithms", "biological system modeling", "optimal location", "companies", "linear bounded expenditures chain stores location problem bounded linear consumption expansion function service facilities flow interception problem", "distance measurement", "chain stores location problem", "flow interception problem", "computational modeling", "gold", "service industries", "computer experiment", "heuristic algorithms", "service industries facility location", "linear bounded expenditures", "service facilities", "algorithm design and analysis", "heuristic algorithm", "facility location"], "score_lsi": ["0.351589"]}
{"researcher_id": [12329], "researcher_name_in_nsf_list": "David  Hughes", "researcher_paper_title_in_json_file": "Wanted : ATM leader", "projects_cnt": 1, "year": 2006, "paper_citation": 0, "score_lda": ["0.628266"], "field": ["Simulation", "Telecommunications", "Aerospace Engineering", "Engineering", "Operations management", "Air traffic control"], "researcher_nsf_project_abstract": ["One of the most complex examples of symbiosis in nature is the precise manipulation of animal behavior by a microbe. Ophiocordyceps is a fungus that infects zombie ants.   Infection of ant workers by thousands of these fungal cells causes the workers to leave the colony and die attached by their mandibles to plants that overhang the trails of ant colonies. There the fungus uses the dead ant bodies to produce spores that infect other ants. The manipulation is complex and spectacular given the fact that an organism without a brain controls the behavior of one with a brain. In this project the research team will use an integrative approach to ask how fungi change ants from being productive members of their colony to \"fungi in ant's clothing?\". The project will use measurement of gene expression, metabolism and tissue structure to ask how, during the 3 week period of infection, the fungi effectively take control of the ant.  This work will provide many general insights into the nature of parasitism and may have broader societal relevance because these fungi are known to be important sources of small molecules with medical relevance. The work is relevant to the broader mission of NSF to increase scientific literacy as the zombie ant system has been shown to be a very useful tool for communicating the elegance and beauty of natural systems. \n\nHow can a microbe control the central nervous system of an animal? Animals are intimately associated with microbes that span the symbiotic spectrum from mutualism to parasitism. In some cases, microbial parasites of animals have evolved to control animal behavior in ways that enhance parasite transmission. The zombie ants represent one prominent example.  In this system a fungal parasite (Ophiocordyceps unilateralis) has evolved a precise level of behavioral control over the ants it infects. Worker ants infected by O. unilateralis move out from their colonies at precise times of day to highly specific locations on leaves in forests, where they bite into vegetation before dying. This altered behavior provides a platform for the eventual release of spores from a long fungal stalk that grows from the cadaver of the ant. This striking system involves an organism in one kingdom of life (Fungi) that controls the behavior of an organism in another (Animalia).    How does an organism without a brain control the behavior of one with a brain?  The investigators will address this using time series infectionstudies to measure chemical changes in both the parasite and its host.  By using three different but complementary tools (serial block face scanning electron microscopy, metabolomics and transcriptomics) to examine these changes across time, the investigators hope to understand the basis of these behavioral changes."], "researcher_paper_abstract_in_json_file": "Subtitle: More than two dozen nations are involved in designing a new European air traffic system, but who will manage it?", "paper_keywords": ["air traffic control", "europe"], "score_lsi": ["0.676929"]}
{"researcher_id": [9747, 10441], "researcher_name_in_nsf_list": "Michael  Wolf", "researcher_paper_title_in_json_file": "A series of unfortunate events : have you got your mojo?", "projects_cnt": 2, "year": 2011, "paper_citation": 0, "score_lda": ["0.494381", "0.956929"], "field": ["Google Panda", "Simulation", "Geography", "Advertising", "Cartography"], "researcher_nsf_project_abstract": ["A surface is a space which looks locally like the 2-dimensional plane, e.g. the surface of a basketball or a pretzel. Surfaces arise naturally in many scientific fields. A geometric structure is a way of measuring distances and angles on a surface or more complicated object. Studying spaces of geometric structures (or shapes) on a fixed object gives further information about their nature. The classical Teichmuller theory studies a space which parametrizes certain geometric structures (of constant curvature) on a fixed surface. Teichmuller theory has impacted diverse areas in mathematics, including algebraic geometry, complex analysis, low-dimensional topology, and dynamics, as well as theoretical physics through its connections with string theory. A metric on Teichmuller space is a way of measuring the distance, or difference, between two such geometric structures. The PIs plan to study metrics on a generalization of this theory called Higher Teichmuller Theory. Higher Teichmuller spaces may be viewed as deformation spaces of geometric structures on higher-dimensional spaces. It shares some of the nice properties of the classical theory and has become a very active field of research. The PIs will mentor graduate students who will be engaged in aspects of the project. They will also run a program which helps science and engineering students from low-resource high schools transition to college studies. \n\nHigher Teichmuller theory studies spaces of \"geometric\" representations of a hyperbolic group into a semi-simple Lie group. The main goal is to develop a theory which shares the richness, beauty and versatility of classical Teichmuller theory. The Higher theory has exploded in popularity because of the interactions it fosters between the subjects of geometric topology, real and complex differential geometry, Lie theory, algebraic geometry, string theory, and dynamics. Bridgeman, Canary, Labourie and Sambarino used thermodynamic formalism to construct a pressure metric on many higher Teichmuller spaces which is motivated by Thurston's definition of the Weil-Petersson metric on Teichmuller space (and its reformulations by Bonahon and McMullen). In the special case of the Hitchin component, the pressure metric is a mapping class group invariant, analytic Riemannian metric whose restriction to the Fuchsian locus is a multiple of the Weil-Petersson metric. Wolf developed an analogous approach to the Weil-Petersson metric, and has results on the isometry group and curvature of the Weil-Petersson metric, degeneration of hyperbolic structures, and on harmonic maps (Hitchin equations) approaches to Teichmuller theory. Wentworth has worked on the pressure metric, Weil-Petersson geometry, Higgs bundles and harmonic maps. The PIs together propose to study the isometry group, curvature and metric completion of both the pressure metric and variants on Hitchin components and quasifuchsian spaces, aiming to understand the pressure metric on general higher Teichmuller spaces.", "This significant project will fund thirteen additional participants to the successful Rice Emerging Scholars Program (RESP) for STEM majors at Rice University. The program is designed to meet the needs of talented students whose preparation leaves them at risk of attrition. The program is comprehensive in its attack on the barriers to persistence with activities that focus on academic preparation for college and navigational skills in college. It provides students with a challenging and immersive bridge experience focusing on the most difficult topics a student will face within their first semesters, followed by mentoring and at least two years of intensive advising.\n                            \nAll thirteen participants will have high potential, substantial financial need, and the vast majority will be from under-represented groups. This funding will greatly increase the chance that these students will major in STEM disciplines and then later graduate into leadership in the sciences and engineering. Spillover effects to the broader URM and first-generation collegian population include the creation of a visible supportive community of engaged young STEM scholars. The investigators will study the effectiveness of this non-remedial comprehensive program and its elements in overcoming obstructions to persistence by comparing RESP students to a non-RESP control group. The study will examine academic outcomes such as course grades, STEM attrition, and study skills and attitudinal outcomes such as STEM self-efficacy and career interest. The research planned comprises qualitative interviews about the college experience, quantitative longitudinal assessment of academic achievement, and quantitative longitudinal assessment of STEM attitudes and study skills using survey methodology. Research results will be disseminated broadly to the academic community in STEM education and learning sciences. The results will be scalable and exportable to inform the development of interventions at other universities in which underprepared high potential students attempt the difficult transition from high school to college."], "researcher_paper_abstract_in_json_file": "These images, all photographed from Google Street View, were taken by placing a camera on a tripod in front of a computer screen in Paris. Google Street View is a technology that displays images taken by a fleet of specially adapted cars, providing online panoramic views of different places around the globe. Since its launch in May 2007, it has expanded from just a few cities in the US to cover a range of locations worldwide. The technology has raised privacy issues, although Google maintains that photos are taken from public property, that features can be blurred on-screen and that users can flag inappropriate or sensitive imagery for Google to remove.", "paper_keywords": ["sabinet", "saepubs", "reference"], "score_lsi": ["0.203046", "0.159439"]}
{"researcher_id": [5510], "researcher_name_in_nsf_list": "RYAN  RUSSELL", "researcher_paper_title_in_json_file": "Chapter 14 \u2013 Operating Systems Overview", "projects_cnt": 1, "year": 2004, "paper_citation": 0, "score_lda": ["0.11464"], "field": ["Embedded operating system", "Real-time computing", "Simulation", "System requirements", "Computer Science", "Computer Engineering"], "researcher_nsf_project_abstract": ["SCIENTIFIC AND ENGINEERING INTEREST HAS RECENTLY SIGNIFICANTLY SHIFTED BEYOND THE LARGE PLANETS AND MOONS, WHICH GRAVITATIONALLY DOMINATE THEIR RESPECTIVE LOCALITIES, TO THE SMALLER, IRREGULARLY SHAPED, COMETS AND ASTEROIDS ROAMING THE SOLAR SYSTEM AROUND"], "researcher_paper_abstract_in_json_file": "Publisher Summary#R##N#Understanding the operating software of a computer or electronic device is much more difficult than it seems at first glance. Yet, it is also very rewarding. This chapter provides a basic introduction to the concepts of operating systems (OSs) as well as to a few specific operating OSs that should be useful in hacking adventures. In electronic devices and computer systems, operating systems are a key function that provides a layer of abstraction between the user program and the actual hardware, and a good understanding of the systems makes future programming projects much more successful. With the resources of an OS at the fingertips, one can manage hardware devices, computing resources, and software processes like a pro. Physically, a computer contains processors, memory, disks, and input/output devices such as keyboards, monitors, and drives. The OS is the software that controls all these components. The OS's job is to take care of hardware access information and to monitor resource allocation. A computer might be running two programs, such as a Web browser and an e-mail program, at the same time. However, both of them cannot use the physical network interface simultaneously. The OS assigns physical resources to each application so that the application designers do not have to worry about such things.", "paper_keywords": "", "score_lsi": ["0.362733"]}
{"researcher_id": [1658], "researcher_name_in_nsf_list": "Manish  Parashar", "researcher_paper_title_in_json_file": "A computational model to support in-network data analysis in federated ecosystems", "projects_cnt": 1, "year": 2017, "paper_citation": 0, "score_lda": ["0.855273"], "field": ["Parallel computing", "Simulation", "Building automation", "Operating system", "Database", "Distributed computing", "Software-defined networking", "Computer security"], "researcher_nsf_project_abstract": ["This project develops a virtual data collaboratory that can be accessed by researchers, educators, and entrepreneurs across institutional and geographic boundaries, fostering community engagement and accelerating interdisciplinary research.  A federated data system is created, using existing components and building upon existing cyberinfrastructure and resources in New Jersey and Pennsylvania.  Seven universities are directly involved (the three Rutgers University campuses, Pennsylvania State University, the University of Pennsylvania, the University of Pittsburgh, Drexel University, Temple University, and the City University of New York); indirectly, other regional schools served by the New Jersey and Pennsylvania high-speed networks also participate.  The system has applicability to a several science and engineering domains, such as protein-DNA interaction and smart cities, and is likely to be extensible to other domains.  The cyberinfrastructure is to be integrated into both graduate and undergraduate programs across several institutions.   \n\nThe end product is a fully-developed system for collaborative use by the research and education community.   A data management and sharing system is constructed, based largely on commercial off-the-shelf technology.  The storage system is based on the Hadoop Distributed File System (HDFS), a Java-based file system providing scalable and reliable data storage, designed to span large clusters of commodity servers.  The Fedora and VIVO object-based storage systems are used, enabling linked data approaches.  The system will be integrated with existing research data repositories, such as the Ocean Observatories Initiative and Protein Data Bank repositories.  Regional high-performance computing and network infrastructure is leveraged, including New Jersey's Regional Education and Research Network (NJEdge), Pennsylvania's Keystone Initiative for Network Based Education and Research (KINBER), the Extreme Science and Engineering Discovery Environment (XSEDE) computing capabilities, Open Science Grid, and other NSF Campus Cyberinfrastructure investments.  The project also develops a custom site federation and data services layer; the data services layer provides services for data linking, search, and sharing; coupling to computation, analytics, and visualization; mechanisms to attach unique Digital Object Identifiers (DOIs), archive data, and broadly publish to internal and wider audiences; and manage the long-term data lifecycle, ensuring immutable and authentic data and reproducible research."], "researcher_paper_abstract_in_json_file": "Software-defined networks (SDNs) have proven to be an efficacious tool for undertaking complex data analysis and manipulation within data intensive applications. SDN technology allows us to separate the data path from the control path, enabling in-network processing capabilities to be supported as data is migrated across the network. We propose to leverage software-defined networking (SDN) to gain control over the data transport service with the purpose of dynamically establishing data routes such that we can opportunistically exploit the latent computational capabilities located along the network path. This strategy allows us to minimize waiting times at the destination data center and to cope with spikes in demand for computational capability. We validate our approach using a smart building application in a multi-cloud infrastructure. Results show how the in-transit processing strategy increases the computational capabilities of the infrastructure and influences the percentage of job completion without significantly impacting costs and overheads.", "paper_keywords": ["in transit", "smart buildings", "cometcloud", "cloud federation", "software defined networks"], "score_lsi": ["0.777774"]}
{"researcher_id": [8039], "researcher_name_in_nsf_list": "Lei  Chen", "researcher_paper_title_in_json_file": "Simulation on Displacement Characteristics of Variable Displacement of Double-Action Vane Pump", "projects_cnt": 1, "year": 2010, "paper_citation": 0, "score_lda": ["0.572926"], "field": ["Control engineering", "Simulation", "Variable displacement pump", "Systems modeling", "Engineering", "Hydraulic pump", "Control theory"], "researcher_nsf_project_abstract": ["Rechargeable lithium ion batteries help to enable sustainable energy systems by storing electricity generated by intermittent renewable resources such as wind and solar energy, or by powering zero-emission electric vehicles charged by electricity from renewable resources.  The two key performance measures of lithium ion batteries are capacity and recharge rate, which determine how much energy a battery can store and how long it takes to fully recharge. One approach to significantly improve capacity is to replace conventional graphite anodes with alloy-type anode materials that include the elements silicon (Si), germanium (Ge), and tin (Sn).  However, these alloy materials swell up after charging, which promotes mechanical failure. This project will address this issue by adding the element selenium (Se) to alloy-type anodes made from micrometer sized particles.  The resulting Se-doped microparticles may be able to reduce swelling of the anode.  Advanced imaging and computational studies will gain a fundamental scientific understanding of these processes, with the long-term goal of developing commercially affordable, high-performance anode materials for better batteries. The research will be a collaborative effort between researchers at three universities - Indiana University, Mississippi State University, and the University of Texas at Austin.  Furthermore, the educational activities associated with this project will be coordinated between these three institutions, and will include integration of the research into undergraduate and graduate course lectures, involvement of undergraduate students and K-12 teachers in research, and outreach to pre-college students through development of short, energy-related animated videos.\n\nThe overall goal of the research is to develop a fundamental understanding of the electrochemical, material phase, and morphological dynamics of Se-doped Ge and Sn microparticles during lithiation and de-lithiation reactions with lithium ion battery alloy-type anodes.  The research plan has two objectives. The first objective is to investigate the dynamics of Se-doped materials during lithiation and de-lithiation, focusing on in situ measurement of phase and morphology change via in situ X-ray powder diffraction (XRD), transmission electron microscopy (TEM) and transmission X-ray microscopy (TXM).   Concurrently, the composition of the Se-containing inactive phase will be identified and its ionic conductivity will be determined. Furthermore, the effect of the active/inactive mixed phases on cycling performance for both Ge- and Sn-based electrodes will be studied.  The second objective is to develop correlations between lithium ion battery cell performance and changes in Se-Ge and Se-Sn electrode microstructure through the afore-mentioned experiments and theoretical modeling.  A phase field model that integrates the processes of electrochemical reaction, species diffusion, interfacial effects, as well as large elastoplastic deformation will be developed to simulate the concurrent evolution of phases, morphologies and stress within a Ge-Se or Sn-Se particle during lithiation and de-lithiation.  Since it is likely that future high-capacity electrode materials will have large volume changes, the outcomes from the research may enable development of these new battery systems."], "researcher_paper_abstract_in_json_file": "On of the practical difficulties of high speed automotive hydraulic power steering is that the output exceeds the actual demands of the system, i.e., there is a substantial power loss. This paper discusses the configuration and the action principle of a new variable displacement of double-action vane pump, which consists of floating blocks. The pump belongs to an automotive hydraulic power steering system, and prosperous utilization is expected. In the meantime the mathematical and simulation model for hydraulic power steering of automobile were established and the Matlab Simulink simulation model was presented. Different parameters of pump are selected in simulating programming. The simulating results are analyzed and compared.", "paper_keywords": ["double action vane pump", "simulation", "floating block", "modeling"], "score_lsi": ["0.29183"]}
{"researcher_id": [11118], "researcher_name_in_nsf_list": "XI  ZHANG", "researcher_paper_title_in_json_file": "Vehicle power managment : modeling, control and optimization", "projects_cnt": 1, "year": 2011, "paper_citation": 0, "score_lda": ["0.0"], "field": ["Control engineering", "Simulation", "Engineering", "Automotive engineering"], "researcher_nsf_project_abstract": ["WE PROPOSE TO INVESTIGATE THE LARGE-SCALE DYNAMICS AND TRACER TRANSPORT IN THE MIDDLE ATMOSPHERE OF JUPITER, SATURN, URANUS AND NEPTUNE USING A COMPREHENSIVE HIERARCHY OF STATE-OF-ART THREE-DIMENSIONAL CIRCULATION MODELS (GCMS) THAT INCLUDE REPRESENTATION"], "researcher_paper_abstract_in_json_file": "Vehicle Power Management - Basic Concepts.- Modeling of Vehicle Propulsion Systems.- Application of Optimal Control to Vehicle Power Management.- Intelligent System Approaches for Vehicle Power Management.- Wavelet Technology in Vehicle Power Management.- Hardware-In-The-Loop for Vehicle Power Management.- Future Trends in Vehicle Power Management.", "paper_keywords": "", "score_lsi": ["0.25873"]}
{"researcher_id": [13338], "researcher_name_in_nsf_list": "Warren B Powell", "researcher_paper_title_in_json_file": "From Single Commodity to Multiattribute Models for Locomotive Optimization: A Comparison of Optimal Integer Programming and Approximate Dynamic Programming", "projects_cnt": 1, "year": 2016, "paper_citation": 50, "score_lda": ["0.184934"], "field": ["Mathematical optimization", "Simulation", "Integer programming", "Reactive programming", "Computer Science", "Engineering", "Operations management", "Dynamic programming", "Mathematics", "Inductive programming", "Algorithm"], "researcher_nsf_project_abstract": ["Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.\n\nComputational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.\n\nCompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. \n\nAdvancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."], "researcher_paper_abstract_in_json_file": "We present a general optimization framework for locomotive models that captures different levels of detail, ranging from single and multicommodity flow models that can be solved using commercial integer programming solvers, to a much more detailed multiattribute model that we solve using approximate dynamic programming (ADP). Both models have been successfully implemented at Norfolk Southern for different planning applications. We use these models, presented using a common notational framework, to demonstrate the scope of different modeling and algorithmic strategies, all of which add value to the locomotive planning problem. We demonstrate how ADP can be used for both deterministic and stochastic models that capture locomotives and trains at a very high level of detail.", "paper_keywords": ["dynamic programming", "locomotive planning", "locomotive operations", "norfolk southern railway company", "integer programming", "programming mathematics", "approximate dynamic programming"], "score_lsi": ["0.420187"]}
{"researcher_id": [5058], "researcher_name_in_nsf_list": "L. Jean  Camp", "researcher_paper_title_in_json_file": "Measuring efficacy of a classroom training week for a cybersecurity training exercise", "projects_cnt": 1, "year": 2016, "paper_citation": 0, "score_lda": ["0.987956"], "field": ["Simulation", "Engineering", "Knowledge management", "Software Engineering"], "researcher_nsf_project_abstract": ["More and more objects used in daily life have Internet connectivity, creating an \"Internet of Things\" (IoT). Computer security and privacy for an IoT ecosystem are fundamentally important because security breaches can cause real and significant harm to people, their homes, and their community. These security issues also are very challenging not only because of the properties of IoT devices themselves but also because the users are diverse, vary in their technical knowledge and access to technical support, and include vulnerable populations such as children and those using in-home care technologies.  Moreover, additional risks emerge when users combine technologies in unexpected ways.\n \nMeeting the challenges of IoT security and privacy requires a large, interdisciplinary effort. An effective approach to IoT security and privacy is holistic, integrating human-computer interaction, network security, cryptography, and pervasive computing. Enforcing cryptographic requirements requires not only building systems that can function on low-capacity IoT devices, but also using threat models that incorporate human requirements. Translating security and privacy requirements and preferences requires understanding what people want, presenting the technologies in a manner people can understand, and knowing what is technologically realistic. This requires behavioral and organizational research, with discussions involving public and private sector stakeholders.   The project is developing a foundation for IoT security and privacy that is intuitive, natural to the human experience, provides the necessary technical guarantees, and facilitates adoption by the larger IoT community of users and manufacturers."], "researcher_paper_abstract_in_json_file": "In this paper we describe the execution and evaluation of a classroom training week for a Red Team/Blue Team military cybersecurity exercise. We give a brief description of the exercise itself, the planning, execution, and the post-event survey results for the classroom training. The design and evaluation of the training is the primary contribution of our research. Evaluation of pre-exercise cybersecurity training for military cybersecurity exercises is not a widely explored area. Large-scale cybersecurity training events such as the one described here are becoming increasingly popular within the military, across governmental organizations, and within the private sector. While there are some significant differences in the scale and execution of a military cybersecurity exercise in comparison to academic events, the lessons-learned from this event can inform military and governmental training exercises, and offer useful insights for any academic organization developing intensive short-term hands-on experiences.", "paper_keywords": ["cyber", "exercise", "security of data computer based training military computing", "training", "classroom", "academic organization classroom training week cybersecurity training exercise red team blue team military cybersecurity exercise large scale cybersecurity training event governmental training exercise", "training computer security planning government games", "cybersecurity", "security", "classroom cyber security cybersecurity training exercise education"], "score_lsi": ["0.633235"]}
{"researcher_id": [1229], "researcher_name_in_nsf_list": "Jie  Wang", "researcher_paper_title_in_json_file": "Experiment investigation on the influence of low pressure on ceiling temperature profile in aircraft cargo compartment fires", "projects_cnt": 1, "year": 2015, "paper_citation": 50, "score_lda": ["0.418642"], "field": ["Meteorology", "Simulation", "Engineering", "Forensic engineering"], "researcher_nsf_project_abstract": ["Research in big data involves analyzing growing data sets with huge numbers of samples, very high-dimensional feature vectors, and complex and diverse structures. The ever-growing volume and complexity of these data sets make many traditional techniques inadequate to extract knowledge from them. An emerging area, known as sparse learning, has achieved great success in learning from big data by identifying a small set of explanatory features and/or samples. Typical examples include selecting features that are most indicative of users? preferences for recommendation systems, identifying brain regions that are predictive of neurological disorders based on imaging data, and extracting semantic information from raw images for object recognition. However, training sparse learning models can be computationally prohibitive due to the sparsity-inducing regularization, which is non-smooth and can be highly complex when incorporating complex structures. This project aims at developing algorithms and tools to significantly accelerate the training process of sparse learning models for big data applications. The key idea is to efficiently identify redundant features and/or samples, which can be removed from the training phase without losing useful information of interests. Success in these unique techniques is expected to dramatically scaling up sparse learning for big data by orders of magnitude in terms of both time and space. The PIs plan to integrate the big data reduction tools developed in this project into their education and outreach activities, including development of new courses and integration of project components into existing courses. The PIs will make special efforts to recruit female and underrepresented students to this project.\n\nThe major technical innovations of this project include the following components: (1) the PIs will develop efficient feature reduction methods for the generic scenario where the structures of both input and output can be represented by directed acyclic graphs; the proposed formulations include many existing approaches as special cases; (2) the PIs will develop efficient methods to reduce the numbers of features and samples simultaneously under a unified formulation, which can also incorporate various structures; (3) the PIs will develop efficient methods to discard irrelevant data subspaces to accelerate the process of uncovering low-rank structures commonly seen in big data. All the proposed data reduction methods are exact, i.e., the models learned on the reduced data sets are identical to the ones learned on the full data sets. This project heavily relies on optimization theory, especially on sensitivity analysis and convex geometry. The outcome of this project includes a unified approach to accelerate sparse learning and provide a systematic framework for developing efficient and exact data reduction methods. The systematic study and in-depth exploration of redundant data identification is expected to deepen the understanding of sparse learning techniques and dramatically enhance their applications in big data analytics."], "researcher_paper_abstract_in_json_file": "Abstract   The objective of the present study is to evaluate the low pressure effects on ceiling temperature profile in aircraft cargo compartment fires, which will affect the activation of fire detectors. A Series of fire tests were carried out in a full scale simulated aircraft cargo compartment at four atmospheric pressures (100\u00a0kPa, 90\u00a0kPa, 80\u00a0kPa and 70\u00a0kPa) corresponding to the pressure within an actual aircraft cargo compartment from the sea level to the cruising altitude (about 10,000\u00a0m). Results show that the maximum ceiling temperature increases and the ceiling temperature decays faster as ambient pressure reduces. The air entrainment ratio  C   \u03b1   is proposed in the correlation to predict the maximum ceiling temperature based on previous plume theory, considering the low pressure effect and entrainment coefficient. Meanwhile, modified by the air entrainment ratio  C   \u03b1  , the previous classic correlations established by Alpert, Heskestad and Delichatsios for the ceiling temperature decay profile are further extended to low pressure conditions. The results based on Heskestad and Delichatsios method are more accurate than that of Alpert method in the experiments. All these findings would provide theoretical basis for the design of fire detection system in the aircraft cargo compartment.", "paper_keywords": "", "score_lsi": ["0.310362"]}
{"researcher_id": [9064], "researcher_name_in_nsf_list": "Wei  Wang", "researcher_paper_title_in_json_file": "The Area Traffic Control System Evaluation Method Based on Travel Time Reliability", "projects_cnt": 1, "year": 2010, "paper_citation": 0, "score_lda": ["0.593262"], "field": ["Simulation", "Floating car data", "Geography", "Vehicle Information and Communication System", "Civil Engineering", "Traffic congestion reconstruction with Kerner's three-phase theory", "Transport engineering", "Traffic optimization"], "researcher_nsf_project_abstract": ["The proper function and health of an organism rests on the correct expression of it's genes: in the first step in expression, RNA molecules are produced from the genes in a number of possible forms. Accurately determining how much RNA is produced and the structure of that RNA are the goals of this research. Many experiments do high throughput sequencing of RNA to show how much gene expression is taking place, what parts of the genomic DNA are making the RNA, and how DNA regions combine to make functional RNA. There are many steps required to process RNA and get sequence data, leading to a lot of noise in the data. Errors also occur when trying to compare the RNA sequence to a genome sequence that has gaps in it or that was not correctly assembled. The effect of the noise and errors is that calculating how much of each type of RNA is present is not very accurate, which can give misleading results. The aim of this research is to develop methods that overcome the technical problems so that good quantitation and better understanding of biological processes are possible. The new algorithms will be incorporated into software packages available for use by interested members of the scientific community, so that the benefits of the improvements will be widely shared. In addition, better analysis of RNA sequencing experiments is expected to have a positive impact on many scientific disciplines, from basic cell biology to development of clinical tests.  \n\nHigh-throughput sequencing of RNA has proven itself as an invaluable tool for gene discovery and the annotation of new isoforms for both coding and non-coding genes. However, it is still falls short on its ultimate promise of providing quantitative and comparative measures of transcript abundance. This gap is due to a series of technical factors. Among them are biases introduced by employing an inexact reference genome as the standard for associating sequence data to transcripts, noise due to misalignments causes by paralogous sequence such as pseudogenes, biases introduced by unannotated transcripts, sense/antisense transcript interference, and origin bias due to aligning diploid data to a haploid model. The objective of the project is to develop methods that either overcome or side-step all of these factors in an effort to deliver on the promise of RNA sequencing for quantitative analysis. Our research plan includes developing computational models and efficient algorithms for simultaneous rebalancing reads between genes and pseudogenes and genes within gene families, robust alignment-free methods for estimating transcript abundances and allele-specific expression patterns, and de novo approach for isoform and novel transcript discovery using DNAseq and RNAseq from a single sample. The proposed computational tools will be integrated into software packages under common application framework adopted by the broad scientific community. The results of the project can be found at http://www.cs.ucla.edu/~weiwang/NSF1565137.html"], "researcher_paper_abstract_in_json_file": "Travel time reliability is one of the parameters that evaluating the quality of traffic service in urban road network. The paper suggests a new method of area traffic control evaluation based on the travel time reliability. First, the concept of travel time reliability is defined and the travel time reliability model is built. Subsequently, the paper emphasizes on establishing the area traffic control evaluation model, which is based on the travel time reliability. The demonstration and utilization of evaluating the area traffic control system illustrates that the new method proposed in this paper is feasible and efficient. Key Word: area traffic control, threshold, travel time reliability, evaluation", "paper_keywords": ["travel time", "traffic control", "highway traffic control systems", "urban areas", "traffic models", "travel time reliability"], "score_lsi": ["0.292067"]}
{"researcher_id": [3102], "researcher_name_in_nsf_list": "Blake  Hannaford", "researcher_paper_title_in_json_file": "Overcoming barriers to wider adoption of mobile telerobotic surgery: engineering, clinical and business challenges", "projects_cnt": 1, "year": 2008, "paper_citation": 50, "score_lda": ["0.985604"], "field": ["Simulation", "Engineering", "Operations management", "Biological engineering"], "researcher_nsf_project_abstract": ["Telemanipulation systems consist of a human interacting with a mechanical device on the master side to operate a robot at the remote side. They provide natural opportunities for research in intelligent human/robot collaboration, but existing commercial systems, used in areas such as telesurgery, are not intelligent and therefore only replicate the actions of the human operator. These systems are also proprietary, expensive, and not available for modification by researchers. The goal of this NRI project is to provide an open-source software infrastructure that is designed to work with a broad range of hardware and simulated devices to enable a larger community to pursue research and education in intelligent telemanipulation at a lower cost.\n\nThe increasing pace of robotics research can be attributed, at least in part, to the increasing availability of software infrastructure, such as Robot Operating System (ROS), and open hardware platforms. This NRI project focuses on providing a software infrastructure for research in intelligent telemanipulation, leveraging infrastructure developed for the Raven II robot and the da Vinci Research Kit (dVRK) and continuing to extend it to other systems, including simulated robots. The three main tasks are to: (1) engage the community to guide development, (2) develop and implement a common API for the diverse hardware platforms, and (3) provide a set of high-level, platform-independent software modules. The goal is to support research towards semi-autonomous telerobotic systems that can more effectively combine the knowledge, reasoning, and decision-making capabilities of a human with the sensing and manipulation capabilities of a robot."], "researcher_paper_abstract_in_json_file": "A portable robotic telesurgery network could remove the geographic disparity of surgical care and provide expert surgical support for first responders to traumatic injury. This is particularly relevant to battlefield medicine where surgical intervention is currently not available to the most perilous fighting circumstances. Similar utility applies to the peacetime healthcare mission. The authors identify the potential advantage to healthcare from a mobile robotic telesurgery system and specify barriers to the employability and acceptance of such a system. This presentation will describe a collaborative effort to design and develop one or more portable robotic systems for telesurgery and develop those systems through successful animal trials. Recent advances in engineering, computer science and clinical technologies have enabled prototypes of portable robotic surgical platforms. Specific challenges remain before a working platform is suitable for animal trials, such as the inclusion of image-guidance and automated tasks   Other barriers to the development of mobile robotic surgical platform will be described. These include technical challenges of refinement of robotic surgical platforms, reduction of weight, cube, complexity and cost, and expansion of applications of technology to several procedures. Clinical challenges involve the protection of patient rights and safety, selection of surgical procedures appropriate for the system, the application of surgical skill to evaluate hardware and the application of surgical lore to software programs. Finally, business challenges include resolution of intellectual property considerations, legal liability aspects of telesurgery, patient safety and HIPPA, reimbursement and insurance issues, FDA approval of the final product and development of a commercialization plan.", "paper_keywords": ["image guidance", "intellectual property", "design and development", "mobile robot", "robotic telesurgery", "access to surgical care", "surgery", "first responder", "patient safety", "surgical procedure"], "score_lsi": ["0.700321"]}
{"researcher_id": [11615], "researcher_name_in_nsf_list": "WEI  LIU", "researcher_paper_title_in_json_file": "A Dynamic Solution to Mobile E-Commerce on Trust Status Prediction", "projects_cnt": 1, "year": 2011, "paper_citation": 50, "score_lda": ["0.0226508"], "field": ["Simulation", "Computer Science", "Data science", "Data mining"], "researcher_nsf_project_abstract": ["QFPS ARE IMPORTANT BECAUSE THEY ARE CORRELATED WITH QUASI-PERIODIC PULSATIONS OF SOLAR FLARES, WHICH ARE MAJOR\\nMANIFESTATIONS OF SOLAR ACTIVITY AND DRIVERS OF SPACE-WEATHER DISTURBANCES. QFPS CAN PROVIDE CRITICAL NEW CLUES TO FLARE ENERGY RELEASE, A LONG"], "researcher_paper_abstract_in_json_file": "a trust evaluation model for mobile e-commerce trading platform was proposed, which is based on improved Grey-prediction-model. The model utilizes the theory of Gray prediction, constructs the GM (1,1) model according to the changes of previous trust values of entities interaction and predict the next trust value. Experiments show that the model is a better solution to mobile e-commerce on trust status prediction.", "paper_keywords": ["prediction method", "electronic commerce", "prediction error", "telecommunication industry", "e commerce", "trust management", "prediction algorithms", "fuzzy set theory", "data model", "mobile communication predictive models business data models prediction algorithms educational institutions mathematical model", "prediction theory", "mobile radio", "business", "mobile communication", "mathematical model", "grey systems", "predictive models", "telecommunication industry electronic commerce grey systems mobile radio prediction theory", "prediction model", "gm 1 1 model trust status prediction trust evaluation model mobile e commerce trading platform grey prediction model", "mobile terminal", "data models", "value prediction"], "score_lsi": ["0.102364"]}
{"researcher_id": [3122, 3621], "researcher_name_in_nsf_list": "Farinaz  Koushanfar", "researcher_paper_title_in_json_file": "TESTING SECURITY OF MAPPING FUNCTIONS", "projects_cnt": 2, "year": 2009, "paper_citation": 0, "score_lda": ["0.803344", "0.476951"], "field": ["Reliability engineering", "Simulation", "Engineering", "Computer security"], "researcher_nsf_project_abstract": ["Computing on sensitive data is a standing challenge central to several modern-world applications. Secure Function Evaluation (SFE) allows mistrusting parties to jointly compute an arbitrary function on their private inputs without revealing anything but the result. The GC@Scale project focuses on novel scalable methods for addressing SFE, which directly translate to stronger cryptography and security for myriads of tasks with sensitive data. The applications are wide reaching and include privacy-preserving processing of medical, genome, and biometric data, as well as personal, government, and industrial cloud computing. The project includes an ambitious educational program that targets both undergraduate/ graduate students, and also addresses issues related to outreach.\n\nThe concept of SFE using Garbled Circuits (GC) was introduced by Yao. Despite a decade of research in GC implementation and several key progresses, scalability of the available methods has been hampered by the circuit representation as a directed acyclic graph, and software-level local logic optimizations. GC@Scale leverages PI's recent work, which has changed the SFE landscape by viewing GC generation as an atypical sequential logic synthesis. The project plans to advance the understanding and enable expanded exploration of SFE methodologies, while simultaneously enriching the theory, practice, and tools for logic design, synthesis, mapping and optimization. The proposed plan includes: (i) design and FPGA implementation of an efficient general purpose Garbled Processor for secure computation; (ii) Creating the challenging application-specific GC matching and search engines with a higher than linear complexity. (iii) Devising new custom SFE engines for Machine Learning tasks.", "The growing hardware security community is faced with an immediate need to develop effective tools and benchmarks. The purpose of this project is to lead a community-wide movement toward stronger assurances in our integrated circuits, computational platforms, and electronics supply chain. Based on the proven effectiveness of information sharing in many communities including crypto and computer-aided design, the PIs plan a significant enhancement and increase in scope of Trust-Hub, a central web-based repository for benchmarks, hardware authentication platforms, evaluation software, source codes, tools, and much more in the area of hardware security and trust.\n\nThe intellectual merits of this project include development of (i) procedures to dynamically generate trust benchmarks with hard-to-detect Trojan instances, (ii) hardware platforms for security validation, (iii) a suite of security metrics, (iv) a suite of ready to apply attacks, and (viii) comprehensive hardware/software validation test suites and data. The results of this project would be of interest to semiconductor companies, US government agencies and university researchers worldwide. Benefits to the society include secure and trustworthy electronics for healthcare, defense, finance, transportation, automotive and other applications. This research will impact the education of students, industry professionals, and government agencies' engineers through courses, online materials, and on- and off-campus seminars."], "researcher_paper_abstract_in_json_file": "Methods, apparatuses and articles for testing security of a mapping function\u2014such as a Physically Unclonable Function (PUF)\u2014of an integrated circuit (IC) are disclosed. In various embodiments, one or more tests may be performed. In various embodiments, the tests may include a predictability test, a collision test, a sensitivity test, a reverse-engineering test and an emulation test. In various embodiments, a test may determine a metric to indicate a level of security or vulnerability. In various embodiments, a test may include characterizing one or more delay elements and/or path segments of the mapping function. Other embodiments may be described and claimed.", "paper_keywords": "", "score_lsi": ["0.52373", "0.493277"]}
{"researcher_id": [10712], "researcher_name_in_nsf_list": "Jun  Li", "researcher_paper_title_in_json_file": "Stochastic Traffic Assignment Considering Road Guidance Information", "projects_cnt": 1, "year": 2012, "paper_citation": 50, "score_lda": ["0.916903"], "field": ["Traffic generation model", "Traffic engineering", "Simulation", "Engineering", "Civil Engineering", "Operations management", "Traffic flow", "Transport engineering", "Statistics"], "researcher_nsf_project_abstract": ["Online social networks (OSNs) face various forms of fraud and attacks, such as spam, denial of service, Sybil attacks, and viral marketing.  In order to build trustworthy and secure OSNs, it has become critical to develop techniques to analyze and detect OSN fraud and attacks.  Existing OSN security approaches usually target a specific type of OSN fraud or attack and often fall short of detecting more complex attacks such as collusive attacks that involve many fraudulent OSN accounts, or dynamic attacks that encompass multiple attack phases over time.  This research, dubbed oSAFARI (Online SociAl network Fraud and Attack Research and Identification), models, analyzes and characterizes OSN frauds and attacks; designs, develops, and evaluates a new approach to detecting static OSN frauds and attacks; and further enhances the approach to handle dynamic attacks with multiple phases.  The research team plans to develop a new course focused on OSN attacks and defenses, which has the potential to be offered across many institutions.  To increase public security awareness, the team also plans to develop tutorial courses on typical OSN attacks and their defense and offer them at popular public events and in freshman classes.  The research team will broadly disseminate their results, tools, software, and documents to the research community, IT industries, and to OSN companies.\n \nThis project embraces a systematic, comprehensive study of OSN frauds and attacks.  It models OSN threats by viewing an OSN as a graph embedded with attacker nodes and edges, identifies and analyzes specific forms of frauds and attacks, and evaluates state-of-the-art attack analysis and defense approaches.  It develops a spectral-analysis-based framework for OSN fraud and attack detection.  The framework transforms topological information of an OSN graph into patterns formed by spectral coordinates in the spectral space, and introduces the use of the spectral graph perturbation theory to more easily model and capture changes of spectral coordinates for attacker, victim, and regular nodes.  Further, this research develops spectral-analysis-based detection approaches for complex networks where nodes can carry attributes and edges can be negative, weighted, or asymmetric.  Through a novel combination of the network dynamics and the vector autoregressive model, it develops an automatic spectral-analysis-based approach to detecting dynamic attacks while avoiding the high cost and low accuracy of traditional approaches.  It also transforms attack characteristics from high-dimensional spectral spaces into distinctive visual patterns, and develops interactive mechanisms for analysts to incorporate domain knowledge and flexibly handle attacks.  The research team will build a simulation framework to evaluate the detection approaches against different types of OSN attacks, where one can plug in different OSN datasets to evaluate and compare different detection approaches.  Moreover, the research team will build a prototype oSAFARI on top of an OSN, and evaluate how oSAFARI can withstand various attacks in a real setting."], "researcher_paper_abstract_in_json_file": "Abstract  A Logit-based stochastic traffic assignment method considering the acceptance of traffic information is proposed to evaluate the impact of road guidance information on flow patterns. Travelers are divided into two groups by market penetration, namely, one group following the guidance information and the other choosing routes normally for which a Logit-style choice model is applied. A mixed stochastic network loading algorithm according to the mixed behavior is proposed, and the method of successive average (MSA) is then employed to assign traffic in congested network. The numerical examples show that proper information can improve network performance and there exists a best market penetration for given OD. It is found that the best market penetration increases when traffic demand becomes higher, and overall network time can be reduced even more by adding guidance properly.", "paper_keywords": ["stochastic traffic assignment", "route guidance", "\u4ea4\u901a\u5de5\u7a0b", "traffic assignment", "\u968f\u673a\u4ea4\u901a\u5206\u914d", "market penetration", "network loading", "traffic flow", "journal", "\u5e02\u573a\u5360\u6709\u7387 traffic engineering", "advanced traveler information systems", "stochastic processes", "\u671f\u520a\u8bba\u6587", "\u7f51\u7edc\u52a0\u8f7d", "traffic engineering", "\u9053\u8def\u6307\u5f15\u4fe1\u606f", "road guidance information", "logits"], "score_lsi": ["0.557859"]}
{"researcher_id": [10020], "researcher_name_in_nsf_list": "Charles F Cadieu", "researcher_paper_title_in_json_file": "Probabilistic models of phase variables for visual representation and neural dynamics", "projects_cnt": 1, "year": 2009, "paper_citation": 50, "score_lda": ["0.372055"], "field": ["Simulation", "Computer Science", "Artificial intelligence", "Machine learning"], "researcher_nsf_project_abstract": ["The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be in the field of healthcare.  The United States spends approximately $9,000 per person per year on healthcare.  Ultrasound medical imaging is a medical imaging technology that could lower costs by providing an alternative to higher-cost imaging techniques.  The technology created during this Phase II project is expected to increase the quality, value, and accessibility of medical ultrasound, which would in turn reduce medical imaging costs in the US healthcare system.  Furthermore, the company's technology is expected to bring ultrasound to more clinical settings and improve system-wide efficiencies in the diagnosis and treatment of disease.  The technology also has commercial potential in the international market, with $5.8B spent annually on medical ultrasound devices worldwide.  Finally, by improving the utility of ultrasound, the technology will lead to improved patient care and may ultimately save lives.\n\nThis Small Business Innovation Research (SBIR) Phase II project will develop deep learning technology for ultrasound imaging in medicine.  Ultrasound imaging has numerous benefits including real-time image acquisition, non-invasive scanning, low-cost devices, and no known side-effects (it is non-ionizing).  However, variability in quality has encumbered its adoption and utility.  As a result, more expensive imaging is typically utilized, often exposing patients to ionizing radiation.  Our objective is to develop, improve, and test machine learning techniques, based on deep learning, to improve ultrasound acquisition and interpretation.  We expect this project will create novel technologies that make ultrasound easier to use and improve the quality of ultrasound examinations.  The end result will improve the quality, value, and accessibility of medical ultrasound examinations, will result in cost savings to the healthcare system, will produce improvements in patient care, and will support a sustainable business opportunity."], "researcher_paper_abstract_in_json_file": "My work seeks to contribute to three broad goals: predicting the computational representations found in the brain, developing algorithms that help us infer the computations that the brain performs, and producing better statistical models of natural signals. At first glance these goals may not seem compatible; however, my work finds a common thread among them through the probabilistic modeling of phase variables. My thesis is broken down into three major chapters that reflect these three goals. Within each chapter I develop novel probabilistic models of phase variables and apply these models to the invariant representation of visual motion, to the inference of connectivity in networks of coupled neural oscillators, and to the development of statistical models of edge structure in images. First, I develop a hierarchical model of visual processing that learns from the natural world the higher-order structure of visual motion by modeling phase transformations. The model exhibits an important invariance: the model represents the way the world moves irrespective of the way it looks. This model has implications for our interpretation of biological visual processing and provides a functional roll for feedback in cortex. Second, I present a model and estimation technique that captures the dynamics of coupled oscillator systems and recovers the interactions of the oscillators from measurements. From a statistical perspective, the model is the multivariate phase distribution analogue to the multivariate Gaussian distribution and the estimation technique is then analogous to finding the inverse covariance matrix for a Gaussian distribution. From a dynamical systems perspective, the technique provides a solution to the inverse problem of the generalized Kuramoto model and infers from measurements the true connectivity between oscillators even when phase correlations or other phase measurements would lead to false conclusions. This technique can be broadly applied to a range of neurobiological phenomena including the inference of cortical dynamic functional networks from phase measurements. Third, I present a model that captures aspects of the local phase structure of edges in images. We first explore the pairwise phase statistics of local, oriented filters in response to natural images and determine that pairwise phase relationships do not explain the `interesting' relationships in natural images, such as long range phase alignments. Given this finding we develop a conditional latent variable model that captures the non-stationary phase structure produced by continuous edges. This model is capable of generating long range, continuous edge structure, a hallmark of natural images. The major contributions of this thesis can be divided into two types. First, this work provides demonstrative examples of how multivariate phase distributions may be modeled in a probabilistic framework. My hope is that the models I have developed will provide the basis for additional exploration of the mathematical development of probabilistic models of phase. Second, the results obtained from applying these models have important implications for understanding invariant visual representations of motion, investigating coherence mediated intracortical communication, and describing the statistical structure of edges in natural images.", "paper_keywords": ["probabilistic models", "visual neuroscience", "berkeley bruno a olshausen cadieu", "neurosciences probabilistic models of phase variables for visual representation and neural dynamics university of california", "charles", "biology neuroscience", "phase variables", "neural dynamics"], "score_lsi": ["0.420901"]}
{"researcher_id": [10114, 11854], "researcher_name_in_nsf_list": "Yan  Wang", "researcher_paper_title_in_json_file": "Research Article Investigating the Effectiveness of Computer Simulations for Chemistry Learning", "projects_cnt": 2, "year": 2012, "paper_citation": 0, "score_lda": ["0.947053", "0.9848"], "field": ["Mathematics education", "Simulation", "Computer Science"], "researcher_nsf_project_abstract": ["This project exploits mobile sensing and vehicle localization to identify fine-grained abnormal driving behaviors, such as weaving, swerving, and fast U-turns, and further to infer location-aware dangerous vehicular status. Several existing works have tried to detect abnormal driving behaviors by focusing on detecting drivers' status based on pre-deployed infrastructure, such as alcohol sensors, infrared sensors, and cameras. Such approaches incur extra installation cost and are thus difficult to be widely adopted. In order to build pervasive location-aware driving safety systems, this project tries to deploy low power consumption sensing (utilizing mobile devices carried by users in vehicles) and learning techniques based on statistical analysis to localize vehicles and identify fine-grained abnormal driving behaviors. More importantly, the proposed system keeps tracking the drivers' behaviors and determines fine-grained location-related dangerous vehicular status, such as driving on the center line of two-way roads or occupying left lanes for a long time.\n\nThis project seeks to conduct a comprehensive study to understand to what extent the current mobile devices can model various real-world driving behaviors and corresponding vehicle dynamics. A new real-time mobile sensing system, which combines real-time mobile sensing and heterogeneous driving environments, is developed to address driving safety concerns. The final results will be the abiding principles of cyber-physical architecture that resolve dynamic impacts of complex environments and provide clear guidelines over Internet of Things (IoTs). Specifically, effective features are investigated from mobile sensor readings that are able to depict each type of abnormal driving behaviors. These features can thus be extracted to localize the vehicles and derive the patterns of abnormal driving behaviors (e.g., weaving, swerving, fast U-turn, and sudden breaks) with the consideration of generic driving scenarios and heterogeneous mobile devices. Techniques based on machine learning are developed to generate a classifier model that could clearly identify fine-grained abnormal driving behaviors. The classifier model will be further utilized as a foundation to devise the location-aware driving safety system, which can track users' driving behaviors and realize location-related dangerous vehicular status in real-time using low-computing-capability mobile devices.", "The ACM MobiCom conference is the leading international conference focusing on systems issues in the emerging area of mobile computing and wireless communications. The topics addressed by MobiCom 2016 are at the core of the current wireless network evolution. The conference attracts research contributions spanning multiple disciplines, including wireless networking, vehicular communications, personal area networks, ad hoc networks, operating systems, distributed algorithms, data processing, scheduling, sensors, and signal processing. In addition to the main session, the conference also provides good opportunities to students and researchers to participate in panels, workshops and demos that bring in brilliant ideas and applications in mobile, wireless and ad hoc networking and computing.\n\nThe NSF travel grant program will help increase representation and participation of United States-based graduate and undergraduate students at this conference, and enable sharing of information and preliminary research results among participating students. Participation of top conferences like ACM MobiCom is an extremely important part of the graduate school experience, providing students with an opportunity to interact with more senior researchers, and exposing them to leading edge research in the field. The award will enable graduate students who would be otherwise unable, to attend the main MobiCom 2016 and the associated workshops."], "researcher_paper_abstract_in_json_file": "Are well-designed computer simulations an effective tool to support student understand- ing of complex concepts in chemistry when integrated into high school science classrooms? We investigated scaling up the use of a sequence of simulations of kinetic molecular theory and associated topics of diffusion, gas laws, and phase change, which we designed and experimentally tested. In the two effectiveness studies reported, one in a rural and the other in an urban context, chemistry teachers implemented two alternate versions of a curricular unit\u2014an experimental version, incorporating simula- tions, and a control version, using text-based materials covering the same content. Participants were 718 high school students (357 rural and 361 urban), in a total of 25 classrooms. The implementation of the simulations was explored using criteria associated with fidelity of implementation (FOI). Each context provided insights into the role of FOI in affecting the effectiveness of the interventions when working with groups of teachers. Results supported the effectiveness of this sequence of simula- tions as a teaching tool in a classroom context, and confirmed the importance of FOI factors such as adherence and exposure in determining the specific environments in which these materials were most effective. 2012 Wiley Periodicals, Inc. J Res Sci Teach 49: 394-419, 2012", "paper_keywords": "", "score_lsi": ["0.254737", "0.284846"]}
{"researcher_id": [1240], "researcher_name_in_nsf_list": "Hrvoje  Petek", "researcher_paper_title_in_json_file": "Quasiparticle Interfacial Level Alignment of Highly Hybridized Frontier Levels: H2O on TiO2(110)", "projects_cnt": 1, "year": 2015, "paper_citation": 50, "score_lda": ["0.526672"], "field": ["Biology", "Text mining", "Medical research", "Simulation", "Chemistry", "Computer Science", "Bioinformatics", "Nanotechnology", "Physics"], "researcher_nsf_project_abstract": ["Titanium dioxide (Ti02) is a model system for solar energy capture and transfer to drive chemical processes.  The material has the ability to absorb ultraviolet (UV) light and use it to assist in, or catalyze, the decomposition of water (H2O) to hydrogen (H2) and oxygen (O2) gases, which can be stored and later combined to produce energy on demand.  The material can also catalyze the reduction of carbon dioxide (CO2), a greenhouse gas that is the final product of burning fossil fuels, and transform this environmentally harmful gas into useful hydrocarbons for the chemical industry. Dr. Petek is engaged in fundamental studies of how this material absorbs UV light to create electron-hole pairs and how these electrons and holes are transported through the material and used to effect chemical change for sustainable solar energy conversion. The fast speeds of these reactions are measured in femtoseconds (1 femtosecond equals 0.000000000000001 second), and are studied by ultrafast laser spectroscopy.  Dr. Petek and his collaborator, Dr. Zhao, combine advanced ultrafast laser spectroscopy experiments with theoretical calculations to understand these photo-catalytic events.  In addition to the broader impacts of the research to contribute to the development of clean and efficient solar energy capture, there are substantial educational benefits for the students involved in the project. The collaborative arrangement that Dr. Petek has established with Dr. Zhao at the University of Science and Technology China and her co-worker Dr. Min Feng at Wuhan University provides excellent opportunities for students from the University of Pittsburgh to work in the Chinese laboratories, as well as for Chinese students to serve as summer interns in Dr.  Petek's laboratory.  To broaden student participation in research, Dr. Petek uses the Pittsburgh Quantum Initiative to recruit talented undergraduate students into research assistantships.  He is an ardent promotor of photocatalytic research, working to organize of international symposia on the topic as well as serving in the role of Editor-in-Chief of the journal Progress in Surface Science. \n\nWith funding from the Chemical Catalysis Program of the Chemistry Division, Dr. Petek and Dr. Zhao study the electron spectroscopy and dynamics related to photocatalytic processes by means of time-resolved two-photon photoemission (TR-2PP) experiments and advanced electronic structure theory.  The 2PP spectra and time resolved measurements reveal the polaronic character of electrons introduced into the conduction band of TiO2, the interaction of chemisorbed molecules with the polarons, and the unoccupied resonances of chemisorbed molecules such as CO2.  Dr. Petek conducts TR-2PP experiments on noble metal decorated TiO2 surfaces to reveal the mechanisms of plasmonically enhanced photocatalysis. In collaboration, Dr. Zhao performs electronic structure calculations to identify the electron and hole acceptor states, potentially involved in photocatalysis, for molecules adsorbed on TiO2 surface. In addition, nonadiabatic molecular dynamics calculations are used to describe the interfacial charge transfer dynamics and carrier energy relaxation rates for chemisorbed molecular overlayers at finite temperatures. The research has broader impacts as it brings together experimentalists and theorists from the University of Pittsburgh and University of Science and Technology of China (USTC) to work on the fundamental aspects of sustainable solar energy conversion via TiO2 photocatalysis. In addition to the broader impacts of the research, there are substantial educational benefits for students involved in the established collaborative project between the USTC and Wuhan University with the University of Pittsburgh, which provides opportunities for undergraduate and graduate student international exchange. To broaden student participation in research, Dr. Petek uses the Pittsburgh Quantum Initiative to recruit talented undergraduate students into research assistantships.  He is an ardent promotor of photocatalytic research, working to organize of international symposia on the topic as well as serving in the role of Editor-in-Chief of the journal Progress in Surface Science"], "researcher_paper_abstract_in_json_file": "Knowledge of the frontier levels\u2019 alignment prior to photoirradiation is necessary to achieve a complete quantitative description of H2O photocatalysis on TiO2(110). Although H2O on rutile TiO2(110) has been thoroughly studied both experimentally and theoretically, a quantitative value for the energy of the highest H2O occupied levels is still lacking. For experiment, this is due to the H2O levels being obscured by hybridization with TiO2(110) levels in the difference spectra obtained via ultraviolet photoemission spectroscopy (UPS). For theory, this is due to inherent difficulties in properly describing many-body effects at the H2O\u2013TiO2(110) interface. Using the projected density of states (DOS) from state-of-the-art quasiparticle (QP) G0W0, we disentangle the adsorbate and surface contributions to the complex UPS spectra of H2O on TiO2(110). We perform this separation as a function of H2O coverage and dissociation on stoichiometric and reduced surfaces. Due to hybridization with the TiO2(110) surface, the H2O 3a1 and 1b1 levels are broadened into several peaks between 5 and 1 eV below the TiO2(110) valence band maximum (VBM). These peaks have both intermolecular and interfacial bonding and antibonding character. We find the highest occupied levels of H2O adsorbed intact and dissociated on stoichiometric TiO2(110) are 1.1 and 0.9 eV belowmore\u00a0\u00bb the VBM. We also find a similar energy of 1.1 eV for the highest occupied levels of H2O when adsorbed dissociatively on a bridging O vacancy of the reduced surface. In both cases, these energies are significantly higher (by 0.6 to 2.6 eV) than those estimated from UPS difference spectra, which are inconclusive in this energy region. Finally, we apply self-consistent QPGW (scQPGW1) to obtain the ionization potential of the H2O\u2013TiO2(110) interface.\u00ab\u00a0less", "paper_keywords": ["biological patents", "biomedical journals", "environmental molecular sciences laboratory", "text mining", "europe pubmed central", "articulo", "citation search", "citation networks", "research articles", "abstracts", "open access", "life sciences", "clinical guidelines", "full text", "rest apis", "orcids", "europe pmc", "biomedical research", "bioinformatics", "literature search"], "score_lsi": ["0.536412"]}
{"researcher_id": [13152], "researcher_name_in_nsf_list": "RUSS  TEDRAKE", "researcher_paper_title_in_json_file": "Path planning in 1000+ dimensions using a task-space Voronoi bias", "projects_cnt": 1, "year": 2009, "paper_citation": 67, "score_lda": ["0.613871"], "field": ["Control engineering", "Simulation", "Any-angle path planning", "Computer Science", "Artificial intelligence", "Control theory", "Motion planning", "Kinodynamic planning"], "researcher_nsf_project_abstract": ["IN JUNE 2012, MIT DECIDED TO FORM A TEAM TO COMPETE IN THE DARPA ROBOTICS CHALLENGE DRC DESPITE HAVING NEVER WORKED ON HUMANOID ROBOTS BEFORE. THANKS TO A PRINCIPLED OPTIMIZATION-BASED APPROACH TO PERCEPTION, PLANNING, AND CONTROL THAT EMPHASIZED TASK LEV"], "researcher_paper_abstract_in_json_file": "The reduction of the kinematics and/or dynamics of a high-DOF robotic manipulator to a low-dimension \u201ctask space\u201d has proven to be an invaluable tool for designing feedback controllers. When obstacles or other kinodynamic constraints complicate the feedback design process, motion planning techniques can often still find feasible paths, but these techniques are typically implemented in the high-dimensional configuration (or state) space. Here we argue that providing a Voronoi bias in the task space can dramatically improve the performance of randomized motion planners, while still avoiding non-trivial constraints in the configuration (or state) space. We demonstrate the potential of task-space search by planning collision-free trajectories for a 1500 link arm through obstacles to reach a desired end-effector position.", "paper_keywords": ["design process", "high dimensionality", "path planning manipulator dynamics kinematics orbital robotics adaptive control state feedback process design motion planning process planning trajectory", "task space voronoi bias", "path planning", "degree of freedom", "random sampling", "collision free trajectory", "manipulator dynamics", "mobile robots", "data mining", "manipulator kinematics", "high dimensional configuration space", "robot manipulator", "feedback", "trajectory", "rapidly exploring random tree", "three dimensional displays", "control system synthesis", "heuristic algorithms", "human body", "state space", "mobile robots collision avoidance control system synthesis feedback manipulator dynamics manipulator kinematics", "aerospace electronics", "motion planning technique", "distance metric", "motion planning", "planning", "collision avoidance", "feedback controller design", "jacobian matrices", "collision free trajectory path planning task space voronoi bias robotic manipulator dynamics robotic manipulator kinematics feedback controller design motion planning technique high dimensional configuration space", "robotic manipulator kinematics", "robotic manipulator dynamics"], "score_lsi": ["0.868773"]}
{"researcher_id": [13027], "researcher_name_in_nsf_list": "Yang  Wang", "researcher_paper_title_in_json_file": "Research on the Application of Multimedia Simulation Technology", "projects_cnt": 1, "year": 2013, "paper_citation": 0, "score_lda": ["0.867784"], "field": ["Computer vision", "Simulation", "Computer Science", "Video tracking", "Frame", "Multimedia"], "researcher_nsf_project_abstract": ["Replication is widely used in the IT industry to protect users' data against various kinds of errors, such as power loss, disk corruption, and network partition. It is generally the case that we need to pay a higher cost for stronger replication protocols that can tolerate more kinds of errors. This presents a challenging trade-off to developers who will sometimes choose to use weaker forms of replication, willing to take the risk of occasional data loss in exchange for low cost. This project presents how to reduce the cost of Paxos, a popular strong replication protocol in today's data centers, while preserving its other properties.\n\nWhile this topic has already drawn a continuous effort in academia, existing approaches usually give up certain useful properties of Paxos, such as availability, presenting a different but equally hard question to the developers. This work presents ThriftyPaxos, a replication protocol that can achieve the same properties as Paxos with lower cost. To reduce cost, ThriftyPaxos incorporates the idea of on-demand instantiation, which activates the minimal number of replicas when there are no failures, and activates backup replicas when the active ones fail. To solve the key limitation of on-demand instantiation, that the system is unavailable when the backup is rebuilding its state, ThriftyPaxos incorporates the idea of delayed recovery, which allows the system to proceed while recovering a backup replica in the background. Such design is motivated by the observation that when acceptors and learners, the two key components of Paxos, are decoupled, it is possible to design separate mechanisms to delay their recovery without blocking the system.\n\nA cheaper Paxos protocol will be appealing to both existing Paxos users and to those who are still using weaker protocols, because they will not have  the same type of difficult choice between data consistency and cost. To realize such impact, the PI will apply ThriftyPaxos to popular open-source software, as well as publishing the source code of ThriftyPaxos.  This project has impact to education as well.  The code will be converted for use in a course project so that graduate and undergraduate students can build a similar system with reasonable effort in a course project."], "researcher_paper_abstract_in_json_file": "The multimedia technology has been widely applied to many engineering fields. However, because the data contained in video content is very large, it is always being a difficult problem of computer data analysis and processing to analyze the video. Based on the content analysis, this paper takes use of many technologies aimed at the problem of video, such as analysis and processing of multimedia, simulation classification of computer and computer vision and so on. At the same time, combined with the model of color information semantics and the real target tracking principle, this paper builds model and designs the algorithm for the video simulation. At last, this paper makes trajectory extraction and recognition for the real process goals of football, establishing the simulation process of football. Through the numerical simulation, it is found that frames extracted from the video capture are different from each other in the process of real football game and the recognition rate and accuracy of simulation trajectory are also not the same. Among them, when frame is 85, the effects of recognition rate and accuracy are best, which respectively reach 80% and 89%. Thus, it gains a better simulation effect.", "paper_keywords": ["computer data analysis", "multimedia simulation technology", "frames", "color semantic identification", "football", "target tracking"], "score_lsi": ["0.616691"]}
{"researcher_id": [4305, 4319], "researcher_name_in_nsf_list": "Kai  Zhang", "researcher_paper_title_in_json_file": "Seeing the Forest from the Trees in Two Looks: Matrix Sketching by Cascaded Bilateral Sampling", "projects_cnt": 2, "year": 2016, "paper_citation": 0, "score_lda": ["0.806502", "0.915665"], "field": ["Mathematical optimization", "Simulation", "Computer Science", "Theoretical computer science", "Machine learning", "Mathematics", "Statistics"], "researcher_nsf_project_abstract": ["In modern statistical analysis, datasets often contain a large number of variables with complicated dependence structures. This situation is especially common in important problems in economics, engineering, finance, genetics, genomics, neurosciences, etc. One of the most important measures on the dependence between variables is the correlation coefficient, which describes their linear dependence. In the new paradigm described above, understanding the correlation and the behavior of correlated variables is a crucial problem and prompts statisticians to develop new theories and methods. Motivated by this challenge, the PI proposes to study the correlation through novel geometric perspectives. The overall objective is (1) to develop useful theories and methods on the correlation and (2) to build a stronger connection between geometry and statistics. The PI anticipates the achievement of his goals through an integration of research and education plans.\n\nThe research agenda is to systematically investigate three fundamental aspects of the correlation: (1) the magnitude and distribution of the maximal spurious sample correlation; (2) the detection of a low-rank correlation structure; and (3) the probability measure over the space of correlation matrices. In these studies, the novel integration of statistical and geometric insights characterizes the proposed solutions and facilitates precise probability statements. Completion of the proposed research will provide a comprehensive understanding of the correlation and a stronger connection between geometry and statistics. The PI also has comprehensive plans on educating graduate and undergraduate students and on disseminating the research results to the broader scientific community.", "With recent advances in technology, it is now possible to measure and record significant numbers of features on a single individual.  The volume, velocity, and variety, the \"3Vs\", of Big Data pose significant challenges for modeling and analysis of these massive datasets.  For example, to understand cancer at the genetic level, researchers need to detect rare and weak signals from thousands, or even millions, of candidate genetic markers obtained from a limited number of subjects.  Existing methods typically assume that the number of subjects is very large, an assumption often violated in practice.  The main goal of this project is to develop efficient methods for extremely large-dimensional, small sample size data.  The methodological advances will be extremely valuable in addressing Big Data challenges in different areas such as medical research, bioinformatics, financial analysis, and astronomic image analysis.  Efficient software packages and algorithms to implement the proposed methods will be developed and made publicly available.\n\nThe key innovative idea motivating this research is viewing a high-dimensional problem from a novel packing perspective, which allows the number of variables, p, to be arbitrarily large and the number of observations, n, to be finite.  The proposed research will systematically investigate three fundamental problems under this \"finite n, arbitrarily large p\" paradigm: (1) asymptotic theory of spurious correlations, (2) fast detection of low-rank correlation structures, and (3) detection boundary and optimal testing procedures for detecting rare and weak signals.  This research will transform the current asymptotic framework, transitioning from the regimes of \"large n, small p\" and \"large n, larger p\" to the regime of \"finite n, arbitrarily large p\"."], "researcher_paper_abstract_in_json_file": "Matrix sketching is aimed at finding close approximations of a matrix by factors of much smaller dimensions, which has important applications in optimization and machine learning. Given a matrix A of size m by n, state-of-the-art randomized algorithms take O(m * n) time and space to obtain its low-rank decomposition. Although quite useful, the need to store or manipulate the entire matrix makes it a computational bottleneck for truly large and dense inputs. Can we sketch an m-by-n matrix in O(m + n) cost by accessing only a small fraction of its rows and columns, without knowing anything about the remaining data? In this paper, we propose the cascaded bilateral sampling (CABS) framework to solve this problem. We start from demonstrating how the approximation quality of bilateral matrix sketching depends on the encoding powers of sampling. In particular, the sampled rows and columns should correspond to the code-vectors in the ground truth decompositions. Motivated by this analysis, we propose to first generate a pilot-sketch using simple random sampling, and then pursue more advanced, \"follow-up\" sampling on the pilot-sketch factors seeking maximal encoding powers. In this cascading process, the rise of approximation quality is shown to be lower-bounded by the improvement of encoding powers in the follow-up sampling step, thus theoretically guarantees the algorithmic boosting property. Computationally, our framework only takes linear time and space, and at the same time its performance rivals the quality of state-of-the-art algorithms consuming a quadratic amount of resources. Empirical evaluations on benchmark data fully demonstrate the potential of our methods in large scale matrix sketching and related areas.", "paper_keywords": "", "score_lsi": ["0.444528", "0.678081"]}
{"researcher_id": [8111], "researcher_name_in_nsf_list": "ZHENG  LIU", "researcher_paper_title_in_json_file": "Cushion Surface Modeling Based on Body Pressure Distribution and Subjective Rating", "projects_cnt": 1, "year": 2012, "paper_citation": 0, "score_lda": ["0.133298"], "field": ["Structural engineering", "Simulation", "Engineering", "Engineering drawing"], "researcher_nsf_project_abstract": ["HOW ATMOSPHERE AND SEA ICE INTERACT DEPENDS ON THE PREVAILING WEATHER. SYNOPTIC ACTIVITIES TRANSPORT ENERGY AND MOISTURE INTO THE ARCTIC AND MODIFY THE STRUCTURE OF THE ATMOSPHERE, CLOUDS, AND THE ENERGY BUDGET OVER SEA ICE. THE STRUCTURE OF THE ATMOSPHER"], "researcher_paper_abstract_in_json_file": "This paper presents a method of ergonomic design based on subjective rating for the purpose of modeling cushion surface mapped by body pressure distribution (BPD) test data. A sitting comfort evaluation scale was designed to collect subjective comfort perception. Optimal BPD test data were selected by comparing comfort rating after experiments on a trial seat. A data mapping model was established between point clouds in three dimensional coordinate and BPD test data, which can be recognized and transferred in CAD system. In this context, an ergonomic-aided system was developed in practical application to demonstrate the viability of the method. Two designers tried out the system to design a seat cushion, and compared it with conventional method in Rhino software. Results show that the system is more interactive to designers, which can save time of surface modeling by about 50%.", "paper_keywords": ["body pressure distribution", "seat comfort", "subjective rating", "surface design"], "score_lsi": ["0.159114"]}
{"researcher_id": [8137], "researcher_name_in_nsf_list": "Tom  Peterka", "researcher_paper_title_in_json_file": "Adaptive Performance-Constrained In Situ Visualization of Atmospheric Simulations", "projects_cnt": 1, "year": 2016, "paper_citation": 0, "score_lda": ["0.965411"], "field": ["Data modeling", "Atmospheric model", "Parallel computing", "Real-time computing", "Simulation", "Visualization", "Computer Science", "Operating system", "Pipeline transport", "Data visualization", "Measurement", "Statistics", "Computer graphics (images)"], "researcher_nsf_project_abstract": ["Trusting scientific applications requires guaranteeing the validity of computed results. Unfortunately, many examples of scientific computations have led to incorrect results, sometimes with catastrophic consequences. Currently known validation techniques cover only a fraction of the possible corruptions that numerical simulation and data analytics applications may suffer during execution. As science processes grow in size and complexity, the reliability and validity of their constituent steps is increasingly difficult to ascertain. Assessing validity in the presence of potential data corruptions is a serious and insufficiently recognized problem. Corruption may occur at all levels of computing, from the hardware to the application. An important aspect of these corruptions is that until they are discovered, all executions are at risk of being corrupted silently. In some documented cases, months have elapsed between the discovery of a corruption and notification to users. In the meantime, a potentially large number of executions may be corrupted, and incorrect conclusions may result. It may be difficult, after the fact, to check whether executions have actually been corrupted or not, so that even if corruptions do not lead to mistakes, they may lead to significant productivity losses. Virtually all simulations producing very large results need to reduce their data volume in some way before saving it --one technique is called lossy compression. \nThis project strives to validate the end result of the simulation coupled with lossy compression. This approach is useful for scientific simulations in such diverse areas as climate, cosmology, fluid dynamics, weather, and astrophysics --the drivers of this project. \nThis collaborative project applies the principle of an external algorithmic observer (EAO), where the product of a scientific application is compared with that of a surrogate function of much lower complexity. Corruptions are corrected using a variation of triple modular redundancy: if a corruption is detected, a second surrogate function is executed, and the correct value is chosen from the two results that are most in agreement. This new online detection/correction approach involves approximate comparison of the lossy compressed results of the scientific application and the surrogate function. The project explores the detection performance of surrogate functions, lossy compressors, and approximate comparison techniques. The project also explores how to select the surrogate, lossy compression, and approximate functions to optimize objectives and constraints set by the users. The evaluation considers a set of five applications spanning different computational methods, producing large datasets with I/O bottlenecks, and covering a variety of science problem domains relevant to the NSF. \nIn addition to serving the needs of scientists working in the fields listed above, this project will enhance the research experience of undergraduate students. A summer school focused on resilience is planned for summer 2016, and corruption detection/correction will be a major topic. The project is also organizing tutorials in major science conferences that include online detection/correction of numerical simulations."], "researcher_paper_abstract_in_json_file": "While many parallel visualization tools now provide in situ visualization capabilities, the trend has been to feed such tools with large amounts of unprocessed output data and let them render everything at the highest possible resolution. This leads to an increased run time of simulations that still have to complete within a fixed-length job allocation. In this paper, we tackle the challenge of enabling in situ visualization under performance constraints. Our approach shuffles data across processes according to its content and filters out part of it in order to feed a visualization pipeline with only a reorganized subset of the data produced by the simulation. Our framework leverages fast, generic evaluation procedures to score blocks of data, using information theory, statistics, and linear algebra. It monitors its own performance and adapts dynamically to achieve appropriate visual fidelity within predefined performance constraints. Experiments on the Blue Waters supercomputer with the CM1 simulation show that our approach enables a 5x speedup with respect to the initial visualization pipeline and is able to meet performance constraints.", "paper_keywords": ["measurement", "exascale computing", "visualization", "hpc", "damaris", "pipelines", "data visualization", "smart visualization", "atmospheric modeling", "in situ", "rendering computer graphics", "adaptation models", "data models"], "score_lsi": ["0.572114"]}
{"researcher_id": [10663], "researcher_name_in_nsf_list": "Zhen  Liu", "researcher_paper_title_in_json_file": "Automatic composition of secure workflows", "projects_cnt": 1, "year": 2006, "paper_citation": 50, "score_lda": ["0.897999"], "field": ["Planning", "Web service", "Time complexity", "Workflow", "Knowledge base", "Simulation", "Systems modeling", "Constraint satisfaction", "Information processing", "Computer Science", "Artificial intelligence", "Lattice", "Database", "Semantics", "Compatibility", "Grid", "Expert system"], "researcher_nsf_project_abstract": ["This project aims to answer a very fundamental yet very old scientific question: \"Why and how does water move due to temperature gradients in porous materials?\" This thermally induced water flux ubiquitously exists in porous materials, whenever both heat transfer and water movement are present. A scientific understanding of this phenomenon is an essential base for many important scientific and social challenges: climate effects on geomaterials, geothermal energy applications, behavior of porous materials under extreme conditions, and recovery of non-conventional fossil fuels such as gas hydrates and shale gas. However, despite the significance, this phenomenon has been an historically unsolved and perplexing issue affecting many science and engineering areas involving porous materials from traditional applications in civil engineering, soil science and petroleum engineering to emerging needs in microfluidics, material processing and biomechanics. This award supports the exploration of a new research concept/methodology and its application to reveal the physical mechanisms underlying thermally induced water flux for a complete scientific description and analysis framework for this phenomenon. As an exploratory study, which pioneers a very high-risk but possibly high-return concept, the success of the study may provide the geotechnical community a new understanding to tackle many issues which are hard to solve in the existing frameworks, and also provide a way to integrate porous material research which is currently distributed in various disciplines.  In addition to supporting a doctoral student, the project will support outreach activities for rural, low-socioeconomic students and native tribal communities in the Upper Peninsula of Michigan. An annual summer program will be established to engage K-12 students in hands-on-learning for understanding of porous materials.\n\nThis project will utilize a new concept/methodology, multiscale-driven multiphysics, to tackle the issue which appears difficult to solve with existing methods. The research will first focus on the macroscopic mechanisms underlying thermally induced water flux. For the purpose, thermally induced water flux will be related to the temperature dependence of the contact angle by conducting two sub-tasks: 1. measuring thermally induced water flux with a newly designed research setup, and 2. measuring the contact angle using a modified capillary rise method. Then molecular dynamics analysis will be carried out to reveal the microscopic mechanisms underneath the temperature dependence of the contact angle, which is hypothesized to be attributable to the temperature dependence of vapor adsorption. Finally, adsorption isotherms will be measured to experimentally validate the hypotheses and to couple the frameworks at both macro- and micro-scales, aiming at a physically-based and practically implementable framework for thermally induced water flux. The research is potentially very important as it attempts at breakthroughs via innovations on the theoretical (new theories at both macro and micro-scale), experimental (contact angle and thermally induced flux measurements), and numerical aspects (molecular simulations for water-mineral system).  In the long term, the project also serves as an exploratory effort to examine the concept of multiscale-driven multiphysics, for the purpose of enabling solutions to critical historical issues and pressing challenges arising from upcoming applications in sustainability, energy and environmental protection, which more and more involve non-isothermal behavior of soils, and in a broad sense, multiphysics in porous materials."], "researcher_paper_abstract_in_json_file": "Automatic goal-driven composition of information processing workflows, or workflow planning, has become an active area of research in recent years. Various workflow planning methods have been proposed for automatic application development in Web services, stream processing and grid computing. Significant progress has been made on the definition of composition rules. The composition rules can be specified based on the schema, interface and semantics-driven compatibility of processes and data. Workflows must also satisfy information flow security constraints. In this paper we introduce and study the problem of workflow planning in MLS systems under Bell-LaPadula (BLP) policy, or a similar lattice-based policy, such as Biba integrity model. Extending results from AI planning literature, we show that under certain simplifying assumptions the workflows satisfying BLP constraints can be constructed in linear time. When the policy allows downgraders for data declassification, the problem is NP-complele; nevertheless, with additional assumptions efficient algorithms do exist.", "paper_keywords": ["modelizacion", "groupware", "streaming", "sistema experto", "haute performance", "procesamiento informacion", "traitement flux donnee", "transmision continua", "securite informatique", "temps lineaire", "distributed computing", "service web", "semantics", "base connaissance", "integrite", "intelligence artificielle", "integridad", "compatibilidad", "web service", "tiempo lineal", "constraint satisfaction", "semantica", "semantique", "grid", "computer security", "modelisation", "satisfaction contrainte", "enrejado", "planificacion", "transmission en continu", "integrity", "treillis", "rejilla", "seguridad informatica", "linear time", "data flow processing", "information processing", "compatibility", "alto rendimiento", "grille", "calculo repartido", "workflow", "artificial intelligence", "base conocimiento", "planning", "compatibilite", "inteligencia artificial", "satisfaccion restriccion", "systeme expert", "planification", "traitement information", "collecticiel", "modeling", "high performance", "calcul reparti", "servicio web", "lattice", "knowledge base", "expert system"], "score_lsi": ["0.460371"]}
{"researcher_id": [9696], "researcher_name_in_nsf_list": "Steven J Miller", "researcher_paper_title_in_json_file": "The Pythagorean Won-Loss Formula and Hockey: A Statistical Justification for Using the Classic Baseball Formula as an Evaluative Tool in Hockey", "projects_cnt": 1, "year": 2012, "paper_citation": 0, "score_lda": ["0.79298"], "field": ["Simulation", "Artificial intelligence"], "researcher_nsf_project_abstract": ["The central questions in this project concern how events are distributed in diverse systems, such as energy levels of heavy nuclei, leading digits in sets of data, and the prime numbers among the integers. Similar to the central limit theorem in probability and statistics, there seem to be universal spacing laws that govern these and other phenomena; thus studies in one of these topics can frequently provide useful insights in the others. Understanding these systems requires the development of tools and techniques in complex analysis, Fourier analysis, number theory, and probability. Some of the topics have immediate practical applications; for example, the Internal Revenue Service uses Benford's law to locate corporate tax fraud. Many of questions under study in this project have components that are amenable to numerical experimentation; these and tractable special cases will be investigated with undergraduate, graduate, and postdoctoral research assistants. The investigator will also continue work in mathematics education. In addition to providing professional development opportunities to students (such as arranging for them to referee for journals, contribute to Mathematical Reviews, write expository articles for journals, and co-organize special sessions at professional society meetings), the investigator will involve students in expanding the Math Riddles web page (mathriddles.williams.edu), a site that is used in junior high and high schools around the world to excite students about mathematics.\n\nThis research project studies a variety of problems on L-functions, additive number theory, and Benford's law. A central theme is an analysis of gaps between events. The main topic concerns zeros of L-functions; connections have been observed between these and high energy nuclear physics and random matrix theory (RMT). Among the questions under study are: n-level densities (main and lower order terms) for zeros of L-functions, alternatives to the Katz-Sarnak determinantal expansions that are more amenable for comparisons between number theory and RMT, determining the optimal test functions to bound excess rank, biases in second moments of Fourier coefficients of L-functions, modeling zeros near the central point through excised RMT ensembles, large gaps between zeros of L-functions, the density of states and behavior of the eigenvalues of structured random matrix ensembles, generalized Zeckendorf decompositions and the gaps between summands, generalized sum and difference sets, Ramsey theory for sets avoiding 3-term geometric progressions in finite fields and non-commutative settings, and Benford's law in fragmentation problems and fraud detection."], "researcher_paper_abstract_in_json_file": "Originally devised for baseball, the Pythagorean Won-Loss formula estimates the percentage of games a team should have won at a particular point in a season. For decades, this formula had no mathematical justification. In 2006, Steven Miller provided a statistical derivation by making some heuristic assumptions about the distributions of runs scored and allowed by baseball teams. We make a similar set of assumptions about hockey teams and show that the formula is just as applicable to hockey as it is to baseball. We hope that this work spurs research in the use of the Pythagorean Won-Loss formula as an evaluative tool for sports outside baseball.", "paper_keywords": "", "score_lsi": ["0.661055"]}
{"researcher_id": [5085], "researcher_name_in_nsf_list": "Soroush  Saghafian", "researcher_paper_title_in_json_file": "Data-Driven Percentile Optimization for Multi-Class Queueing Systems with Model Ambiguity: Theory and Application", "projects_cnt": 1, "year": 2017, "paper_citation": 0, "score_lda": ["0.799647"], "field": ["Simulation", "Computer Science", "Operations management", "Management science"], "researcher_nsf_project_abstract": ["Medical research has recently established the high frequency of New-Onset Diabetes After Transplant (NODAT), which refers to the incidence of diabetes in transplanted patients with no prior history of diabetes. The dynamic and complex interactions between immunosuppressive drugs used to ensure organ survival, medications used to prevent NODAT, and the simultaneous risks of NODAT and organ rejection has created a conundrum for physicians, leaving them in an ambiguous state in their post-transplant decisions. To assist physicians, the research will develop mathematical models using techniques from operations research, statistics, and econometrics. The models will consider multiple perspectives including a patient's quality adjusted lifespan, the risk of developing NODAT, the risk of organ rejection, the potential errors in estimating the health transition and observation probabilities, and the sensitivity and specificity of available medical tests. If successful, this collaborative award will help generate new guidelines and a data-driven decision support system that has the potential to increase patient safety and help hospitals reduce NODAT, organ rejection, and patient mortality. \n\n\nThe intellectual merits of the research include new directions for applications of operations research to healthcare. Specifically, the models consider the decision-maker's pessimism/optimism, direct incorporation of time-varying medical risk factors, and empower the decision maker to dynamically optimize with respect to a \"cloud\" of models (as opposed to a single model), thereby gaining robustness to potential model misspecifications without the need to perform sensitivity analyses. This is in sharp contrast with currently available techniques that solve a single dynamic optimization model (with parameters estimated from data sets), and then attempt to mitigate potential estimation errors via sensitivity analyses. Although motivated by the interactions between immunosuppressive drugs and diabetes medications for NODAT patients, the methodological contributions have other potential uses, such as in advancing the science of medication management for a variety of diseases for which therapeutic interventions have conflicting effects."], "researcher_paper_abstract_in_json_file": "Multi-class queueing systems widely used in operations management typically experience ambiguity in real-world settings in the form of unknown parameters. For such systems, we incorporate robustness in the control policies by applying a novel data-driven percentile optimization technique that allows for (1) expressing a controller's optimism level toward ambiguity, and (2) utilizing incoming data in order to learn the true system parameters. We show that the optimal policy under the percentile optimization objective is related to a closed-form index-based policy. We also identify connections between the optimal percentile optimization and c\u03bc-like policies, which in turn enables us to establish effective but easy-to-use heuristics for implementation in complex systems. Using real-world data collected from a leading U.S. hospital, we also apply our approach to a hospital Emergency Department (ED) setting, and demonstrate the benefits of using our framework for improving current patient flow policies.\u0000", "paper_keywords": ["multi class queueing systems", "percentile optimization", "ed operations", "robustness", "model ambiguity"], "score_lsi": ["0.560582"]}
{"researcher_id": [11335], "researcher_name_in_nsf_list": "Wei  Zhang", "researcher_paper_title_in_json_file": "Improved action point model in traffic flow based on driver's cognitive mechanism", "projects_cnt": 1, "year": 2004, "paper_citation": 50, "score_lda": ["0.983734"], "field": ["Simulation", "Intelligent driver model", "Engineering", "Automotive engineering", "Transport engineering"], "researcher_nsf_project_abstract": ["Many complex engineering systems involve interactions among a large number of agents with coupled dynamics and decisions due to their shared environment and resources. Such systems are often operated using a hierarchical architecture, where a coordinator determines some macroscopic control signal to steer the population to achieve a desired group objective while respecting local preferences and constraints for individual agents. Examples include electricity demand response programs, ground and air transportation systems, data center power management, robotic networks, among others. The goal of this project is to establish new control and game theoretic foundations, along with numerical algorithms, to enable formal and scalable design of hierarchical population control systems. \n\nIn contrast to the existing literature that primarily focuses on static strategic agents, this project will consider both strategic and non-strategic agents with nontrivial dynamics. The project involves three tasks. (i) First, it will establish control theoretic foundations for hierarchical population control of non-strategic agents (HPCN). Each non-strategic agent is associated with a predefined local response rule and is modeled as a hybrid system. A novel approach based on abstraction of stochastic hybrid systems (SHS) will be investigated to solve the HPCN problem. (ii) The project will also develop a uniform-price dynamic mechanism design framework for hierarchical population control of strategic agents (HPCS).  The framework is based the near-Nash equilibrium concept that can facilitate the analysis of the game-theoretic population behaviors. Advanced bi-level optimization algorithms will also be developed to address the computational challenges associated with the proposed mechanism design approach. (iii) The two proposed hierarchical population control frameworks will be used to study important demand response applications for the future power grid. \n\nThis research will significantly advance our understanding in complex engineering systems that involve coordination of a large population of dynamic agents. In collaboration with the Pacific Northwest National Laboratory, the project is also expected to yield practical algorithms and numerical tools for the design of electricity demand response programs. Moreover, the project will impact several education activities such as use of new pedagogical tools in teaching, involvement of undergraduate students in research, and research integration with teaching."], "researcher_paper_abstract_in_json_file": "Car-following modelling in traffic flow theory has been becoming of increasing importance in traffic engineering and Intelligent Transport System(ITS), the point of concentration in this research field is how to analysis and measurement of driver cognitive behaviour. Based on qualitative description of driving behaviour with the new concept of driver's multi-typed information process and multi-ruled decision-making mechanism, this paper has analysed in more detail the AP (action point) model, and ameliorated AP model by eliminating its deficiency. The emphasis of this paper is placed on the deduction of the acceleration equations by considering that the following car is subjected in congested traffic flow. Furthermore, from the cybernetics perspective, this paper has carried out numeral simulation to car-following behaviour with deceleration and acceleration algorithms. The model validation and simulation results have shown that the improved action point car-following model can replicated car-following behaviour and be able to use to reveal the essence of traffic flow characteristics.", "paper_keywords": ["automotive engineering", "automobiles", "traffic control vehicle safety road safety automotive engineering road vehicles vehicle driving acceleration equations educational programs computer vision", "intelligent transport system", "behavioural sciences computing", "road traffic", "improved action point model", "qualitative description", "traffic flow characteristics improved action point model car following modelling traffic flow theory traffic engineering intelligent transport system driver cognitive behaviour qualitative description driver multityped information process multiruled decision making mechanism acceleration equations numeral simulation car following behaviour acceleration algorithms deceleration algorithm", "traffic control", "acceleration equations", "deceleration algorithm", "vehicle driving", "traffic flow", "multiruled decision making mechanism", "acceleration", "computer vision", "model validation", "car following behaviour", "car following modelling", "behavioural sciences computing driver information systems road traffic automobiles decision making", "information processing", "numeral simulation", "traffic flow characteristics", "educational programs", "vehicle safety", "driver cognitive behaviour", "traffic engineered", "traffic engineering", "road safety", "traffic flow theory", "driver information systems", "acceleration algorithms", "numerical simulation", "road vehicles", "driver multityped information process"], "score_lsi": ["0.455873"]}
{"researcher_id": [6899], "researcher_name_in_nsf_list": "Amy S Gladfelter", "researcher_paper_title_in_json_file": "Summer at the Marine Biological Laboratory", "projects_cnt": 1, "year": 2013, "paper_citation": 0, "score_lda": ["0.984875"], "field": ["Humanities", "Meteorology", "Simulation", "Geography"], "researcher_nsf_project_abstract": ["This project will facilitate the attendance and participation of early career scientists in the Gordon Research Conference on Cellular and Molecular Fungal Biology to be held at the Holderness School, June 19-24, 2016. The goal of the conference is to disseminate information about fungal biology among an interdisciplinary group of researchers, and to increase our collective understanding of basic fungal biology and its application to socially important problems.  Fungi are essential parts of the terrestrial nutrient cycle, play a central role in the development of biofuels, and produce many critically important chemicals.  These diverse applications of fungi require the interdisciplinary acquisition and application of fundamental fungal biology.  This project will support the convergence and exchange of new findings amongst an interdisciplinary group of scientists dedicated to the study of fungi.  \n\nThe intellectual merit of the project is rooted in the meeting's highly interdisciplinary and interactive format. The meeting will feature topics that integrate multiple time and space scales for different questions in fungal biology to promote interactions amongst researchers with diverse perspectives within the community. There is a specific emphasis on integrating mathematical modeling and biophysics as a new addition to this meeting and an entire session is dedicated to the interface of fungal biology with the physical sciences. The meeting enables cross-fertilization of ideas, from cell biology to evolution, that occurs in and outside of the sessions and especially between junior and senior scientists.  Young investigators emphasize from previous meetings how interactive the conference is and how responsive it is to the presentation of their work.\n\nThis conference has broad impacts on training and is dedicated to extending the research community by emphasizing women and members of underrepresented groups in inviting speakers. The current invited speakers are approximately 50% women, including several Latinas.  The small size of the meeting and the emphasis on discussion (40% of meeting time is dedicated to discussions) encourages active participation. Poster sessions are featured without competing events to focus attention on the most junior scientists, who often have the newest data. The GRC on Cell and Molecular Fungal Biology also is dedicated to research that applies basic knowledge to socially important questions involving filamentous fungi, particularly mutualisms with plants (mycorrhizae), parasitism with plants (plant pathology) and animals (animal pathology), and industrial mycology (enzyme production). The interactions among researchers focused on both basic and socially important research speeds research aimed at solving societal problems caused by or that can be improved by fungi."], "researcher_paper_abstract_in_json_file": "Many scientists return to the MBL summer after summer to teach or do research. When we asked why, their answers included words like \u201crecharge\u201d, \u201cmagical\u201d, \u201c inspire\u201d, \u201cinteract\u201d, \u201cconnections\u201d, \u201clearn\u201d, and \u201clife changing\u201d. Clearly the MBL, and similar institutions that bring scientists together, impact researchers\u2019 lives long after the summer sun has set.", "paper_keywords": ["moving image", "science profession"], "score_lsi": ["0.708316"]}
{"researcher_id": [3727], "researcher_name_in_nsf_list": "Marc W Howard", "researcher_paper_title_in_json_file": "Optimally fuzzy temporal memory", "projects_cnt": 1, "year": 2012, "paper_citation": 50, "score_lda": ["0.900093"], "field": ["Simulation", "Computer Science", "Artificial intelligence", "Machine learning"], "researcher_nsf_project_abstract": ["Computer vision algorithms examine images and make sense of what these images depict. Current computer vision algorithms  are able to interpret images at the level of a typical middle school student for many image interpretation tasks.  Recent advances in computer vision have led to rapid technological advances which are still unfolding but affect not only the technology industry, but education,  national security and health care. However, these new algorithms are as yet poorly understood and do not describe how natural learners such as a typical middle school student learn to understand the visual world.  This proposal draws together a team of cognitive psychologists, neuroscientists, and computer scientists to develop a new class of algorithms for computer vision inspired by the way people learn.  \n\nThe key insight of this proposal is that human learners, unlike many leading computer vision techniques, make extensive use of the temporal structure of visual experience to extract structure.  In the real world the image  on the human retina is almost never static.   Changes in eye position and movements of the head and body create a rich and complex temporal structure over a range of scales from hundreds of milliseconds up to days and weeks. This proposal a) develops databases of realistic and dynamically changing images in the real world and in immersive virtual reality environments, b) develops computational models for learning visual representations from temporally structured experiences  and, c) examines the brain structures supporting representations integrating time and space across scales using fMRI. The algorithms pursued in this project are inspired by recent theoretical work in the neuroscience of scale-invariant memory.  However, because the databases will be made publicly available, other researchers will be able to develop other algorithms that exploit temporal and spatial correlations.  Taken together, these efforts are intended to catalyze a new generation of techniques for human-like machine learning algorithms with applications in computer vision."], "researcher_paper_abstract_in_json_file": "Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register\u2014a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system.", "paper_keywords": ["temporal information compression", "forecasting long range correlated time series"], "score_lsi": ["0.577632"]}
{"researcher_id": [5693], "researcher_name_in_nsf_list": "He  Huang", "researcher_paper_title_in_json_file": "STUMBLE DETECTION SYSTEMS AND METHODS FOR POWERED ARTIFICIAL LEGS", "projects_cnt": 1, "year": 2012, "paper_citation": 0, "score_lda": ["0.916586"], "field": ["Control engineering", "Embedded system", "Simulation", "Engineering"], "researcher_nsf_project_abstract": ["Emerging powered lower limb prostheses hold great promise for restoring normative locomotion in amputees.  However, these robotic devices currently lack inter- and intra-wearer adaptability to cope with wearers' physical variations and changes.  Frequent manual and heuristic adjustment in clinics is required, which limits the practical use of these advanced prostheses.  A new generation of prosthesis control that is intelligent, adaptable, and interactive is needed to better support walking function and improve the quality of life of lower limb amputees.  The PIs' long-term research goal is to create bionic legs that can adapt to the individual amputee's physical and cognitive capabilities, coordinate with the wearer's movement and intent, adjust to changing environments, and essentially restore the full function of patients with lower limb impairments.  To this end, the PIs' objective in this project is to create a novel optimal control framework for these prostheses.   They will systematically address the challenge of supporting automatic adaptation to the wearer's physical capability while achieving desired gait performance for the integrated amputee-prosthesis system.  And they will provide a preliminary design and evaluation for an interactive interface that would allow wearers to personalize prosthesis control safely and easily.  Project outcomes will open up a new frontier of wearable robotics and lay the foundation for clinical translations of these innovative devices, which will impact not only the prosthetics and orthotics industry but also the robotics community by providing new knowledge relating to human-robot interaction, the biomechanics and neuromotor control community by elucidating the control mechanism of amputee locomotion, and healthcare in general by providing innovative and cost-effective prosthesis solutions.\n\nThe new amputee-prosthesis performance-based framework for control of powered lower limb prostheses which this work will introduce represents a departure from existing approaches that mainly focus on design for the prosthesis (a local machine), in that it adopts a global approach by accounting for co-adaptation between amputees and prostheses in order to provide optimal, personalized assistance based on wearers' physical conditions.  The PIs will use approximate dynamic programming (ADP) to achieve the global control goal.  Such innovative use of ADP will provide an opportunity to demonstrate its optimal adaptive control capability in a new test domain of co-adaptive robotic prosthesis, a unique and significant challenge only seen in human wearable robotics but not in lifeless robots.  The ADP scheme is based on approximation and learning that alleviate problems associated with the requirement of accurately modeling wearers' neuromuscular control and dynamics that is difficult, if not impossible, to achieve.  Additionally, the PIs will conduct an experimental investigation on subjects with transfemoral amputations of the interactions between amputees and prostheses, including evaluations of the compensatory strategies of amputees, and discrepancy assessment between subjective (human) and objective (machine) preferences in prosthesis control."], "researcher_paper_abstract_in_json_file": "A stumble detection system is disclosed for use with a powered artificial leg for identifying whether a stumble event has occurred. The stumble detection system includes an acceleration sensor for providing acceleration data indicative of the magnitude of acceleration of a person's foot, and a detector that determines whether a stumble event has occurred responsive to the acceleration data and provides an output signal.", "paper_keywords": "", "score_lsi": ["0.350686"]}
{"researcher_id": [3228], "researcher_name_in_nsf_list": "Dieter  Fox", "researcher_paper_title_in_json_file": "Learning to navigate through crowded environments", "projects_cnt": 1, "year": 2010, "paper_citation": 136, "score_lda": ["0.775014"], "field": ["Mobile robot", "Computer vision", "Simulation", "Computer Science", "Engineering", "Artificial intelligence"], "researcher_nsf_project_abstract": ["Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts. This project addresses this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.\n\nThe proposed experiential learning framework will build on recent advances in deep neural networks. The key problem is to learn the mappings between raw perceptual and control data via a low-dimensional implicit physics space representing a perception-based physical model of how an object acts in the environment. Three directions will be investigated: 1) the development of experiential physics models for object interaction and fluid flow that have strong predictive capabilities, 2) creating mappings directly from experiential models to control of actions such as pouring or moving an object, 3) the assembly of local experience-based controllers into complex tasks from interactive demonstration. Additionally, the project will develop unique data sets that include physical models, simulations, data components, and learned components that other groups can access and build on to enable comparative research similar to what has emerged in machine perception."], "researcher_paper_abstract_in_json_file": "The goal of this research is to enable mobile robots to navigate through crowded environments such as indoor shopping malls, airports, or downtown side walks. The key research question addressed in this paper is how to learn planners that generate human-like motion behavior. Our approach uses inverse reinforcement learning (IRL) to learn human-like navigation behavior based on example paths. Since robots have only limited sensing, we extend existing IRL methods to the case of partially observable environments. We demonstrate the capabilities of our approach using a realistic crowd flow simulator in which we modeled multiple scenarios in crowded environments. We show that our planner learned to guide the robot along the flow of people when the environment is crowded, and along the shortest path if no people are around.", "paper_keywords": ["robot navigation mobile robots human like motion behavior inverse reinforcement learning realistic crowd flow simulator", "robot sensing systems", "shortest path", "inverse reinforcement learning", "path planning gaussian processes learning artificial intelligence mobile robots", "learning", "gaussian processes", "human like motion behavior", "mobile robot", "path planning", "training", "robot navigation", "mobile robots", "navigation learning cost function mobile robots airports robot sensing systems gaussian processes robotics and automation usa councils computer science", "navigation", "feature extraction", "partial observation", "flow simulation", "learning artificial intelligence", "realistic crowd flow simulator"], "score_lsi": ["0.808143"]}
{"researcher_id": [2460], "researcher_name_in_nsf_list": "Tulga  Ersal", "researcher_paper_title_in_json_file": "Structural Simplification of Modular Bond-Graph Models Based on Junction Inactivity", "projects_cnt": 1, "year": 2006, "paper_citation": 50, "score_lda": ["0.167389"], "field": ["Simulation", "Engineering", "Artificial intelligence"], "researcher_nsf_project_abstract": ["This research team envisions that connected testbeds, i.e., remotely accessible testbeds integrated over a network in closed loop, will provide an affordable, repeatable, scalable, and high-fidelity solution for early cyber-physical evaluation of connected automated vehicle (CAV) technologies. Engineering testbeds are critical for empirical validation of new concepts and transitioning new theory to practice. However, the high cost of establishing new testbeds or scaling the existing ones up hinders their wide utilization. This project aims to develop a scientific foundation to support this vision and demonstrate its utility for developing CAV technologies. This application is significant, because a synergistic combination of connected vehicles and automated driving technologies is poised to transform the sustainability of our transportation system; automated driving technologies can leverage the information available from vehicle-to-vehicle (V2V) connectivity in optimal ways to dramatically reduce fuel consumption and emissions. However, state-of-the-art simulation and experimental capabilities fall short of addressing the need for realistic, repeatable, scalable, and affordable means to evaluate new CAV concepts and technologies. The goal of this project is to enable a high-fidelity integration of geographically dispersed powertrain testbeds and use this novel experimental capability to develop and test powertrain-level strategies to increase sustainability benefits of CAVs.\n\nTo realize this vision, the first objective of this research is to develop a cyber-integration interface to increase coupling fidelity in connected testbeds. This objective will be pursued through a model-free predictor framework to compensate for network delays robustly. The second objective is to leverage this cyber-integration interface to create a connected testbed for CAVs. To this end, existing powertrain testbeds distributed across the University of Michigan campus and Environmental Protection Agency will be leveraged. The third objective is to use this connected testbed for (i) developing powertrain-level strategies to minimize fuel consumption and emissions in CAV platoons of mixed vehicle types, including light-, medium-, and heavy duty vehicles, (ii) uncovering the untapped potential of aggressively downsized powertrains, and (iii) understanding the limits of the benefits of connectivity due to various V2V communication issues. \n\nThis research area provides a rich space to advance the science of cyber-physical systems and demonstrate their impact, as it spans multiple disciplines including time delay systems, system dynamics and control, hardware-in-the-loop simulation, engine control, powertrain management, and communication networks. The potential of CAVs to improve the sustainability of transportation is an outstanding example of how cyber-physical systems can have a societal impact. The connected testbeds concept, on the other hand, can benefit not only CAVs, but also a wide range of applications such as telerobotics, haptics, networked control systems, earthquake engineering, manufacturing, and aerospace. It can open new doors for researchers to perform unparalleled integrative collaborations by enabling them to leverage each other's testbeds remotely."], "researcher_paper_abstract_in_json_file": "The modular modeling paradigm facilitates the efficient building, verification and handling of complex system models by assembling them from general-purpose component models. A drawback of this paradigm, however, is that the assembled system models may have excessively complex structures for certain purposes due to the amount of detail of the component models, which is introduced to promote modularity. This work presents a domain-independent structural simplification technique that can detect such unnecessary complexities in a modular bond-graph model and eliminate them from the model without compromising accuracy. To this end, the activity concept in the literature is extended to define \"inactivity\" for junction elements, and simplification is obtained by detecting and eliminating inactive junction elements and by propagating the implications. It is shown that this simple idea can result in models that are conceptually and computationally more efficient. Some subtleties associated with this approach are highlighted.Copyright \u00a9 2006 by ASME", "paper_keywords": ["junctions"], "score_lsi": ["0.341208"]}
{"researcher_id": [1393], "researcher_name_in_nsf_list": "Bo  Li", "researcher_paper_title_in_json_file": "Crime Modeling with Truncated Levy Flights and Effects of Police Patrol", "projects_cnt": 1, "year": 2016, "paper_citation": 0, "score_lda": ["0.882889"], "field": ["Simulation", "Mathematics", "Advertising"], "researcher_nsf_project_abstract": ["Interfacial fluctuations are common in many physical and biological systems. Understanding the principles that underlie such fluctuations has far-reaching scientific and technological consequences. For instance, the manipulation of interfacial fluctuations in the so-called molecular beam epitaxy of growing nanometer-scale semiconductor materials can largely improve the quality and functionality of such materials that are widely used for high-technology electronic and military sensor devices. Effective treatment of some fatal diseases relies critically on our knowledge of anomalous water-protein interfacial structures that result from fluctuations and biological mutations and that characterize such diseases. This project develops a state-of-the-art computer program to investigate how the fluctuation affects the structures and long-time dynamics of interfaces, with a particular application to the binding of a drug molecule to a target protein that is a crucial step in the computer-aided drug design. The success of this project can therefore potentially help reduce the high cost often needed for laboratory experiments and speed up the process of drug discovery. In addition, this highly interdisciplinary research brings unique opportunities for students at different levels, particularly those from under represented groups, to receive training at the interface of mathematical and biological sciences. Such training is critical to keeping our nation's strength in scientific research in a highly competitive international environment.  \n\nComputationally tracking the motion of fluctuating interface is in general rather challenging, as such motion involves multiple but correlated spatial and temporal scales, high energy barriers between one stable interfacial structure to another, and the coupling of interface and bulk processes. The PIs construct two methods to overcome some of these difficulties. One is the stochastic level-set method that describes the fluctuating interface by solving a stochastic differential equation. The noise in the equation is spatially localized on or near the interface. Rigorous stochastic analysis is carried out to reformulate such an equation for accurate and efficient computations. The other is a stochastic lattice-phase method that treats the coupling of both interfacial and bulk fluctuations. This method describes the interface geometry by assigning a binary value on each of the discrete sites, and minimizes a Hamiltonian of all possible discrete binary fields using a Monte Carlo simulation method. This Hamiltonian mimics the continuum one with spatial gradient-square term and a double-well potential. The mathematical analysis using the notion of Gamma-convergence reveals the interplay between the numerical grid size and the interfacial width, and directly guides the design of fast algorithms. The PIs also develop a parallel computational algorithm with the GPU implementation to speed up their computations. They combine their new techniques with a molecular solvation theory to study molecular recognition, particularly the binding of a small drug molecule to a target protein. The computational models, numerical algorithms, and computer codes developed in the project can be incorporated into existing software that are used on a daily basis to study biomolecular interactions, and in particular, for computer-aided drug design"], "researcher_paper_abstract_in_json_file": "In this paper, we developed a truncated Levy flight model to study the crime dynamics. In the discrete case, our model allows criminals to perform long jumps in between committing crimes with a speed light. This is a more realistic extension of a pioneering random walk model by Short et. al and a Levy flight model thereafter in Chaturapruek, et al. We also derive a continuum limit and perform a stability analysis to study the formation of crime hotspots. Our model is more realistic than the Levy Flight Model, and provides an alternative to the Random Walk Model when the criminals can perform long jumps in between committing crimes. In the next step, we introduce patrolling police officers to our new model following that in. We examine the effects of police patrol when the police choose to adopt different strategies, including unbiased random walk, biased random walk, and truncated Levy flight. We evaluate the effectiveness of the police patrol with the number of crime events in a given time frame. With spatially non-uniform initial conditions, we find that the truncated Levy flight to be the best strategy in general.", "paper_keywords": ["35r60 35q84", "physics soc ph"], "score_lsi": ["0.460986"]}
{"researcher_id": [6217], "researcher_name_in_nsf_list": "Iris D Tommelein", "researcher_paper_title_in_json_file": "Site\u2010Layout Modeling: How Can Artificial Intelligence Help?", "projects_cnt": 1, "year": 1992, "paper_citation": 77, "score_lda": ["0.981858"], "field": ["Layout", "Simulation", "Systems modeling", "Decision analysis", "Physical model", "Computer Science", "Engineering", "Artificial intelligence", "Mathematical model", "Location", "Construction management"], "researcher_nsf_project_abstract": ["A new conceptualization for planning and delivering Architecture-Engineering-Construction (AEC) projects is needed because today's approaches all too often fail: projects are late, over budget, and accident prone. This unduly burdens public and private facility owners, other stakeholders, and society at large. Failure stems from the inadequacy of the planning methods being used, especially those that focus disproportionately on maximizing resource utilization while ignoring network effects and the systemic impacts of variability. This research counterposes a method based on Takt time. Takt refers to the beat with which work progresses; it is being used successfully in repetitive manufacturing (e.g., Lean Production). The use of Takt time in AEC project production is novel. The method has been piloted and is promising, but must be formally studied. This award supports fundamental research that involves designing, analyzing, and testing principles, methods, and computational optimization tools for Takt time planning (TTP). The objective of this method is to optimize project delivery speed within total cost limits, and to provide a mechanism for continuous improvement. Investigation of the potential uses of the TTP method to improve project delivery performance will advance the theory of project production. Use of the new planning model, once formulated, understood, and documented, will increase the reliability with which projects get delivered, to the benefit of all stakeholders. Students underrepresented in science, technology, engineering, and mathematics will be engaged in conducting this research. Research findings will be integrated into engineering curricula and disseminated to industry to promote the use of more systemic methods for planning and delivering facilities.\n\nExisting planning models focus on maximizing resource utilization (point speed) and buffer for uncertainty using time (schedule buffers), which comes at the expense of throughput. This research flips the paradigm used in existing planning methods to one that focuses on overall project delivery performance and buffers with capacity. Planning using capacity means judiciously under-loading production resources, such as labor and equipment, to less than 100% utilization, so that they will have stand-by capacity to readily mitigate negative impacts of variation, thereby avoiding system-wide repercussions. Accordingly, a novel method for project planning will be designed and developed, based on the concept of Takt time. Computer algorithms will be created to automate the determination of zones and corresponding Takt times, suited to characteristics of various phases of project delivery. Their optimality and robustness will be established using Building Information Modeling (BIM) and discrete-event simulation. In parallel with the virtual study of TTP, the method will be deployed on actual projects so as to reveal socio-technical system characteristics that must be addressed for its successful deployment. Learning from actual projects will in turn inform further refinement of algorithms."], "researcher_paper_abstract_in_json_file": "Using the site\u2010layout task as an example to compare existing practices and tools used in industry and research environments, this paper puts artificial intelligence (AI) modeling techniques in perspective. The site\u2010layout task is characterized, field practice is described, and a thorough review of available tools for layout product modeling (including physical models and computer\u2010aided design tools) and process modeling (using AI as well as operations research methods) is presented. A rationale is provided for why many such tools have failed to gain widespread use in the construction industry. Comparing the capabilities as well as the data and knowledge needs of computer programs with those of construction practitioners reveals a large discrepancy, which is also apparent when fitting the layout literature in a comprehensive table. This paper argues that AI\u2010based systems can reduce this discrepancy by better matching model capabilities with industry needs, and, therefore, suggests that such models will bec...", "paper_keywords": ["modelizacion", "concepcion asistida", "complexite t\u00e2che", "computer aided design", "organizacion funcional", "implantation topometrie", "expert systems", "analisis decision", "construction industry", "industries", "intelligence artificielle", "recherche developpement", "layout", "location", "physical models", "construction equipment", "decision analysis", "artificial intelligent", "modelisation", "industrie construction", "computer models", "research and development", "obra", "mathematical models", "investigacion desarrollo", "building site", "construction sites", "decision support systems", "conception assistee", "artificial intelligence", "site preparation construction", "inteligencia artificial", "industria construccion", "task complexity", "modeling", "task organization", "building sites", "organisation fonctionnelle", "models", "analyse decision", "knowledge based systems", "construction management", "complejidad tarea", "chantier construction", "lay out", "implantacion topometria"], "score_lsi": ["0.574451"]}
{"researcher_id": [13529], "researcher_name_in_nsf_list": "Stuart T Smith", "researcher_paper_title_in_json_file": "Exergames for the elderly: Towards an embedded Kinect-based clinical test of falls risk", "projects_cnt": 1, "year": 2012, "paper_citation": 50, "score_lda": ["0.984968"], "field": ["Simulation", "Medicine", "Physical therapy", "Computer security"], "researcher_nsf_project_abstract": ["UC Santa Barbara doctoral candidate Jessika Akmenkalns, supervised by Dr. Stuart Tyson Smith, will conduct research on how cross-cultural interactions and colonialism transformed cultural identities in hinterland communities in ancient Nubia (northern Sudan) between 2500 and 1000 BC.  Archaeology provides a unique window through which researchers and the general public alike can come to understand how colonialism and long-term struggles for political and economic power affect the daily lives of individuals and groups, both locally and on a broad regional scale. This research is particularly relevant in today's global political climate, in which ethnic, religious, and national identities bear significant impact on access to land, resources, and opportunities.  This project will contribute to a better understanding of these issues on a broad scale because investigations of colonialism crosscut the social sciences, including anthropology, sociology, history, geography, economics, and political science.  In addition, the project will provide university students with educational opportunities and scientific training, and the field and laboratory experience gained on the project will fulfill students' degree requirements.  The data collected as part of this research will form the basis of Ms. Akmenkalns' doctoral dissertation.\n\nPrevious research on cross-cultural interactions has focused on large, urban populations, or it has focused on the strategies of colonizing powers.  While it is certainly important to examine how such interactions impact major centers and powerful political entities, it is also essential to investigate how rural communities experience changing power structures and how they construct their community identities in opposition to those of foreign cultural groups.  The following research questions guide this project:  1) What was the extent of Egyptian presence or influence in rural Nubia before and during Egyptian conquest of the region?  2) What was the importance of local and foreign cultural traditions in expressing cultural identity before and during the Egyptian conquest?  3) What differences, if any, were present in elite vs. non-elite and urban vs. rural expressions of cultural identity in the region?  To answer these questions, this project will employ archaeological excavations at the sites of Hannek and Abu Fatima, which are located in the hinterlands of the ancient kingdom of Kerma in northern Sudan.  Researchers will investigate the remains of houses, public buildings, and tombs to examine ancient diet and health, as well as the production and use of pottery, tools, jewelry, and architecture.  These artifact types are useful in understanding cultural identities because they are integral to daily life and the routine behaviors that structure those identities.  Archaeologists will perform preliminary analysis on artifacts and samples during the excavation season, followed by additional in-depth analyses that will be performed in US laboratories, to include radiocarbon dating, ceramic and stone tool classification, and the analysis of dietary samples such as animal and plant remains."], "researcher_paper_abstract_in_json_file": "Falls are the leading cause of disability, injuries or even death among older adults. Exercise programmes that include a balance component reduce the risk of falling by 40%. However, such interventions are often perceived as boring and drop-out rates are high. The characteristics of videogames may overcome this weakness and increase exercise adherence. The use of modern input devices, such as the Microsoft Kinect, enables quantification of player performance in terms of motor function while engaging with games. This capability has just started to be explored. The work presented in this paper focuses on the development of a Kinect-based system to deliver step training while simultaneously measuring parameters of stepping performance that have shown to predict falls in older people. Language: en", "paper_keywords": ["medical and health sciences", "pedestrian safety", "poison control", "injury prevention", "other health", "safety literature", "traffic safety", "injury control", "journal article", "home safety", "injury research", "safety abstracts", "human factors", "occupational safety", "safety", "other medical and health sciences", "safety research", "accident prevention", "violence prevention", "bicycle safety", "health", "poisoning prevention", "health not elsewhere classified", "falls", "ergonomics", "medical and health sciences not elsewhere classified", "suicide prevention"], "score_lsi": ["0.400526"]}
{"researcher_id": [4044], "researcher_name_in_nsf_list": "Stephanie L Carey", "researcher_paper_title_in_json_file": "Motion Analysis of Transradial Prosthesis While Lifting a Box for Development of a Biomechanical Model", "projects_cnt": 1, "year": 2008, "paper_citation": 0, "score_lda": ["0.52848"], "field": ["Simulation", "Engineering", "Biological engineering", "Surgery"], "researcher_nsf_project_abstract": ["The broader impact/commercial potential of this I-Corps project focuses on allowing a greater number of people with varying abilities to independently operate a power wheelchair. The kit developed with this project can also provide more independence by freeing up the hands from operating the wheelchair and allowing for the wheelchair to be operated within a specified range - even if the user is not seated in it.  This wireless control technology can be used to control other devices such as robots, travel scooters and adaptive sport devices. Using sensors that are already part of smart phones, and the software application including in the kit, a control device that is user friendly and cost effective is created. Expanding the concept of how humans control machines beyond a typical joystick can change the way society views wheelchairs and other mobile devices.\n\nThis I-Corps project will explore the commercialization of a hands free wheelchair control kit that can be used by power wheelchair users, including those with a spinal cord injury, multiple sclerosis, muscular dystrophy or poly trauma injuries.  The device is designed to allow power wheelchair users the ability to operate their wheelchair without the need to manipulate a joystick with their hands. A smartphone and its accelerometer sensors becomes the joystick by sending the control signals via Bluetooth to the controller of the wheelchair or any mobile device. The smartphone can be attached to the user's arm, head, chest, or remotely, to control the wheelchair. The kit is also designed to toggle between the wheelchair's original joystick controller and the customized smartphone controller using a switch. A wiring harness is used to connect the custom controller to the original controller of most wheelchair manufacturers for easy installation and operation. Preliminary work by the team has shown its capabilities as a successful control alternative to a joystick opening up wheelchair manipulation to a wider group of users."], "researcher_paper_abstract_in_json_file": "In order to design a prosthetic device, it is essential to analyze the movements of the limb that it is replacing. For improvement of prosthetic design, it is important to understand the limitations of current designs. The main purpose of this study was to evaluate the kinematic and the kinetics of transradial prosthesis users while bliaterally lifting a box, for experimental input of a biomechanical model. Creating a model for the movements of a transradial prosthesis can lead to its design improvement, selection criteria and virtual reality training.Copyright \u00a9 2008 by ASME", "paper_keywords": ["biomechanics", "prostheses"], "score_lsi": ["0.501275"]}
{"researcher_id": [9064], "researcher_name_in_nsf_list": "Wei  Wang", "researcher_paper_title_in_json_file": "The Temperature-Online-Measuring and Energy-Saving System for Drying Cylinder Based on Network Communication", "projects_cnt": 1, "year": 2013, "paper_citation": 0, "score_lda": ["0.662248"], "field": ["Structural engineering", "Simulation", "Engineering", "Engineering drawing"], "researcher_nsf_project_abstract": ["The proper function and health of an organism rests on the correct expression of it's genes: in the first step in expression, RNA molecules are produced from the genes in a number of possible forms. Accurately determining how much RNA is produced and the structure of that RNA are the goals of this research. Many experiments do high throughput sequencing of RNA to show how much gene expression is taking place, what parts of the genomic DNA are making the RNA, and how DNA regions combine to make functional RNA. There are many steps required to process RNA and get sequence data, leading to a lot of noise in the data. Errors also occur when trying to compare the RNA sequence to a genome sequence that has gaps in it or that was not correctly assembled. The effect of the noise and errors is that calculating how much of each type of RNA is present is not very accurate, which can give misleading results. The aim of this research is to develop methods that overcome the technical problems so that good quantitation and better understanding of biological processes are possible. The new algorithms will be incorporated into software packages available for use by interested members of the scientific community, so that the benefits of the improvements will be widely shared. In addition, better analysis of RNA sequencing experiments is expected to have a positive impact on many scientific disciplines, from basic cell biology to development of clinical tests.  \n\nHigh-throughput sequencing of RNA has proven itself as an invaluable tool for gene discovery and the annotation of new isoforms for both coding and non-coding genes. However, it is still falls short on its ultimate promise of providing quantitative and comparative measures of transcript abundance. This gap is due to a series of technical factors. Among them are biases introduced by employing an inexact reference genome as the standard for associating sequence data to transcripts, noise due to misalignments causes by paralogous sequence such as pseudogenes, biases introduced by unannotated transcripts, sense/antisense transcript interference, and origin bias due to aligning diploid data to a haploid model. The objective of the project is to develop methods that either overcome or side-step all of these factors in an effort to deliver on the promise of RNA sequencing for quantitative analysis. Our research plan includes developing computational models and efficient algorithms for simultaneous rebalancing reads between genes and pseudogenes and genes within gene families, robust alignment-free methods for estimating transcript abundances and allele-specific expression patterns, and de novo approach for isoform and novel transcript discovery using DNAseq and RNAseq from a single sample. The proposed computational tools will be integrated into software packages under common application framework adopted by the broad scientific community. The results of the project can be found at http://www.cs.ucla.edu/~weiwang/NSF1565137.html"], "researcher_paper_abstract_in_json_file": "During the process of drying fabrics by drying cylinder dryer, the over-drying of fabrics leads to a waste of energy. A temperature-online-measuring and energy-saving system for drying cylinder dryer based on network communication was designed in the study. The temperature-measuring and steam-controlling part of the system set in the cylinder dryer communicates with the remote monitoring part using the W5100 network chip through Ethernet which helps to achieve a high-speed and long-distance communication between the two parts. The remote monitoring part of the system is based on Delphi. The real-time temperature of fabrics in each drying cylinder can be displayed on the screen of the remote computer, and the steam supply of drying cylinders can be manually or automatically adjusted according to the mutations of temperature of fabrics by system administrators or the remote computer. The application of this system helps to avoid the The evaporation of water due to over-drying by 5.3%-6.6% and result in a reduction of the waste of energy by about 10%.", "paper_keywords": ["network communication", "on line technology", "drying cylinder dryer"], "score_lsi": ["0.275001"]}
{"researcher_id": [3100, 6800], "researcher_name_in_nsf_list": "Xiaodong  Zhang", "researcher_paper_title_in_json_file": "Profit-effective parallel computing", "projects_cnt": 2, "year": 1999, "paper_citation": 50, "score_lda": ["0.579993", "0.874967"], "field": ["Parallel processing", "Parallel computing", "Simulation", "Cost-effectiveness analysis", "Speedup", "Computer Science", "Cost\u2013benefit analysis", "Distributed computing", "Profitability index", "Cost efficiency"], "researcher_nsf_project_abstract": ["This project will provide support for students to attend the 36th IEEE International Conference on Distributed Computing Systems (ICDCS 2016) to be held in Nara, Japan, June 27-30, 2016. Conference experience is critical for training undergraduate and graduate students to be well prepared for the computing workforce of the future. ICDCS is a premier conference and has a wide coverage of topics in distributed computing. The conference has made significant impacts to the computing community since it was founded in 1979. The selected papers have influenced the design and implementation of many distributed computing systems, software and applications today. Participation in such a premium conference provides an excellent opportunity for students and junior faculty members to learn the latest advances in distributed computing and interact with peers and world-renowned researchers. The availability of travel grants will be announced widely and the students will be asked to provide a document detailing how the participation will benefit them.", "Solid-state data storage built upon NAND flash memory is fundamentally changing the memory and storage hierarchy for virtually the entire information technology infrastructure. Nevertheless, there have been several fundamental and challenging issues to be addressed before the industry can explore the flash memory to its full potential. First, as flash memory technology scales down, its reliability degradation approaches to an alarming level, leading to serious concerns and skepticism of storage system architects and users in many applications. Second, system and application development of solid-state storage has been independently conducted, resulting in isolation, duplicated operations, and an inefficient management among these layers. Due to the technology scaling and information loss in existing simple interface with storage devices, flash memory has not been efficiently and reliably utilized in practice, and the situation will become worse with the technology scaling. The PIs of this project will apply a holistic system design methodology to cohesively address the challenges preventing wider adoption of flash memory. By innovating well-orchestrated cross-layer information sharing and utilization, this design methodology enables seamless utilization of system-level workload and physical-level device characteristics across the entire software/hardware stack without complicating overall system design. An integrated software and hardware prototyping infrastructure will be developed to demonstrate the potential using major and widely used software systems, such as Hadoop, virtual machines, and database. This project will achieve a high broader impact by transforming basic research results into storage systems, and by training both undergraduate and graduate students with research activities, and by timely integrating new research results to classrooms.\n\nSpecifically, this project will carry out several closely related tasks: (1) It will develop techniques that can learn and predict the varying characteristics and their correlations of individual flash memory devices. This will provide run-time information that makes it possible to optimize the use of flash memory for alleviating the reliability crisis and adapting to varying system-level workload characteristics. (2) It will develop techniques that enable critical information exchange across the storage hierarchy in order to facilitate cross-layer information sharing. (3) It will further develop a set of techniques across the design hierarchy that can effectively utilize these runtime collections and predictions to improve the overall system reliability and performance."], "researcher_paper_abstract_in_json_file": "Researchers widely use speedup, efficiency, and scalability to assess parallel computing performance. These metrics encourage researchers to use any novel technique to design or improve a parallel system, without paying enough attention to the cost increase that such a technique incurs. However, as national-defense applications are downsizing, commercial applications are the dominant users of parallel systems. Customers and vendors are particularly concerned with whether a parallel system can make a profit. Our major goal is to investigate financially justified parallel computing. To evaluate parallel computing's effectiveness, we use a simple profitup metric to measure how performance, cost, and business production affect profit. We focus on investigating the relationship between cost-effective parallel computing and profit-effective parallel computing.", "paper_keywords": ["performance evaluation parallel processing cost benefit analysis", "performance evaluation", "parallel processing concurrent computing production systems educational institutions business high performance computing equations cost function performance analysis velocity measurement", "parallel systems", "parallel computer", "cost effectiveness", "profitability", "cost effective parallel computing profit effective parallel computing speedup efficiency scalability parallel computing performance cost profitup metric business production", "cost benefit analysis", "parallel processing"], "score_lsi": ["0.519064", "0.795496"]}
{"researcher_id": [11615], "researcher_name_in_nsf_list": "WEI  LIU", "researcher_paper_title_in_json_file": "Numerical simulations of the phase separation properties for the thermal aged CDSS with Phase Field Model", "projects_cnt": 1, "year": 2011, "paper_citation": 50, "score_lda": ["0.528368"], "field": ["Computer simulation", "Simulation", "Nuclear physics", "Finite element method", "Forensic engineering", "Physics"], "researcher_nsf_project_abstract": ["QFPS ARE IMPORTANT BECAUSE THEY ARE CORRELATED WITH QUASI-PERIODIC PULSATIONS OF SOLAR FLARES, WHICH ARE MAJOR\\nMANIFESTATIONS OF SOLAR ACTIVITY AND DRIVERS OF SPACE-WEATHER DISTURBANCES. QFPS CAN PROVIDE CRITICAL NEW CLUES TO FLARE ENERGY RELEASE, A LONG"], "researcher_paper_abstract_in_json_file": "Abstract   Experiments and numerical simulations with Phase Field Model and Finite Element Analysis were carried out to investigate the phase separation dynamic properties and the corresponding thermal aging degradation mechanism. Experimental results from transmission electron microscopy and atomic force microscopy show that thermal aging causes the Cr-rich phase precipitate and form clusters. A phase field dynamic model was developed with constitutive relations and empirical potential functions to investigate the phase separation dynamics in the ferrite phase. Numerical results integrated with cell dynamical system method show clearly the micro structure morphology and the phase separation coarsening with aging time. The evolution process of the phase separation was quantitatively illustrated and reproduced macroscopically. The scattering pattern becomes clearer and the corresponding radius becomes smaller along with the increasing aging time. The average characteristic length increases firstly then decreases and enters a more stable stage. With the increment of the local Cr concentration, the evolution of the phase morphology was quite different. Finite Element Analysis simulation results with the Gurson\u2013Tvergaard\u2013Needleman void model show that the damage initiated more easily in the ferrite matrix for the Cr atoms forming clusters with increasing aging time. The phenomenological simulations with Phase Field Model and Finite Element Analysis were in remarkably good agreement with experimental results and analytical considerations.", "paper_keywords": ["modelo dinamico", "thermal ageing", "separacion termica", "phenomenological model", "separation thermique", "concentration effect", "modele phenomenologique", "nuclear reactor", "methode element fini", "metodo elemento finito", "thermal properties", "reacteur nucleaire", "simulacion numerica", "modelo fenomenologico", "dynamic model", "endommagement", "efecto concentracion", "finite element method", "vieillissement thermique", "deterioracion", "transmission electron microscopy", "phase field", "modele dynamique", "simulation numerique", "phase field model", "modele simulation", "dynamical systems method", "propriete thermique", "finite element analysis", "atomic force microscopy", "effet concentration", "constitutive relation", "modelo simulacion", "damaging", "reactor nuclear", "potential function", "thermal separation", "propiedad termica", "simulation model", "phase separation", "envejecimiento termico", "dynamic properties", "numerical simulation"], "score_lsi": ["0.22227"]}
{"researcher_id": [3267], "researcher_name_in_nsf_list": "GLEN  STEWART", "researcher_paper_title_in_json_file": "Perimetrically tensioned flexible signage mount", "projects_cnt": 1, "year": 2008, "paper_citation": 0, "score_lda": ["0.243208"], "field": ["Structural engineering", "Simulation", "Engineering", "Advertising"], "researcher_nsf_project_abstract": ["INTRODUCTION: CURRENT MODELS OF PERTURBED PLANETARY RINGS ARE BASED UPON THE STREAMLINE MODEL OF BORDERIES, GOLDREICH, AND TREMAINE (1983) WHERE THE RING IS DIVIDED INTO A COLLECTION OF ELLIPTICAL WIRES THAT INTERACT WITH ONE ANOTHER VIA GRAVITY AND VISCO"], "researcher_paper_abstract_in_json_file": "The invention provides systems for removably attaching flexible signage to transportable containers, including systems adapted for attaching advertising banners to truck trailers. An aspect of the invention is a support line length adjustment mechanism adapted to be actuated to alternatively shorten and tension a signage sheet support line, and to lengthen and thereby untension the support line, to release the support line and the signage from the container.", "paper_keywords": "", "score_lsi": ["0.211174"]}
{"researcher_id": [2368], "researcher_name_in_nsf_list": "YANG  LIU", "researcher_paper_title_in_json_file": "Performance-based haulage management system", "projects_cnt": 1, "year": 2007, "paper_citation": 0, "score_lda": ["0.596851"], "field": ["Simulation", "Engineering", "Operations management", "Engineering drawing"], "researcher_nsf_project_abstract": ["NUMEROUS EPIDEMIOLOGICAL STUDIES LINKS AMBIENT AIR POLLUTION TO EXCESS MORBIDITY AND MORTALITY. HISTORICALLY THESE STUDIES RELIED ON GROUND MONITORING STATIONS, SUCH AS THE U.S. EPA REGULATORY MONITORING NETWORK, TO ESTIMATE POPULATION EXPOSURE. THE GEOGR"], "researcher_paper_abstract_in_json_file": "A method for managing haul routes in work environments comprises collecting performance data associated with a machine operating in a work environment. The method also includes determining a drive axle torque of the machine and estimating a total effective grade associated with the machine based on the drive axle torque. The estimated total effective grade is compared with a threshold level and, if the estimated total effective grade exceeds the threshold level, a design performance of the machine may be simulated based on the calculated total effective grade. Design performance data is compared with the collected performance data, and a payload limit of the machine is adjusted if the design performance is inconsistent with collected performance data.", "paper_keywords": "", "score_lsi": ["0.317828"]}
{"researcher_id": [12035], "researcher_name_in_nsf_list": "Neil F Johnson", "researcher_paper_title_in_json_file": "Crowd-Anticrowd Theory of Collective Dynamics in Competitive, Multi-Agent Populations and Networks", "projects_cnt": 1, "year": 2003, "paper_citation": 50, "score_lda": ["0.952253"], "field": ["Complex systems", "Simulation", "Computer Science", "Artificial intelligence", "Machine learning", "Collective behavior", "Artificial neural network"], "researcher_nsf_project_abstract": ["Cyberphysical (CPS) systems are set to become ever faster, driven by technological advances that push them toward speed limits set by fundamental physics. This proposal addresses the need for a theory of the dynamical behavior of CPS systems in the sub-second regime beyond human intervention times. Ultrafast instabilities have already been observed in such systems. The theory will allow for networking at multiple scales, coupling across multiple temporal and spatial scales, imperfect network communications and sensors, as well as adaptive reorganization and reconfiguration of the system. Theoretical findings will be checked against available empirical data, e.g., from the decentralized network of autonomous market exchanges with its mandated sensor systems. The project will inform the extent to which instabilities can build up across timescales, potentially threatening CPS system stability on a global level.\n\nThe project goal is a theoretical description of the dynamics in decentralized networks of semi-autonomous machines in which an ecology of algorithms, sensors and network links may be operating, adapting and even competing in response to external inputs. Attention will be paid to the regime of sub-second behavior where human intervention becomes impossible in real-time. The availability of data from such a system provides a test-bed for the multi-agent, complex network analyses to be developed. The project will address how instabilities can be mitigated and eventually controlled. The results are set to advance understanding of CPS system dynamics, not only among academics but also practitioners and regulatory bodies. Application areas that pervade modern life include market exchange systems, resource-allocation systems and remote sensing systems. Opportunities exist to integrate research and education concerning CPS applications across graduate and undergraduate classrooms, outreach through publications, and participation in K-12 activities."], "researcher_paper_abstract_in_json_file": "We discuss a crowd-based theory for describing the collective behavior in a generic multi-agent population which is competing for a limited resource. These systems -- whose binary versions we refer to as B-A-R (Binary Agent Resource) collectives -- have a dynamical evolution which is determined by the aggregate action of the heterogeneous, adaptive agent population. Accounting for the strong correlations between agents' strategies, yields an accurate description of the system's dynamics in terms of a 'Crowd-Anticrowd' theory. This theory can incorporate the effects of an underlying network within the population. Most importantly, its applicability is not just limited to the El Farol Problem and the Minority Game. Indeed, the Crowd-Anticrowd theory offers a powerful approach to tackling the dynamical behavior of a wide class of agent-based Complex Systems, across a range of disciplines. With this in mind, the present working paper is written for a general multi-disciplinary audience within the Complex Systems community.", "paper_keywords": ["agent based", "collective behavior", "complex system", "minority game", "disordered system", "neural network", "dynamic behavior"], "score_lsi": ["0.7359"]}
{"researcher_id": [9124], "researcher_name_in_nsf_list": "Tissa H Illangasekare", "researcher_paper_title_in_json_file": "Determination of DNAPL entrapment architecture using experimentally validated numerical codes and inverse modeling", "projects_cnt": 1, "year": 2004, "paper_citation": 50, "score_lda": ["0.513335"], "field": ["Simulation", "Chemistry", "Hydrology", "Analytical chemistry"], "researcher_nsf_project_abstract": ["The workshop will examine the environmental controls on kidney disease in Sri Lanka and posit hypotheses on how these factors interact in the specific hydrologic, geochemical and social environmental settings to produce the current disease crisis. The workshop will take a multidisciplinary team of scientists from the US to Sri Lanka for a week long site field visit to the affected areas in the north central parts of Sri Lanka and gather firsthand information on the epidemic and its environmental context. They will meet with local researchers in universities and other governmental and non-governmental agencies to study and assess the problem. They will conduct two workshops with local experts. The team members will be selected to represent multiple areas of expertise including environmental sciences and engineering, surface and subsurface hydrology, soil science, geochemistry, behavioral sciences and environmental psychology,  geographic information systems, medicine and public health,. The primary goal is to provide expertise on the assessment of the factors that contribute to the epidemic. Having assessed the problem from a broad perspective and discussed the various environmental interactions that could lead to the present set of conditions, the various disciplines will then be well posed to posit research project to multiple agencies to address those parts of the problems amenable to each agency.  This project is jointly funded by GEO/EAR, ENG/CBET and OD/OISE.\n\nThis workshop will examine the complex environmental processes of hydrology, engineering and agricultural production of Sri Lanka that appear to contribute to the occurrence of widespread kidney disease. There is the consensus that this is a multi-factorial disease related to chronic exposure to arsenic, cadmium and nephrotoxic pesticides. Additional risk factors such as drinking water contamination in shallow wells and the use of pesticides and contaminated triple phosphate fertilizer have also been identified."], "researcher_paper_abstract_in_json_file": "The presence of dense non-aqueous phase liquid (DNAPL) in source zones in aquifers generates continuous mass flux long after the initial spill. It is our hypothesis that observed dissolved concentration in a monitoring well downstream of the source zone will provide inadequate and often misleading information on the entrapment architecture needed to design effective remediation schemes. An inverse modeling study was conducted to evaluate what additional information is needed to determine entrapment architecture. Three synthetic entrapment architectures generated by a multi-phase flow model UTCHEM in a spatially correlated random field were used in the analysis. The three selected from eighty realizations of the random field represent conditions that produce DNAPL pools, zones of residual DNAPL saturation and a combination of pools and residuals, respectively. For each of these three entrapment architectures, a data set of downstream concentration and solute mass flux was generated using a laboratory validated dissolution model based on MODFLOW and RT3D. An inverse modeling algorithm (PEST) was used to back-calculate the saturation distribution of DNAPL in the source zone. The inverse solution (i.e. DNAPL saturation distribution defining the architecture) was found to be non-unique when only concentration data was used. However when mass flux data that combines concentration and water flow, was added as an additional observation, the inverse problem converged rapidly to a unique solution. Further analysis to determine optimal monitoring strategies showed that the mass flux-matching technique could be used to determine entrapment architecture with some limitations. The technique becomes less accurate in terms of both total mass estimation and the ability to resolve the vertical distribution of DNAPL, when a source zone contains more pools than residuals. It was also found that with larger number of observations in the vertical direction (multi-level sampling points), the predictions become more accurate. The distance the monitoring well was placed downstream of the source zone affected the accuracy of prediction, but the estimate of the total mass entrapment was not affected. This is suspected to be an artifact of the two-dimensional test system that was used in this hypothetical analysis. More rigorous analysis using realistic three-dimensional systems is needed to make more definitive conclusions.", "paper_keywords": ["inverse modeling", "experimental validation"], "score_lsi": ["0.371104"]}
{"researcher_id": [9681], "researcher_name_in_nsf_list": "Hartmut  Schneider", "researcher_paper_title_in_json_file": "Method and apparatus for detecting the respiratory activity of a person", "projects_cnt": 1, "year": 2006, "paper_citation": 0, "score_lda": ["0.854087"], "field": ["Control engineering", "Simulation", "Engineering", "Communication"], "researcher_nsf_project_abstract": ["More than 50% of patients with asthma and Chronic Obstructive Lung disease (COPD) fail to use their inhaled medication, which is the most recognized obstacle for controlling the health and economic burden of asthma and COPD. Current methods to improve medication use are ineffective, too cumbersome to use and expensive for widespread adoption. This I-Corps team has developed an inexpensive, yet accurate flow sensor integrated with existing inhaler devices and paired via Bluetooth technology to a smartphone; thereby, providing a monitoring and management tool for asthma and COPD. It provides substantial value for patients and healthcare providers as it streamlines data and disease management and makes COPD treatment more affordable and sustainable for both, individual patients and healthcare providers.\n\nDuring the I-Corps program, this team will develop a viable business plan and validate market assumptions. To accomplish this goal, the team will conduct market research to understand the spectrum of needs of its primary customers which are disease management entities. The team anticipates conducting 100 interviews in approximately 22 to 30 disease management entities. At the end of the program, the team expects to have a detailed understanding of the needs of potential customers and who would be the most likely first customer segment. The team also expects to have a detailed description of how its minimum viable product should be designed to meet the needs of its customer. With this knowledge the team hopes to expedite the technical and analytical development that will go into an FDA ready device. Further this parallel path of customer development will provide clear evidence for investors to evaluate the addressable market and market size for the proposed product."], "researcher_paper_abstract_in_json_file": "A method and a device are provided for detecting the respiratory activity of a person and for controlling the time progression of breathing gas pressure, especially in accordance with physical parameters and considering parameters indicating the momentary physiological condition of the breathing person. The device for detecting the respiratory activity of a person has at least one sensor that provides a first signal indicating the breathing gas flow, wherein at least one signal processing device is provided for processing the first signal. The signal processing device is configured in such a way that said device determines a reference-relation on the basis of the first signal detected during a first time interval. On the basis thereof, the device determines a correlation-relation between the reference-relation and the first signal. The device generates an output signal indicating the respiratory activity and/or the physiological condition of the breathing person by considering at least the correlation-relation.", "paper_keywords": "", "score_lsi": ["0.528726"]}
{"researcher_id": [4860], "researcher_name_in_nsf_list": "Ahmed A Shabana", "researcher_paper_title_in_json_file": "Review of Soil Models and Their Implementation in Multibody System Algorithms", "projects_cnt": 1, "year": 2012, "paper_citation": 0, "score_lda": ["0.831907"], "field": ["Structural engineering", "Simulation", "Engineering", "Geotechnical engineering"], "researcher_nsf_project_abstract": ["Liquid sloshing has a significant impact on the dynamics and stability of highway, rail, marine, and space transportation systems. Nonetheless, because of the lack of computational algorithms and high fidelity models necessary to develop the knowledge and better understanding of the sloshing effect on vehicle dynamics and stability, transportation of hazardous materials has resulted in deadly, costly, and environmentally damaging accidents. Liquid can produce significant forces as the results of the interaction with containers and/or tank cars mounted on trucks, aircraft, railroad vehicles, missiles, rockets, offshore structures, and marine applications among many others. Existing simple liquid sloshing approaches commonly used are not designed for complex motion scenarios. While the use of computational algorithms is necessary to predict the response of detailed vehicle models, there is no in existence today an efficient and accurate continuum-based liquid sloshing approach to quantify the sloshing effect which can be significant during curve negotiations, rapid lane changes, and acceleration and braking scenarios. This award supports fundamental research to develop the first generation of liquid sloshing computational algorithms for modeling complex motion scenarios, developing safety guidelines, and accurate accident reconstructions. In particular, hazardous material accidents, which result in significant economic loss, environmental contamination, loss of lives, and property damage, can be avoided or significantly reduced through better understanding of the effect of sloshing on highway, rail, marine, and space transportation systems. This project will also lead to the development of new knowledge, approaches, and tools that can be used in the education of senior undergraduate and graduate students at University of Illinois at Chicago which is designated as a minority serving institution. \n\nThe use of efficient continuum-based liquid sloshing models that capture the liquid distributed inertia and viscosity can overcome the limitations of existing liquid sloshing models. To this end, new liquid sloshing models will be developed and integrated with multibody system (MBS) algorithms that allow for developing and solving the differential/algebraic equations of vehicle models that include significant details. In order to achieve efficient integration of liquid sloshing/MBS algorithms and avoid the difficulties of integrating existing Eulerian-based fluid formulations with the Lagrangian-based MBS algorithms, new total-Lagrangian non-incremental liquid sloshing solution procedures will be introduced. Three different liquid sloshing approaches will be developed; the finite element (FE) absolute nodal coordinate formulation (ANCF) will be used to develop high fidelity MBS models, the floating frame of reference (FFR) formulation will be used to develop low-order MBS models, and the smoothed particle hydrodynamics (SPH) method will be used to develop a different model to assess the effect of the liquid turbulence on vehicle dynamics. The Lagrangian approaches will allow for accurately capturing the effect of the fluid inertia in complex vehicle motion scenarios."], "researcher_paper_abstract_in_json_file": "Abstract : The mechanical behavior of soils may be approximated using different models that depend on particular soil characteristics and simplifying assumptions. For this reason, researchers have proposed and expounded upon a large number of constitutive models and approaches that describe various aspects of soil behavior. However, there are few material models capable of predicting the behavior of soils for engineering applications and are at the same time appropriate for implementation into finite element (FE) and multibody system (MBS) algorithms. This paper presents a survey of different commonly used terramechanics and continuum-based soil models. The aim is to provide a summary of soil models, compare them, and examine their suitability for integration with large-displacement FE absolute nodal coordinate formulation (ANCF) and MBS algorithms. Special emphasis is placed on the formulations of soils used in conjunction with vehicle dynamic models. A brief review of computer software used for soil modeling is provided and the implementation of these soil models in MBS algorithms used in the analysis of complex vehicle systems is discussed.", "paper_keywords": ["nodes", "paper", "soil models", "mechanical properties", "soils", "soil mechanics", "algorithms", "finite element analysis"], "score_lsi": ["0.69036"]}
{"researcher_id": [1811], "researcher_name_in_nsf_list": "Jaime G Carbonell", "researcher_paper_title_in_json_file": "Improving Interactive Capabilities in Computer-Assisted Instruction", "projects_cnt": 1, "year": 1973, "paper_citation": 50, "score_lda": ["0.973014"], "field": ["Simulation", "Computer Science", "Multimedia"], "researcher_nsf_project_abstract": ["Whom do you ask when you don't know whom to ask? That may be considered a rhetorical question in some contexts, but it is the \"raison d'\u00eatre\" for referral networks. If a person must address a problem, but lacks the knowledge of how to solve it, he or she asks someone who may either provide a solution, or may know someone else who might provide the solution. Referral networks are very useful for professional success, such as in consulting companies, health-care organizations (e.g. referral of patients to medical specialists) or interdisciplinary research endeavors.  The advent of AI-based intelligent agents, who typically have narrow expertise, enables the creation of agent-based or mixed human-and-agent referral networks, but adds complexity to the referral process.  In order to tame this complexity, the new research addresses learning to refer in a distributed setting.  Each expert learns to better estimate the expertise of other experts in the network, whether human or AI-based agents, and thus overall network refers with increasing accuracy. The learning-to-refer methods are robust with respect to gradual expertise change (e.g. experts learn to perform better) or changes in the network (e.g. an experienced expert retires and/or one or more new but less experienced experts join).\n\nThe research starts by modifying methods from reinforcement learning, such as the successful interval-estimation learning approach, extending them to the distributed referral-network setting. Preliminary results show that distributed interval threshold learning is effective in improving the accuracy of referrals with accrued experienced, and performs better than other approaches such as Q-learning or greedy selection of best-known expert.  The research will address issues of robustness to changes in the referral network topology, benefits of informative priors and proactive skill advertisement by individual experts to their peers, and other related aspects relaxing the initial restrictive assumptions in order to address real referral-network scenarios. In addition to establishing this new line of distributed learning, this EAGER will generate data sets useful for further research in the area of expertise-network learning and make them available."], "researcher_paper_abstract_in_json_file": "Abstract : A report is made on the development of interactive capabilities in the SCHOLAR CAI system. Tutorial mode is based on extensive analysis of dialogues between different tutors and students, performed earlier. In this mode SCHOLAR first questions the student to find out what he knows about each topic, and then presents some related information limited to what the student can assimilate. Block-test mode is based on the strategy used in programmed instruction. In this mode SCHOLAR first presents information and then asks questions about the information presented.", "paper_keywords": ["computers", "interfaces", "programmed instruction", "data transmission systems", "feedback", "adaptive systems", "test methods", "man machine systems", "teaching methods", "students"], "score_lsi": ["0.310851"]}
{"researcher_id": [6782, 6783], "researcher_name_in_nsf_list": "Andrew  Schotter", "researcher_paper_title_in_json_file": "Workaholics and Drop Outs in Optimal Organizations", "projects_cnt": 2, "year": 2007, "paper_citation": 0, "score_lda": ["0.946629", "0.924351"], "field": ["Simulation", "Engineering", "Operations management", "Social psychology"], "researcher_nsf_project_abstract": ["Many economic and social relationships such as employment contracts, membership of voluntary organizations, international alliances, friendships and marriages feature voluntary separation: each participant can unilaterally and voluntarily exit the relationship. On the other hand, voluntary separation often makes it hard to build trust and long-term cooperation. Voluntary separation does not only change how business or social relationships end, it also governs the nature of relationships from the very beginning. Its effect on the level of cooperation will have a profound implication for economic growth, social cohesion and subjective well-being of individuals.\n\nThis study experimentally investigates how voluntary separation affects cooperation. Cooperative behavior of subjects will be compared to theoretical predictions. Different social mechanisms will be tested for their effectiveness in inducing cooperation, which could help policy makers or community designers in their choice of regulatory options.\n\nIn the experiment, subjects will interact repeatedly in a computer laboratory setting under costly and costless separation. The baseline treatment will not be allowing any separation at all. Theoretical studies on this class of strategic interactions yield predictions based on gradual trust building. In the equilibria they find, long-term cooperation is achieved by mutually defecting in the beginning. This is a highly counter-intuitive way of building trust. There is very little empirical/experimental evidence to support it. This study aims to fill the gap in the literature by answering the following questions: Under what circumstances do people cooperate? Does voluntary separation help or hurt cooperation? How do people achieve cooperation? \nTo test the theoretical predictions, subjects' cooperative behavior will be compared across treatments. By comparing separation treatments, we can also find out which social mechanisms effectively facilitate cooperation. Finally, potential teaching/learning patterns and persistently heterogeneous behavior among subjects can shed light on formation of norms and socialization.", "Auctions are an important mechanism used to allocated goods and raise revenue. Oil-lease and spectrum auctions raise large amounts of revenue for the government and have been the focus of much economic research. Many of these auctions are for goods which have an uncertain value which is the same for all bidders (e.g., the amount of oil in a lease). Traditional theoretical models assume that bidders have the same amount of information concerning the value of the good. However, this assumption ignores the fact that often bidders may be asymmetrically informed, which has been empirically shown to affect bidding behavior. Understanding how asymmetries in bidders' information affects their bidding behavior is important for predicting the revenue an auction will generate and for designing auctions to increase the revenue they raise. \n\nThe PIs expand the traditional auction framework to allow for bidders to differ both in their estimates of a good's value and the precision of their estimates. They study how bidding behavior changes when bidders can and cannot observe the precision of opposing bidders' information. In many sealed-bid or online auctions, the identity of opposing bidders is hidden and each bidder may not be perfectly informed about how precise his opponent's information is. With the rise of internet auctions, however, sellers have a large degree of control of the information bidders possess about one another. How much information about bidders' characteristics an auction designer should release is an important consideration when designing auctions and is one that has not previously been studied. The current research project shows how sellers can optimally design the release of this information. By studying the impact of changing how much information bidders possess about one another, the PIs find a new avenue by which auction design can improve revenue generation. \n\nThe theoretical predictions show that publicly revealing the precision of each bidder's information decreases the expected revenue an auction will generate. This finding suggests that controlling how much information bidders have about each other is an important factor to consider when designing auctions. \nThe PIs seek to answer the following question by performing a series of experiments: how do bidders behave when they know that their information about the good is more (or less) precise than other bidders? How do their bidding strategies change when they do not know the precision of their opponents information? The proposed study intends to answer these questions, which will contribute to a better understanding of how much information to disclose in auctions. This experiment will provide a better understanding of how to implement information design into auction formats."], "researcher_paper_abstract_in_json_file": "This paper reports the results of experiments designed to test the theory of the optimal composition of prizes in contests. We find that while in the aggregate the behavior of our subjects is consistent with that predicted by the theory, such aggregate results mask an unexpected compositional effect on the individual level. While theory predicts that subject efforts are continuous and increasing functions of ability, the actual efforts of our laboratory subjects bifurcate. Low ability workers drop out and exert little or no effort while high ability subjects try too hard. This discontinuity, which is masked by aggregation, has significant consequences for behavior in organizations.", "paper_keywords": ["all pay auctions", "contests", "experiments"], "score_lsi": ["0.776509", "0.72175"]}
{"researcher_id": [12680], "researcher_name_in_nsf_list": "Rastislav  Bodik", "researcher_paper_title_in_json_file": "Software Synthesis (Dagstuhl Seminar 12152)", "projects_cnt": 1, "year": 2012, "paper_citation": 0, "score_lda": ["0.944677"], "field": ["Simulation", "Computer Science", "Software Engineering"], "researcher_nsf_project_abstract": ["Title: Student Travel Support for 43rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL) 2016\n\nThis award supports student travel to the 2016 ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL). The conference takes place in St. Petersburg, Florida, USA, in January 2016. POPL is a forum for the discussion of leading work on all aspects of programming languages and programming systems. The conference has a rich tradition of supporting ideas that have made enduring contributions to the theory, design, implementation or application of programming languages. Supporting student travel to attend professional conferences and workshops is a very important mission of the NSF. The broader significance and importance includes supporting travel for women and underrepresented minorities, thereby fostering a diverse pool of next generation researchers in this important research area. In particular, students have the opportunity to learn state-of-the-art methodologies, be exposed to novel techniques, and interact with senior researchers in their areas of expertise."], "researcher_paper_abstract_in_json_file": "This report documents the program and the outcomes of Dagstuhl Seminar 12152 ``Software Synthesis''. During the seminar, several participants presented#R##N#their current research, ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar are put together in this paper.#R##N##R##N#The rise of multiprocesser computers and of software verification as applied#R##N#in industry combine to create an opportune moment for software synthesis.#R##N#To facilitate research in this area, research from several fields of Computer#R##N#Science presented tutorials on techniques they developed. This lead to (1) the definition of what challenges synthesis has to tackle in the future and (2) insights into how the several fields of synthesis are related.#R##N##R##N#Finally, several groups described their experience with teaching synthesis#R##N#to graduate and undergraduate students, demonstrating that synthesis is challenging for students but that they can also rise to the challenge and enjoy the field.", "paper_keywords": ["004", "compiler optimization", "software synthesis verification and model checking theorem proving program analysis programming by demonstration program derivation"], "score_lsi": ["0.273805"]}
{"researcher_id": [6506], "researcher_name_in_nsf_list": "Benjamin Mako  Hill", "researcher_paper_title_in_json_file": "Scratch Community Blocks: Supporting Children as Data Scientists", "projects_cnt": 1, "year": 2017, "paper_citation": 50, "score_lda": ["0.942974"], "field": ["Simulation", "Human\u2013computer interaction", "Computer Science", "Multimedia", "World Wide Web"], "researcher_nsf_project_abstract": ["This research project seeks to understand the factors that encourage success in computer-supported peer production - the form of online collaborative organization used to create public information goods like Wikipedia and Linux.  Why do some peer production systems mobilize large communities of contributors and create valuable information goods while most do not?  One answer for this challenging question is that success-related factors may change significantly as a collaborative organization grows, such that conditions that encouraged explosive growth in the beginning may prevent further growth later on. This work will provide actionable insights for initiators and managers of online collaborative organizations, informing the design and management of distributed collaboration across different topic domains at different stages of project development. It will also produce freely licensed and publicly available computational research systems and datasets that will enable reproducible research and the dissemination of the new techniques developed by the research.  Peer production and related forms of online collaboration in virtual communities have diffused widely in software production, knowledge management, cultural production, and education. Another sign of its significance is the fact that a growing number of organizations look to distributed collaboration managed through virtual and volunteer communities as a source of innovation and customer support.\n\nThis research uses longitudinal comparative analysis of populations of peer production communities to elaborate a novel and transformative science of pathways to effective collaborative organization. In doing so, it will extend the rich traditions of sociotechnical systems research and organization science on these topics.  This empirical work will explore three central facets of peer production: (1) the relationship between participation equality and growth; (2) the extent to which community effectiveness is limited by competition for volunteer resources; and (3) the role of social interaction and coordination in productive collaboration. In every case, empirical predictions will be developed from prior work and tested using trace data from a large population of peer production wikis. The research will then explore how the observed relationships may diminish or even reverse as communities grow. The findings will become the basis for a broader theory of collaborative organization that explains how key drivers of mobilization in nascent groups differ systematically from those in established communities."], "researcher_paper_abstract_in_json_file": "In this paper, we present Scratch Community Blocks, a new system that enables children to programmatically access, analyze, and visualize data about their participation in Scratch, an online community for learning computer programming. At its core, our approach involves a shift in who analyzes data: from adult data scientists to young learners themselves. We first introduce the goals and design of the system and then demonstrate it by describing example projects that illustrate its functionality. Next, we show through a series of case studies how the system engages children in not only representing data and answering questions with data but also in self-reflection about their own learning and participation.", "paper_keywords": "", "score_lsi": ["0.544638"]}
{"researcher_id": [5034], "researcher_name_in_nsf_list": "Wei  Wei", "researcher_paper_title_in_json_file": "A method for monitoring the managed devices", "projects_cnt": 1, "year": 2005, "paper_citation": 0, "score_lda": ["0.658997"], "field": ["Real-time computing", "Simulation", "Engineering", "Operations management"], "researcher_nsf_project_abstract": ["Understanding microbe-plant interactions is crucial to combating agriculturally important plant diseases and improving plant production. Agrobacterium species cause crown gall disease on over 300 plant genera, including many economically important crops. Current control methods for crown gall disease are not very effective, resulting in up to 80% infection and loss in nursery plants. Alternatively, Agrobacterium-mediated transformation is a powerful tool for creating transgenic plants with increased agricultural productivity. But many crops are recalcitrant to transformation. This award supports research that will employ novel methods developed by Professor Sanjay Swarup?s lab at the National University of Singapore (NUS) to look at the community composition of the rhizosphere microbiomes of transgenic and untreated A. thaliana using 16S rRNA metagenomic analysis. Understanding how an Agrobacterium virulence protein manipulates plant response to increase infection can help improve prevention and control strategies for crown gall disease, as well as create better plant transformation technologies for making useful transgenic crops. \n\nBacterial and plant metabolites are abundant in the rhizosphere environment and modulate plant-microbe interactions1. I propose to investigate the changes in root exudate metabolites of transgenic Arabidopsis thaliana expressing an Agrobacterium rhizogenes effector protein, GALLS-CT, from a betaestradiol inducible promoter2, and assay if expression of this protein alters the plant rhizosphere microbiome. The GALLS-CT protein is transferred from Agrobacterium into the plant3 and alters host defense gene expression to enhance infection (unpublished data). I will employ novel methods developed by Professor Sanjay Swarup?s lab at the National University of Singapore (NUS) to look at the community composition of the rhizosphere microbiomes of transgenic and untreated A. thaliana using 16S rRNA metagenomic analysis. The goal is to observe changes in the microbial community that might be caused by GALLS-CT expression. I will also collect root exudates from induced GALL-CT expressing plants and untreated plants and perform metabolomics analysis using ultra high-pressure liquid chromatography-mass spectrometry (UHPLC-MS) to look for differences in metabolite profiles that might be caused by GALLS-CT.\n\nThis award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the National Research Foundation of Singapore."], "researcher_paper_abstract_in_json_file": "A method for monitoring the managed devices comprises that the manage center preserves the integrality list in advance, which includes the system integrality values of the managed devices and the corresponding relations of the managed devices and the system integrality values of themselves, and the managed device gathers the current system integrality value of itself and saves it when it starts; the managed device sends the information including the current system integrality value to the manage center after receiving the monitor command from the manage center; the manage center determines whether the received current system integrality value of the managed device coincides with the integrality value of the managed device saved by itself according to the received information and said integrality list, and implements the alert process when they do not coincide with each other. The manage center can know whether the managed device is believable currently so that the manage center can determine whether the unknown attack to the managed device exists or not according to the present invention.", "paper_keywords": "", "score_lsi": ["0.213997"]}
{"researcher_id": [420], "researcher_name_in_nsf_list": "Tian  Lan", "researcher_paper_title_in_json_file": "Rethink energy accounting with cooperative game theory", "projects_cnt": 1, "year": 2014, "paper_citation": 50, "score_lda": ["0.925795"], "field": ["Real-time computing", "Simulation", "Throughput accounting", "Accounting method", "Energy accounting", "Cost accounting", "Energy management"], "researcher_nsf_project_abstract": ["This project aims to develop an analytical framework that quantifies tail latency and reliability of erasure-coded storage through investigation of novel scheduling and repair strategies, mandating rethinking of erasure codes for online storage. As erasure coding is increasingly adopted by large-scale storage systems such as Microsoft Azure and Facebook, conventional approaches that primarily rely on system design heuristics has become inadequate in pushing the performance boundaries in terms of latency and reliability optimization. Quantifying tail latency and reliability of an erasure-coded storage that employs dynamic workload scheduling and online repair is an open problem. There exists little work illuminating the design space via mathematical crystallization of key tradeoffs and associated engineering \"control knobs\".\n\nThis project will enable a joint optimization of latency, reliability and storage cost, which mandates rethinking of erasure-coded storage optimization and service pricing models. This proposal will concentrate on the following specific aspects: (1) Investigating a family of new probabilistic scheduling policies and extend order statistic analysis to derive closed-form bounds on tail latency for erasure-coded storage with arbitrary configurations, general service-time distributions, and potentially differentiated service classes. (2) Through a novel reliability metric, Time to Data Loss, The research will investigate online repair strategies that significantly improve reliability using a class of bandwidth-efficient codes, enabling a tradeoff between repair timeliness and reliability optimization. (3) Employ the theoretical analysis in this research to pursue a holistic solution that jointly optimizes reliability, latency, and storage costs over seven key control dimensions: choice of erasure codes, chunk placement, network resource allocation, cache management, dynamic scheduling, pricing, and online repair strategy. (4) Integrate the proposed framework with current cloud systems to bridge the gap between the theoretical results and practical systems. By developing new analytical models and algorithms for joint optimization of latency, reliability, and storage cost, the project will mandate rethinking of erasure-coded storage design and service pricing models. It will produces novel, interdisciplinary curriculum modules for teaching both these theories and systems."], "researcher_paper_abstract_in_json_file": "Energy accounting determines how much a software principal contributes to the total system energy consumption. It is the foundation for evaluating software and for operating system based energy management. While various energy accounting policies have been tried, there is no known way to evaluate them directly simply because it is hard to track all hardware usage by software in a heterogeneous multicore system like modern smartphones and tablets.   In this work, we argue that energy accounting should be formulated as a cooperative game and that the Shapley value provides the ultimate ground truth for energy accounting policies. We reveal the important flaws of existing energy accounting policies based on the Shapley value theory and provide Shapley value-based energy accounting, a practical approximation of the Shapley value, for battery-powered mobile systems. We evaluate this approximation against existing energy accounting policies in two ways: ( i ) how well they identify the top energy consuming applications, and ( ii ) how effective they are in system energy management. Using a prototype based on Texas Instruments Pandaboard and smartphone workload, we experimentally demonstrate existing energy accounting policies can deviate by 400% in attributing energy consumption to running applications and can be up to 25% less effective in system energy management when compared to Shapley value-based energy accounting.", "paper_keywords": ["energy accounting", "mobile systems", "energy management"], "score_lsi": ["0.498153"]}
{"researcher_id": [4607], "researcher_name_in_nsf_list": "Abhijeet  Joshi", "researcher_paper_title_in_json_file": "A novel background extraction algorithm for large motion video", "projects_cnt": 1, "year": 2015, "paper_citation": 0, "score_lda": ["0.617717"], "field": ["Computer vision", "Simulation", "Background subtraction", "Computer Science", "Video tracking", "Computer graphics (images)"], "researcher_nsf_project_abstract": ["The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is to accelerate advancement in high-mobility materials.  These materials are being increasingly used in the electronics device industry.  This project's goal is to enable better devices by providing complete data on the effects of manufacturing processes and better enabling their optimization.  Innovative electronic device structures such as faster computer chips, and more powerful RF circuits require development of smaller and smaller devices employing more advanced materials.  The innovation that is being advanced through this Phase II program directly impacts this development.\n\nThis Small Business Innovation Research (SBIR) Phase II project will develop a deployable system to directly measure high-resolution mobility, resistivity, and carrier concentration profiles for high-mobility semiconductor materials.  Current electrical profiling methods provide partial data for these material systems that form the basis of the multi-billion dollar semiconductor logic device and RF/power chip industries.  The objectives of this Phase II program are to further demonstrate a prototype by developing and integrating high-reliability sub-systems to build a beta-level measurement tool with nm-level resolution.  This is expected to reduce the semiconductor wafer area needed to evaluate high-mobility materials, and develop the measurement capability to target all high-mobility materials with potential applications in IC and RF/power industries."], "researcher_paper_abstract_in_json_file": "Development in the field of very large scale integration (VLSI), over the past decade flooded the market with video cameras, which increases the installation of camera for surveillance purpose. Due to this huge amount of data is augmented per day, which is either impossible to handle manually or to store for too long period. These create the need to find out an algorithms and methods for automatic processing of collected data, to detect object of interest. Automatic processing of data requires some lower level tasks of computer vision to be performed, which include tasks such as tracking targets, labeling foreground parts and detection of foreground objects. This can be done by the process like foreground tracking and background subtraction. To achieve these tasks, it is vital to extract a pure background frame from the video, which does not have any traces of moving object of the scene. This paper proposed the simple probability based background extraction algorithm, which will extract pure color backgrounds of the surveillance videos. Initial background extracted, by proposed algorithm give the accurate and quick results. With the help of this pure extracted background, detection of moving objects was take place correctly and quickly.", "paper_keywords": ["analytical models", "shape", "flowcharts"], "score_lsi": ["0.361975"]}
{"researcher_id": [11484], "researcher_name_in_nsf_list": "Gavin J Svenson", "researcher_paper_title_in_json_file": "Impact of youth peer education programs: Final results from an FHI / YouthNet study in Zambia.", "projects_cnt": 1, "year": 2008, "paper_citation": 50, "score_lda": ["0.94194"], "field": ["Simulation", "Medicine", "Nursing"], "researcher_nsf_project_abstract": ["Animal behavior is affected by an individual's internal conditions. For example, as animals feed, their strategies for acquiring food changes. The impact of food odors has a very different effect on a hungry person than one who has just had a large meal. This project brings together laboratories from the Case Western Reserve University Biology department and the Cleveland Museum of Natural History to examine changes in hunting strategy that occur as praying mantises feed. The biology laboratory will examine changes in brain systems that control movement as the insect feeds or receives injections of hormones associated with feeding. Insects provide advantages for monitoring brain activity for long feeding periods. Results will demonstrate how brain systems that are altered by hormones associated with feeding affect hunting and will increase our general understanding of the mechanisms by which hormonal changes alter animal behavior. The museum laboratory will expand the study to a wider range of praying mantis species. The project also has a unique educational component. Project related material will be developed into new high- and middle-school teaching modules for the Cleveland Museum of Natural History's award winning distance learning program, which has reached thousands of students in 48 states. These programs align with Ohio's New Learning Standards. Modules will be offered for free for the duration of the project and 3 subsequent years.\n\nThe project focuses on the highly structured central complex insect brain region that has received much recent attention. Numerous forms of sensory information coupled with motor effects and the presence of behaviorally relevant neuromodulators imply an important role for the central complex in behavioral adaptation. Yet, no study has brought all these components together to demonstrate how these brain circuits generate context dependent adjustments in natural behavior. This project seeks do that by relating changes in praying mantis hunt strategies to central complex activity patterns recorded by multi-channel tetrode implants as the hunt takes place in one generalist and two specialist praying mantis species. Tetrode wires will be implanted in the insect's central complex. Then after recovery the subject will be moved to an arena where it hunts either live prey (cockroach nymphs) or artificial prey (moving dots on a computer screen that makes up the floor of the arena). The artificial stimulus allows repeated trials to provide quantitative data on neural activity associated with hunting. Neural and behavioral changes will be documented as physiological state is modified by feeding or insulin injection. Comparative studies will clarify how evolution acts on brain structures to shape behavior for specific niches. Successful completion will be transformative both in our understanding of the central complex's role in behavioral adjustment and, more generally, in defining mechanisms by which brain regions in all animals can alter adaptive behavior, thereby establishing the praying mantis as a new general model for behavioral selection."], "researcher_paper_abstract_in_json_file": "Youth peer education (YPE) is a widely used approach for promoting sexual and reproductive health (SRH) and preventing HIV. This study sought to determine the effect of YPE programs on SRH behaviors among youth. Phase 1 of this study focused on identifying core components of YPE programs. In Phase 1 conducted March 2003 - December 2004 four successful YPE programs in Zambia and the Dominican Republic were followed using a descriptive process evaluation approach. Phase 1 identified core elements believed to be important for sustainability and peer educator retention. Phase 2 focused on programs in Zambia and was designed to assess the interrelationships among program inputs outputs exposure and outcomes. Phase 2 was conducted February 2005 - August 2006 and sought to link the quality of YPE programming to SRH behaviors. In conclusion this study found a chain of associations indicating that YPE in Zambia promotes HIV prevention behaviors. Overall a significant number of young people in Zambia were exposed to YPE. Exposure to YPE programs was associated with some SRH risk-reduction behaviors appropriate referrals and use of SRH services by highly vulnerable youth. The core components of YPE programming as measured by eight checklists appear to be equally important in terms of program quality \u2013 useful information when designing and implementing effective YPE programs. The higher quality YPE programs were clearly making a difference for Zambian youth. The remaining challenge is to raise the standards of all YPE programs so that they all make a difference in the lives of vulnerable youth. (Excerpts)", "paper_keywords": ["condom use", "referral and consultation", "reproductive health", "sexually transmitted diseases", "hiv infections", "program activities", "program evaluation", "youth programs", "evaluation report", "research methodology", "studies", "clinic activities", "reproductive tract infections", "peer educators", "viral diseases", "diseases", "evaluation", "organization and administration", "risk reduction behavior", "health", "sampling studies", "behavior", "programs", "hiv prevention", "counseling", "infections", "program effectiveness", "developing countries", "surveys", "sex behavior"], "score_lsi": ["0.461533"]}
{"researcher_id": [5425], "researcher_name_in_nsf_list": "Ning  Wu", "researcher_paper_title_in_json_file": "ESTIMATION OF QUEUE LENGTHS AND THEIR PERCENTILES AT SIGNALIZED INTERSECTIONS", "projects_cnt": 1, "year": 1998, "paper_citation": 50, "score_lda": ["0.90326"], "field": ["Simulation", "Engineering", "Operations management", "Transport engineering"], "researcher_nsf_project_abstract": ["Chirality is a fundamental characteristic of living matter. Chiral metamaterials are engineered structures composed of periodic metallodielectric building blocks that exhibit chirality. The resonance of individual building blocks and strong coupling between neighbors allow the realization of exotic electromagnetic properties such as negative refractive index and giant circular dichroism. Chiral metamaterials could revolutionize many modern technologies including superlenses, optical communication, cloaking devices, and high resolution sensors. Three-dimensional chiral metamaterials are, however, extremely difficult to fabricate especially in an economical and large scale fashion. This research will utilize asymmetric particles as building blocks and exploit a time-varying electric field to guide the assembly of these particles into long-range periodic chiral structures. The solution-based process could potentially reduce the fabrication cost significantly. The compatibility of the electrode design with existing microelectronic manufacturing infrastructures will also open the door to commercialize innovative metamaterials for emerging applications. The research team will also engage both high school and undergraduate students with immersive research experiences. Learning modules such as \"Colloids, surfactants, and our daily lives\" and \"Optical technology that changes our lives\" will be developed in collaboration with K-12 teachers. The research findings will also be included in the development of several multi-disciplinary courses such as \"Engineering of Soft Materials\" and \"Nanotechnology: From Molecules to Machines\". \n\nThe goal of this collaborative project is to use electric fields to assemble Janus metallodielectric particles into a periodic array of chiral clusters as an efficient bottom-up route for making three-dimensional chiral metamaterials. Particles with increasing morphological complexity will be fabricated as building blocks by glancing angle deposition. A liquid electrode cell will be built for the manipulation of the Janus particles via time-varying electric fields in three-dimensions. The Coulomb, dipolar, and electrohydrodynamic interactions will be exploited to guide the assembly of Janus particles into chiral clusters. Further manipulation of the chirality of the electric fields will transform the racemic mixture into enantiomerically pure clusters. The photonic response of the fabricated structures and their applications will be measured and explored in both near-infrared and visible regions. Numerical simulations will also be performed to elucidate the particle assembly mechanisms, their optical properties, and guide experimental optimization. The collaborative and iterative nature of this project will reveal the interrelationship between the design of fundamental building blocks, control of lattice symmetry, and targeted photonic properties of chiral metamaterials."], "researcher_paper_abstract_in_json_file": "Queue lengths are important parameters in traffic engineering for determining the capacity and quality of traffic control equipment. At signalized intersections, queue lengths at the end of red time (red-end) are of greatest importance for dimensioning the lengths of lane. While the average queue length reflects the capacity of traffic signals, the so-called 95th and 99th percentile of queue lengths at red-ends are used for determining the length of turning lanes, such that the risk of a blockage in the through lanes could be minimized. Furthermore, lengths of back-of-queue (queue length at queue-end) must be considered for determining the lengths of turning lanes at signalized intersections. The queue length and their distribution can be numerically calculated from Markov chains. The percentiles of queue lengths can be estimated from the distribution. Based on the results of Markov chains, regression are undertaken from obtaining explicit formulas under stationary traffic conditions. For non-stationary traffic conditions, the formulas can be derived using the so-called transition techniques.", "paper_keywords": ["length", "signalized intersections", "traffic control devices", "through traffic", "red interval traffic signal cycle", "turning lanes", "traffic lanes", "estimating", "highway capacity", "traffic engineering", "queuing", "markov chains"], "score_lsi": ["0.270052"]}
{"researcher_id": [10114, 11854], "researcher_name_in_nsf_list": "Yan  Wang", "researcher_paper_title_in_json_file": "Port Container Throughput Forecasting Based on the Multiplicative Seasonal ARIMA Model", "projects_cnt": 2, "year": 2015, "paper_citation": 0, "score_lda": ["0.952674", "0.920608"], "field": ["Simulation", "Engineering", "Operations management", "Operations research"], "researcher_nsf_project_abstract": ["This project exploits mobile sensing and vehicle localization to identify fine-grained abnormal driving behaviors, such as weaving, swerving, and fast U-turns, and further to infer location-aware dangerous vehicular status. Several existing works have tried to detect abnormal driving behaviors by focusing on detecting drivers' status based on pre-deployed infrastructure, such as alcohol sensors, infrared sensors, and cameras. Such approaches incur extra installation cost and are thus difficult to be widely adopted. In order to build pervasive location-aware driving safety systems, this project tries to deploy low power consumption sensing (utilizing mobile devices carried by users in vehicles) and learning techniques based on statistical analysis to localize vehicles and identify fine-grained abnormal driving behaviors. More importantly, the proposed system keeps tracking the drivers' behaviors and determines fine-grained location-related dangerous vehicular status, such as driving on the center line of two-way roads or occupying left lanes for a long time.\n\nThis project seeks to conduct a comprehensive study to understand to what extent the current mobile devices can model various real-world driving behaviors and corresponding vehicle dynamics. A new real-time mobile sensing system, which combines real-time mobile sensing and heterogeneous driving environments, is developed to address driving safety concerns. The final results will be the abiding principles of cyber-physical architecture that resolve dynamic impacts of complex environments and provide clear guidelines over Internet of Things (IoTs). Specifically, effective features are investigated from mobile sensor readings that are able to depict each type of abnormal driving behaviors. These features can thus be extracted to localize the vehicles and derive the patterns of abnormal driving behaviors (e.g., weaving, swerving, fast U-turn, and sudden breaks) with the consideration of generic driving scenarios and heterogeneous mobile devices. Techniques based on machine learning are developed to generate a classifier model that could clearly identify fine-grained abnormal driving behaviors. The classifier model will be further utilized as a foundation to devise the location-aware driving safety system, which can track users' driving behaviors and realize location-related dangerous vehicular status in real-time using low-computing-capability mobile devices.", "The ACM MobiCom conference is the leading international conference focusing on systems issues in the emerging area of mobile computing and wireless communications. The topics addressed by MobiCom 2016 are at the core of the current wireless network evolution. The conference attracts research contributions spanning multiple disciplines, including wireless networking, vehicular communications, personal area networks, ad hoc networks, operating systems, distributed algorithms, data processing, scheduling, sensors, and signal processing. In addition to the main session, the conference also provides good opportunities to students and researchers to participate in panels, workshops and demos that bring in brilliant ideas and applications in mobile, wireless and ad hoc networking and computing.\n\nThe NSF travel grant program will help increase representation and participation of United States-based graduate and undergraduate students at this conference, and enable sharing of information and preliminary research results among participating students. Participation of top conferences like ACM MobiCom is an extremely important part of the graduate school experience, providing students with an opportunity to interact with more senior researchers, and exposing them to leading edge research in the field. The award will enable graduate students who would be otherwise unable, to attend the main MobiCom 2016 and the associated workshops."], "researcher_paper_abstract_in_json_file": "Based on the theoretical research of the time series analysis, this paper systematically analyzes the changes rules of the monthly data of container throughput of Shanghai Port from 2002 to 2009 by", "paper_keywords": "", "score_lsi": ["0.412113", "0.368656"]}
{"researcher_id": [5181], "researcher_name_in_nsf_list": "HOWIE  CHOSET", "researcher_paper_title_in_json_file": "Conical sidewinding", "projects_cnt": 1, "year": 2012, "paper_citation": 50, "score_lda": ["0.918321"], "field": ["Computer vision", "Simulation", "Gait analysis", "Engineering", "Engineering drawing"], "researcher_nsf_project_abstract": ["MODULAR ROBOTIC SYSTEMS HAVE THE POTENTIAL TO BE ADAPTED TO VARYING TASKS USING A SINGLE PLATFORM AND ENABLE CUSTOMIZABLE ROBOTS TO BE DEVELOPED FASTER AND MORE ECONOMICALLY THAN CONVENTIONAL ROBOTS. CURRENTLY, NO PRACTICAL MODULAR ROBOTS EXIST, AND EVEN"], "researcher_paper_abstract_in_json_file": "Sidewinding is an efficient translation gait used by snakes and snake robots over flat ground, and resembles a helical tread moving over a core cylindrical geometry. Most sidewinding research has focused on straight-line translation of the snake, and less on steering capabilities. Here, we offer a new, geometrically intuitive method for steering this gait: Tapering the core cylinder into a cone, such that one end moves faster than the other, changing the heading of the robot as it drives forward. We present several design tools for working with this cone, along with experimental results on a physical robot turning at different rates.", "paper_keywords": ["legged locomotion", "geometry", "gait analysis"], "score_lsi": ["0.833853"]}
{"researcher_id": [4189], "researcher_name_in_nsf_list": "Mark  Brown", "researcher_paper_title_in_json_file": "Potential economic impact of high productivity vehicles for woodchip transport in the Green Triangle region of Australia", "projects_cnt": 1, "year": 2010, "paper_citation": 0, "score_lda": ["0.921095"], "field": ["Simulation", "Engineering", "Operations management", "Transport engineering"], "researcher_nsf_project_abstract": ["The Journal of Undergraduate Research (JUR) Press is an educational platform for improving scientific literacy that includes a range of research journals that are registered with the United States Library of Congress. This innovation serves as a platform for the dissemination and sharing of scholarly work and research findings among undergraduate STEM students throughout the world. Each issue has an ISSN number and is distributed internationally in both print and electronic formats. The editorial boards are comprised of undergraduate STEM students who are trained through a series of online courses and include participating editors from around the world. JUR Press is headquartered at Colorado State University and has four domestic, regional editorial offices along with five international editorial satellite offices which are all at academic institutions. Each submission is refereed by two undergraduates, one graduate student, and one faculty member in a way that allows undergraduate referees to learn from the input of the graduate and faculty referees. The training and assessment platform for editors and referees was developed in a way that any academic institution can include its students. To do so, a faculty advisor is recruited to oversee the process at each participating institution. JUR Press provides a step-by-step procedure that guides each new faculty advisor in the on-boarding, recruitment, training, and assessment processes. This I-Corps L team will use the support and mentoring of the I-Corps L program to determine if the JUR model can be scaled to facilitate the widespread adoption, adaptation, and utilization of these materials to support the improvement of scientific literacy and STEM student engagement at academic institutions across the country. Further, they will use I-Corps L as a platform for reaching out to a broad audience of STEM faculty and students through personal interviews, conferences, and webinars. \n\nThis project includes innovation in STEM education to achieve higher STEM student engagement, scientific literacy, and critical assessment skills. These innovations also provide opportunities for STEM students to execute real-world applications of the scientific method and research ethics. Finally, ongoing research associated with this innovation provides evidence that this platform serves to increase STEM persistence and rates of retention among participating students. This project is built upon prior research related to the improvement of STEM teaching methods to better engage STEM students, to accommodate a broader range of learning styles, and to improve scientific literacy among undergraduates. Products include online courses, teaching guides, supplemental teaching materials, and how-to seminars. Based on pre-/post-assessment of student referee reports, undergraduate referees consistently rank higher in critical review assessments after they have had the benefit of comparing their reviews to those of faculty referees. Almost 200 undergraduates have participated as editors for JUR Press. Based on pre-/post-assessments, editors consistently rank higher in both scientific literacy and critical review assessments after participating in the online editor training courses. These outcomes continue to improve each year that a participant serves as an editor."], "researcher_paper_abstract_in_json_file": "Transport cost are one of the major costs for forest operations and as the industry faces large increase in their freight task of over 350 million t-km in the next three years in the Green Triangle region they are eager to explore options to be as efficient as possible in tackling that new freight task. Through looking at their current payload efficiency using weigh bridge data it was noted that different configurations offer over double potential payload as compared to the basic semi-trailer but suffer from limited access. With the availability of Performance Based Standards to introduce new high productivity configurations the industry is now exploring three new configurations that have the potential to reduce cost by as much as 18 per cent and fuel use and thus greenhouse gas emissions by up to 26 per cent. (a) For the covering entry of this conference, please see ITRD abstract no. E219320.", "paper_keywords": ["forestry", "for 1507 transportation and freight services", "for 0705 forestry sciences", "transport efficiency", "loads", "performance based standards", "dynamics", "transportation", "high productivity", "freight transportation", "heavy vehicles", "conferences"], "score_lsi": ["0.252834"]}
{"researcher_id": [4217], "researcher_name_in_nsf_list": "Jason L Speyer", "researcher_paper_title_in_json_file": "Fuel-Optimal Periodic Solutions to Hypersonic Cruise with Active Engine Cooling", "projects_cnt": 1, "year": 1997, "paper_citation": 0, "score_lda": ["0.947806"], "field": ["Control engineering", "Simulation", "Engineering", "Control theory"], "researcher_nsf_project_abstract": ["The bell shaped curve, known technically as the Gaussian probability density function (pdf), has been a central element in engineering and financial algorithms that process data and automate a desired operation.  For example, in air traffic control the distance and bearing to an aircraft in a dynamic environment is measured by an active radar. This measurement is not exact, having an uncertainty or error in its value. This uncertainty is not described well by the Gaussian pdf because the portion of the bell shaped curve  far from zero, called the tail of the pdf, is far smaller than the radar data suggests: the Gaussian bell shaped curve is known to have a light, rapidly (exponentially) decaying tail, while radar data is said to have a heavy tail.  It has been well recognized that reliance on the Gaussian pdf can be dangerous (reference: The Black Swan by Nessim Taleb).  Many dynamic systems in engineering, economics, biology, financial movements, earthquakes, atmospheric turbulence, etc., are poorly described by Gaussian pdfs and better described by heavy tailed ones. However, majority of current data processing algorithms are based on the Gaussian pdf assumption mainly because it leads to tractable, real-time implementations.  The newly proposed theory is a paradigm shift, which proposes new algorithms based on a heavy tailed pdf, known as the Cauchy pdf. The result is a more accurate and reliable automated system, which even for probabilistic models that are not Cauchy has demonstrated comparable performance to the Gaussian when the probabilistic environment is such. Since extreme data is likely, the estimator is rich in structure and hence is computationally more intense than its Gaussian counterparts.\n\nFrom a more technical viewpoint, a new class of implementable real-time vector-state estimators and stochastic controllers for linear dynamic systems with additive heavy-tailed Cauchy process and measurement noises are to be developed. The estimation methodology for this vector-state, linear dynamic system with additive Cauchy noises was addressed by developing a recursion for the analytic measurement update and propagation of the character function of the unnormalized conditional probability density function (ucpdf) of the state given the measurement history.  Through a spectral transformation, the character function of the ucpdf is used explicitly in the development of stochastic controllers, based on a model predictive structure. These results entail significant analytical and numerical complexities due to the rich analytic form of the character function of the ucpdf, which produces a sum of terms that grows at each measurement update. The primary goal of the proposed study is to determine implementable real-time vector-state estimators and stochastic controllers by using simplifications that are due to the fundamental structure of the algorithms. Approximations that will conserve the basic structure of the character function are sought, and will be implemented on current computational hardware, such as graphic processing units. This work was performed with a colleague at the Technion under a Bi-national Science Foundation (BSF) Grant. This international collaboration will continue under the NSF/ENG/ECCS-BSF and BSF grants."], "researcher_paper_abstract_in_json_file": "In this brief paper, it is shown that the active cooling of convectively- heated engine and airframe components in hypersonic vehicles may be approximately modeled and the associated questions of fuel-optimality are tractable using variational methods. Assuming a fuel-consumpti on which is linear with respect to throttle setting, it is shown that a first-order model of the cooling cycle results in a variational Hamiltonian which is piecewise linear in throttle, which is nonsmooth at a single interior equilibrium throttle setting. This special Hamiltonian control structure is exploited in the solution of the acceleration-constrained, fuel-optimal periodic cruise problem . This work should be seen as a first step towards a more theoretical understanding of the coupling which exists between thermal-optimal and fuel-optimal hypersonic flight.", "paper_keywords": "", "score_lsi": ["0.71312"]}
{"researcher_id": [12035], "researcher_name_in_nsf_list": "Neil F Johnson", "researcher_paper_title_in_json_file": "Ground Zero revisits shape outbreaks: Zika and beyond", "projects_cnt": 1, "year": 2016, "paper_citation": 0, "score_lda": ["0.981774"], "field": ["Simulation", "Operations management", "Operations research"], "researcher_nsf_project_abstract": ["Cyberphysical (CPS) systems are set to become ever faster, driven by technological advances that push them toward speed limits set by fundamental physics. This proposal addresses the need for a theory of the dynamical behavior of CPS systems in the sub-second regime beyond human intervention times. Ultrafast instabilities have already been observed in such systems. The theory will allow for networking at multiple scales, coupling across multiple temporal and spatial scales, imperfect network communications and sensors, as well as adaptive reorganization and reconfiguration of the system. Theoretical findings will be checked against available empirical data, e.g., from the decentralized network of autonomous market exchanges with its mandated sensor systems. The project will inform the extent to which instabilities can build up across timescales, potentially threatening CPS system stability on a global level.\n\nThe project goal is a theoretical description of the dynamics in decentralized networks of semi-autonomous machines in which an ecology of algorithms, sensors and network links may be operating, adapting and even competing in response to external inputs. Attention will be paid to the regime of sub-second behavior where human intervention becomes impossible in real-time. The availability of data from such a system provides a test-bed for the multi-agent, complex network analyses to be developed. The project will address how instabilities can be mitigated and eventually controlled. The results are set to advance understanding of CPS system dynamics, not only among academics but also practitioners and regulatory bodies. Application areas that pervade modern life include market exchange systems, resource-allocation systems and remote sensing systems. Opportunities exist to integrate research and education concerning CPS applications across graduate and undergraduate classrooms, outreach through publications, and participation in K-12 activities."], "researcher_paper_abstract_in_json_file": "During an infection outbreak, many people continue to revisit Ground Zero - such as the one square mile of Miami involved in the current Zika outbreak- for work, family or social reasons. Public health planning must account for the counterintuitive ways in which this human flow affects the outbreak's duration, severity and time-to-peak. Managing this flow of revisits can allow the outbreak's evolution to be tailored.", "paper_keywords": "", "score_lsi": ["0.342013"]}
{"researcher_id": [12516], "researcher_name_in_nsf_list": "Frank K Lu", "researcher_paper_title_in_json_file": "Review of Micro Vortex Generators in High-Speed Flow", "projects_cnt": 1, "year": 2011, "paper_citation": 50, "score_lda": ["0.235003"], "field": ["Simulation", "Engineering", "Forensic engineering", "Engineering drawing"], "researcher_nsf_project_abstract": ["Pressure gain combustion (PGC, commonly called detonation) is a highly efficient process for power generation because the process burns fuels over 30 times faster than conventional, constant pressure combustion which is currently present in all commercial internal combustion engines. In addition to higher efficiency, a PGC power generator is inexpensive to build and to maintain, and it can use a wide variety of fuels, including sustainable biogas. A large, potential market exists for the PGC power generator, either operating as a standalone unit or as part of a grid. The use of sustainable biogas addresses the important issue of greenhouse gas emissions. This PGC generator fits into the solution strategies of advanced countries striving to meet environmental targets, by producing power with lower emissions through efficient combustion. Additionally, the PGC generator can increase availability of low-cost energy with the potential of improving human welfare in large areas of the globe. Further technological spinoffs of the PGC process include developing efficient heat sources, as well as other novel industrial processes. \n\nIn addition to observing fundamental combustion physics, various enabling technologies have been demonstrated by the team  over the past two decades. The PGC technology is protected by six patents with further disclosures in the pipeline. The PGC generator is simple to manufacture and operate, thereby accruing a cost advantage over existing generators in the 10-100 kW range. The technology is scalable and the proposed team has competitive designs ranging from 1 kW to 2 MW, targeting various niche markets that are not necessarily well served at present. The goal of this project is to bring PGC technology to the marketplace. The scope of the project is to explore the power generation and industrial applications market through discovering the customer base and to develop a prototype PGC generator. This team will engage with potential partners and investors through online and telephone contacts, and tradeshows, as well as visiting promising parties. The team will continue to develop a prototype PGC generator to exhibit to potential partners and investors."], "researcher_paper_abstract_in_json_file": "A review of the state-of-the-knowledge of micro vortex generators (MVGs) and their effect on separated shock/boundary-layer interactions is provided. MVGs are thought to be effective for reducing the separation zone. However, details of how they affect the separation zone remain to be understood properly. In addition, metrics on how the MVGs affect the separation have not been well developed. Suggestions for further study are provided.", "paper_keywords": "", "score_lsi": ["0.526452"]}
{"researcher_id": [3973, 8231], "researcher_name_in_nsf_list": "Yu  Cheng", "researcher_paper_title_in_json_file": "Virtual Reality Based Process Integrated Simulation Platform in Refinery\uff1a Virtual Refinery and Its Application", "projects_cnt": 2, "year": 2011, "paper_citation": 50, "score_lda": ["0.853165", "0.864036"], "field": ["Simulation", "Engineering", "Operations management"], "researcher_nsf_project_abstract": ["This travel grant will be used to support US based graduate students to attend IEEE GLOBECOM 2016, to be held in Washington, DC on December 4-8, 2016. IEEE GLOBECOM is one of two flagship conferences of the IEEE Communications Society (ComSoc). This conference offers not only technical paper presentations but also a wide range of other educational formats, including panels, tutorials, industry presentations, posters and keynote speeches by academic and industry leaders. Attending IEEE GLOBECOM is highly valuable for students. Not only will they be exposed to the state-of-the-art researches in the field of communications and networking, but also they can have the opportunity to interact with peers from institutions worldwide, meet with leading researchers from both industry and academia, and take part in discussions that may shape the future of the field.\n\nThis grant encourages participation by students who would normally find it difficult to attend. Recipients of the travel awards will be selected by a Travel Grants Committee, formed by the IEEE GLOBECOM 2016 student travel grant co-chairs. A total of 25 awardees will be selected. Award selection criteria include papers accepted into the conference and research interests in the conference topics as demonstrated by coursework and/or project experience. Women and minorities are given preference when other qualifications are similar. To avoid conflicts of interest, the students of the faculty in the selection committee will not be considered for this travel grant award. This student travel support will enable sharing scientific information and stimulating research interests for U.S. based students in the field of communications and networking.", "The rapid development and wide deployment of wireless networks incur a fast escalation of energy demand, which eagerly calls for energy-efficient networking techniques. At the same time, wireless networks are evolving into complex forms with multi-dimensional resources including communication link, radio, channel, antenna, and transmit power; algorithms with low complexity for energy efficiency optimization are highly demanded. This project targets at a fundamental study on energy-efficient wireless networking through establishment of a uniformed analytical framework, development of efficient and low-complexity algorithms, and application of the generic studies into important scenarios in the fifth generation (5G) cellular systems. This interdisciplinary research will not only provide various training projects to undergraduate and graduate studies, but also inspire students to pursue high-quality research with a creative, open-minded, and cross-disciplinary perspective.\n \nThis project is going to demonstrate that a uniformed multidimensional optimization framework for energy efficiency optimization can be constructed by the principle of scheduling proper transmission patterns, which are defined by the interference model of the network. With such a uniformed optimization model, low-complexity decomposition techniques are fundamentally related to a maximum weighted transmission pattern (MWTP) problem, under a physical interference model according to the signal-to-interference-plus-noise ratio. Approximation algorithms and associated performance analysis for the MWTP problem (which is NP-hard in general) are critical research issues to be studied. Distributed algorithms for solving the energy efficiency optimization problem further involves decomposition with multi-objective optimization, and innovative Lyapunov function design and associated stability analysis under the physical interference model, which will also be addressed in this project. Energy efficient solutions in a couple of important 5G cellular scenarios will be enabled through innovative modeling and algorithms in the uniformed multidimensional framework, including joint optimization that incorporates massive MIMO interference mitigation with flow constraint at network layer and base station sleeping at system level, formulation and algorithm development for a MWTP problem under the massive MIMO interference model, and modeling of the interplay between massive MIMO and device-to-device communications. In this project, the proposed research seamlessly integrates studies in the areas of optimization, graph theory, dual decomposition, approximation algorithms, and wireless communication and networking.  The research outcomes are expected to provide important guidance for the development of the 5G cellular systems."], "researcher_paper_abstract_in_json_file": "With the combination between system simulation and virtual reality, we have established an integrated virtual refinery simulation platform, and analyzed the overall design and principal architecture. This paper introduces a simulation algorithm about a refinery based on virtual reality, and explains how the algorithm can be applied to the virtual refinery in- tegrated simulation platform in detail. The virtual refinery simulation platform, which consists of a three-dimensional scene system, an integrated database system and a dynamic-static simulation system, has many applications, such as dynamic- static simulation of key process unit used as process control and oil tank blending simulation for scheduling. With the vi- sualization and human-computer interaction for acquiring production and process data, this platform can provide effective supports on staff training related with monitoring, control and operation in refinery. Virtual refinery can also be web pub- lished through the internet and it is helpful for the distance training and education.", "paper_keywords": ["zhou zewei feng yiping rong gang wu yucheng \u865a\u62df\u73b0\u5b9e\u6280\u672f \u4eff\u771f\u5e73\u53f0 \u8fc7\u7a0b\u96c6\u6210 \u70bc\u6cb9\u5382 \u5e94\u7528 \u4eff\u771f\u7cfb\u7edf \u4eff\u771f\u7b97\u6cd5 \u8fc7\u7a0b\u63a7\u5236 virtual reality based process integrated simulation platform in refinery virtual refinery and its application"], "score_lsi": ["0.182065", "0.460581"]}
{"researcher_id": [3077], "researcher_name_in_nsf_list": "Fei  Wang", "researcher_paper_title_in_json_file": "Finite Element Analysis of Z-Shaped Pipe in the Directly Buried Heat-Supply Pipeline with Large Diameter", "projects_cnt": 1, "year": 2014, "paper_citation": 0, "score_lda": ["0.928997"], "field": ["Computer simulation", "Structural engineering", "Simulation", "Computer Science", "Engineering", "Forensic engineering", "Engineering drawing"], "researcher_nsf_project_abstract": ["The rapid adoption of Electronic Health Records (EHRs) across the U.S. healthcare systems coupled with the capability of linking EHRs to research biorepositories provides a unique opportunity for conducting large-scale Precision Medicine research. A critical step to make such research possible is identification of cohorts by defining inclusion and exclusion criteria that algorithmically select sets of patients based on available clinical data. For most of the existing research, the criteria for generating those patient cohorts are defined manually, which makes the entire process slow, labor intensive and not scalable. This project develops patient similarity learning algorithms to enable automatic cohort identification, which will accelerate the research of precision medicine.\n\nThe massive clinical data around patients are highly heterogeneous and sparse. Although there are some patient similarity learning algorithms, they typically work with a single type of patient data (e.g., just using diagnosis information in patient EHR) and cannot handle those challenges mentioned above effectively. This project develops advanced patient similarity learning algorithms by 1) learning composite patient similarities through a refinement process from multiple base similarity measures, with each base similarity being evaluated from a specific source of patient data or a specific form of patient representation; and 2) integrating information from multiple related auxiliary domains, such as drug, disease, and genomic information. Those information effectively regularizes the patient similarity learning process and makes it less sensitive to data sparsity."], "researcher_paper_abstract_in_json_file": "In order to improve Z-shaped pipe in the directly buried heat-supply pipeline with large diameter stress calculations, guiding the engineering design when the short arm length is less than two times the elastic arms length. The author used ANSYS software to numerical simulation analysis on Z-shaped pipe in the directly buried heat-supply pipeline of DN800, DN1000, DN1200, when the long arm length of 50 m, 100 m, 150 m, and the short arm length from two times to one times of the elastic arm length. Applying boundary conditions in different models with the short arm length shortens the process to Z-shaped pipe of the elbow stress. For Z-shaped pipe in the directly buried heat-supply pipeline the short arm from two times to one times of the elastic arm length, the elbow\u2019s stress value is the minimum when the short arm length is 1.2 times of the elastic arm length. This article breaks the specification limits on the short arm length, improving the flexibility of directly buried heat-supply pipeline with large diameter, reducing the difficulty of construction, and it\u2019s important for guiding the actual project.", "paper_keywords": ["z shaped pipe", "elastic arm length", "directly buried heat supply pipeline", "numerical simulation"], "score_lsi": ["0.234392"]}
{"researcher_id": [7531], "researcher_name_in_nsf_list": "Yue  Li", "researcher_paper_title_in_json_file": "Research and Design of Flipped Classroom for International Trade Practice in Sino-Foreign Cooperated Higher Vocational Colleges", "projects_cnt": 1, "year": 2017, "paper_citation": 0, "score_lda": ["0.959792"], "field": ["Mathematics education", "Simulation", "Engineering", "Pedagogy"], "researcher_nsf_project_abstract": ["The objective of this project is to systematically integrate the seismic hazard resulting from aftershocks in modern earthquake engineering design codes based on the emerging concept of performance-based earthquake engineering, thereby reducing the potential for loss of life and damage.  Major earthquakes are often followed by multiple aftershocks. These aftershocks can occur very soon or significantly later than the occurrence of the main shock. Aftershocks have the potential to cause severe damage to buildings and threaten life safety even when only minor damage occurs in the main shock. Aftershocks may have different energy content and can occur at a different location on the fault, that is, closer to a population center. To achieve the objective a series of analytical/numerical studies utilizing a portfolio of realistic building models will be conducted. The models will be calibrated using the data available in the repository of the Network of Earthquake Engineering Simulation hub (NEEShub) site. These models will be used to develop system fragilities, which provide the probability of different damage states, for structures with varying levels of damage. The fragilities will be combined with aftershock hazard models to quantify their effect on the building performance. As a final outcome, the project will provide a methodology to future researchers and practitioners to integrate the system fragilities and the effect of aftershocks in current state-of-the-art performance-based design approaches. \n\nIn order to include the effect of aftershocks on a broader scale, the approach developed in this project will be integrated into next generation seismic design methodologies, which are being formulated in the ATC-58 Project of the Applied Technology Council for performance-based design of new buildings and evaluation of existing buildings. This project will generate useful data that can be used by practitioners and code development bodies to incorporate the effect of aftershock in building design for enhanced life safety and performance. This data will be made available to other researchers through the NEEShub repository. The project will also provide advanced training to graduate students in earthquake engineering and performance-based earthquake engineering."], "researcher_paper_abstract_in_json_file": "As a student-centered mode which turns the traditional education upside down, the flipped classroom draws growing interest recently. This paper analyses the characteristics of post-1995 students and the course of International Trade Practice in Sino-Foreign cooperated higher vocational colleges. Then the advantages and challenges of flipped classroom are studied for designing a new model of blended course to meet the demand of teaching. The possible problems, solutions, and implementation scheme are also discussed.", "paper_keywords": ["sino foreign cooperated higher vocational college", "international trade practice", "flipped classroom"], "score_lsi": ["0.430892"]}
{"researcher_id": [11746], "researcher_name_in_nsf_list": "Cyrus  Shahabi", "researcher_paper_title_in_json_file": "Progression-Preserving Dimension Reduction for High-Dimensional Sensor Data Visualization", "projects_cnt": 1, "year": 2013, "paper_citation": 0, "score_lda": ["0.351437"], "field": ["Computer vision", "Simulation", "Projection", "Computer Science", "Machine learning", "Mathematics", "Virtual reality", "Dimensionality reduction", "Clustering high-dimensional data"], "researcher_nsf_project_abstract": ["This award supports participation of 20 graduate and undergraduate students enrolled at universities in the U.S. in the 17th IEEE International Conference on Mobile Data Management (MDM 2016), which will be held June 13-16, 2016 in Porto, Portugal (http://mdmconferences.org/mdm2016/). MDM is a premier conference co-sponsored by the IEEE Computer Society. It encompasses all aspects of mobility. Given the broad scope of mobility, MDM brings together researchers from databases, networking and ubiquitous and pervasive computing, to present and discuss the progress of science in these areas of critical national needs. The conference relies on the experience of senior members of the community, but also aims to generate fresh ideas, foster new collaborations, and expose new people to emerging research opportunities. Attendance at MDM is very beneficial to students. The conference makes for an excellent forum for exchange of ideas and future collaborations. Students also have opportunities to meet distinguished researchers and be inspired and mentored by them to follow careers in mobile data management research. \n\nA number of student-oriented events are planned: workshops, demos, panels, seminars, PhD forum, and a special industrial forum. The availability of the travel support will be well publicized throughout the U.S. research community. Students from underrepresented groups (women, minorities and disabled students) are especially encouraged to apply. The project has established processes for selecting the awardees that would both benefit from participating in MDM and contribute to the MDM program, especially the student-oriented activities. The MDM Conference Proceedings are published by IEEE."], "researcher_paper_abstract_in_json_file": "This letter presents Progression-Preserving Projection, a dimension reduction technique that finds a linear projection that maps a high-dimensional sensor dataset into a two- or three-dimensional subspace with a particularly useful property for visual exploration. As a demonstration of its effectiveness as a visual exploration and diagnostic means, we empirically evaluate the proposed technique over a dataset acquired from our own virtual-reality-enhanced ball-intercepting training system designed to promote the upper extremity movement skills of individuals recovering from stroke-related hemiparesis.", "paper_keywords": ["linear projection", "rehabilitation after stroke", "dimension reduction", "virtual reality", "high dimensional data"], "score_lsi": ["0.186059"]}
{"researcher_id": [10567], "researcher_name_in_nsf_list": "Jerome F Hajjar", "researcher_paper_title_in_json_file": "AUTOMATED DAMAGE ASSESSMENT FROM 3-D LASER SCANS", "projects_cnt": 1, "year": 2014, "paper_citation": 50, "score_lda": ["0.708668"], "field": ["Computer vision", "Simulation", "Engineering", "Forensic engineering"], "researcher_nsf_project_abstract": ["America relies on a robust and resilient building stock to minimize harm to its citizens and damage to its economy due to extreme natural hazards such as earthquakes and hurricanes. In the past, structural engineers have focused their attention on creating stronger, more ductile, more reliable lateral-resistance systems that can be used within the walls of buildings to resist the extreme demands associated with these natural hazards. Comparatively little attention has been paid to the role of the floor systems of buildings in resisting these demands. The floor diaphragm acts as a critical element that distributes the demands developed in a building during an extreme event to the lateral-resistance systems and eventually to the building foundation. Steel deck, i.e., thin corrugated steel panels typically with concrete fill, forms one of the most commonly used diaphragm elements in multi-story steel buildings. This project has as its objectives: to develop fundamental understanding of steel deck diaphragms as structural systems integrated within the overall building performance, to develop improved strategies for accurate modeling of floor systems within three-dimensional building models, and to develop new solutions for steel deck diaphragms that enhance the overall structural resilience of buildings.\n\nCurrent lack of knowledge about floor diaphragm systems impedes a needed evolution for building design approaches from focusing on two-dimensional frame design to enabling creative solutions within three-dimensional building design. The utilization of the diaphragm as an energy dissipating system has not been harnessed nor optimized in buildings. This project will develop a series of building archetypes appropriate to steel deck diaphragms. An integrated experimental program will be conducted at the connection and diaphragm scale, including novel non-contact measurement schemes for revealing damage and deformations during testing, to bridge critical knowledge gaps that currently impede three-dimensional modeling and design of buildings. To explore new solutions for energy-dissipating diaphragms, this project will perform testing of structural fuses and develop prototypes for integrating these fuses into steel diaphragm systems. This project will also complete high fidelity material and geometric nonlinear finite element models to enable detailed investigations of the flow of forces in diaphragms and between the diaphragm and all connected components. A series of lower fidelity, reduced order models will be developed, appropriate for whole building analysis of selected building archetypes. Formal optimization of the role of the diaphragm in building response, including a novel two-level optimization scheme, will be performed. Taken together, these activities will provide a significant advancement in the state-of-the-art for design of building diaphragms. Through a comprehensive outreach effort with industry, the findings will be transferred to engineers and utilized to improve the structural resilience of the nation's buildings."], "researcher_paper_abstract_in_json_file": "Many existing bridges suffer from damage due to age or accumulated damage from hazards. It is essential to accurately assess the current conditions of these aging, deteriorating and damaged structures in order to evaluate their present status and decide on future steps for rehabilitation. Three-dimensional laser-scanning technology, which is used to collect 3D laser scans with coupled texture-mapped images, is a common way to capture the as-is conditions of structures. However, it is challenging to automatically extract meaningful information from the highresolution laser scans even though they provide a detailed geometric representation. Dividing the collected 3D laser scan into useful clusters requires determining the location, orientation and size of objects in a scene by using well-established point cloud processing steps. Once the objects are detected, they are used for identifying the deteriorated locations. This paper describes several new methods developed for automatically locating, quantifying and documenting damage types including large cracks, spalling and misalignment from 3D laser scans. Proposed methods are tested on a collapsed bridge scan, in order to show that defect localization and quantification are performed successfully. The obtained results show that 3D laser-scanning technology could be effectively used to document critical, quantitative information on present conditions related to damage of structures.", "paper_keywords": "", "score_lsi": ["0.521506"]}
{"researcher_id": [15], "researcher_name_in_nsf_list": "Christophe  Bobda", "researcher_paper_title_in_json_file": "Vision-Based Path Construction and Maintenance for Indoor Guidance of Autonomous Ground Vehicles Based on Collaborative Smart Cameras", "projects_cnt": 1, "year": 2016, "paper_citation": 0, "score_lda": ["0.868262"], "field": ["Embedded system", "Computer vision", "Simulation", "Engineering"], "researcher_nsf_project_abstract": ["Cameras are pervasively used for surveillance and monitoring applications and can capture a substantial amount of image data. The processing of this data, however, is either performed a posteriori or at powerful backend servers. While a posteriori and non-real-time video analysis may be sufficient for certain groups of applications, it does not suffice for applications such as autonomous navigation in complex environments, or hyper spectral image analysis using cameras on drones, that require near real-time video and image analysis, sometimes under SWAP (Size Weight and Power) constraints. \n\nThis work hypothesizes that future data challenges in real-time imaging can be overcome by pushing computation into the image sensor. Such systems will exploit the massive parallel nature of sensor arrays to reduce the amount of data analyzed at the processing unit. To this end, vertically integrated technology, such as focal plane sensor processors (FPSP), have been developed to overcome the limitations of conventional image processing systems. While some of these devices are programmable and offer the benefits of close-to-sensor processing such as performance and bandwidth reduction, they exhibit many drawbacks. For instance, each column of pixels is handled by a single processor, which reduces the parallelism and all pixels are treated equally and processed at the same rate, despite differences in input relevance for the application at hand. Consequently, systems spend more time spinning on non-relevant data, which increases sensing and computation time and power consumption. Research on FPSPs has mostly focused on technology aspects with some proof of concepts. Architectural design approaches, that involve high-level synthesis with the goal of mapping applications to low-level architectures, have not gained a lot of attention.\n\nTo overcome the limitations of existing architectures, the goal of this research is the design of a highly parallel, hierarchical, reconfigurable and vertically-integrated 3D sensing-computing architecture (XPU), along with high-level synthesis methods for real-time, low-power video analysis. The architecture is composed of hierarchical intertwined planes, each of which consists of computational units called XPUs. The lowest-level plane processes pixels in parallel to determine low level shapes in an image while higher-level planes use outputs from low-level planes to infer global features in the image. The proposed architecture presents three novel contributions: a hierarchical, configurable architecture for parallel feature extraction in video streams, a machine learning based relevance-feedback method that adapts computational performance and resource usage to input data relevance, and a framework for converting sequential image processing algorithms to multiple layers of parallel computational processing units in the sensor. \n\nThe results of this projects can be used in other fields, where large amounts of processing need to be performed on data collected by generic sensors deployed in the field. Furthermore, mechanisms for translating sequential constructs into functionally equivalent accelerators using hardware constructs will lead to highly parallel and efficient sensing units that can perform domain specific tasks more efficiently."], "researcher_paper_abstract_in_json_file": "In this paper, we present a guidance and coordination of autonomous ground vehicles in indoor environment. The solution is based on a set of distributed ceiling-mounted smart cameras with overlapping field-of-view for global coordination. A mean shift based algorithm is implemented to extract a map of the environment. This map is used for a distributed routing of autonomous-guided-vehicles from source to destination. Shortest paths will be calculated and updated in real-time. Our approach fits the requirements of decentralized coordination, real-time environmental changes, as it is the case in production facilities, and portability to other fields of application. First, tests in a test-bench showed satisfying results in terms of reliability, validity and performance.", "paper_keywords": ["vision based navigation", "path planning", "robot navigation", "distributed camera network"], "score_lsi": ["0.609013"]}
{"researcher_id": [10857], "researcher_name_in_nsf_list": "David L Strayer", "researcher_paper_title_in_json_file": "Compensatory Impact of Lane Changes When Distracted, Slower-Moving Cell-Phone-Using Drivers Impede Traffic Flow Efficiency", "projects_cnt": 1, "year": 2009, "paper_citation": 0, "score_lda": ["0.949199"], "field": ["Simulation", "Engineering", "Automotive engineering", "Transport engineering"], "researcher_nsf_project_abstract": ["Human activities have moved thousands of species around the world, allowing them to establish populations in new places. These non-native species have large effects on biodiversity and ecosystems, cause enormous economic damage, and are one of the most important and least controlled of human impacts on the world's ecosystems. Scientists know that these effects can change through time, but the amount, timing, and nature of these changes are poorly known. This project will continue a long-term study of the changing impacts of the zebra mussel on the Hudson River ecosystem, an ecologically and economically important invasive species that first appeared in the Hudson River in 1991. This study will improve understanding of one of the world's most problematic invaders in aquatic ecosystems, and provide one of very few detailed studies of the long-term effects on a non-native species. The results of this research will continue to be used by government agencies that manage the Hudson River, and should inform management of non-native species in general. Findings will be disseminated by posting research data on open websites, continuing highly successful educational programs to provide data, lesson materials, and training to middle- and high-school teachers and students in the Hudson Valley and New York City school systems, and continuing an active program of public outreach.\n\nThe researchers will continue measuring population parameters (density, mortality rates and size structure) and filtration rates of zebra mussels, phytoplankton biomass, zooplankton density and species composition, bacteria production, water quality parameters (transparency and nutrient concentrations) and particulate and dissolved Carbon to answer the following questions: (1) How will the long-term changes in the zebra mussel population and its effects continue to develop? (2) Can these changes be explained by relationships between the size and characteristics of the zebra mussel population and the Hudson River ecosystem? (3) How does zebra mussel grazing interact with climate to shape properties of the Hudson River ecosystem? The researchers will use time series and regression models to explore zebra mussel population dynamics and the relationship between zebra mussel filtration rates and various components of the river food web. In addition they will analyze three interactions between zebra mussel invasion and climate: 1) effects of freshwater flow on zebra mussel grazing, 2) effects of water temperature and mussel grazing on phytoplankton biomass and 3) correlations between warm temperature and inter-annual variation in zebra mussel mortality. Five additional years of data will allow the researchers to document the remarkable changes that are now occurring in the Hudson River, and substantially increase the power of statistical models to describe these events."], "researcher_paper_abstract_in_json_file": "One of the characteristics of drivers distracted by cell phone conversation is reduced speed. Generally, slower moving vehicles reduce the efficiency of the traffic stream. The aim of this research is to examine the impact of slower moving cell phone distracted drivers on traffic flow efficiency. This paper investigates the impact of lane changing maneuvers on traffic flow both with and without slow moving vehicles. Various flows and their corresponding speeds were examined using the microsimulation software VISSIM. The results indicate that lane changes can partially offset the negative effect of slower moving vehicles. Also, the results suggest that omission of lane changing reduces both the capacity of a road and the optimum speed; whereas, the presence of slower moving cell phone distracted drivers reduces the optimum speed, and, consequentially, increases overall delays\u2014even for the drivers who are not using cell phones. We estimate that applying user costs to simulated delay measurements, for urban freeway traffic in the USA, represents an annual cost of $162.5 million to society.", "paper_keywords": ["cellular telephones", "traffic simulation", "vissim computer model", "human factors in crashes", "traffic flow", "traffic safety", "slow moving vehicles", "operations", "lane changing", "distraction"], "score_lsi": ["0.492913"]}
{"researcher_id": [1091], "researcher_name_in_nsf_list": "Guido  Cervone", "researcher_paper_title_in_json_file": "Chapter 15 \u2013 Usage of Social Media and Cloud Computing During Natural Hazards", "projects_cnt": 1, "year": 2016, "paper_citation": 50, "score_lda": ["0.992314"], "field": ["Simulation", "Computer Science", "Operating system", "Data mining", "Computer security", "Disaster recovery", "Emergency management"], "researcher_nsf_project_abstract": ["The study of hazards and renewable energy are paramount for the development and sustainability of society. Similarly, the emergence of new climatic patterns pose new challenges for future societal planning.  Geospatial data are being generated at unprecedented rate exceeding our analysis capabilities and leading towards a data-rich but knowledge-poor environment.  The use of advanced computing tools and techniques are playing an increasingly important role in contributing to solutions to problems of societal importance. This project will create specialized computational tools that will enhance the ability of scientists to effectively and efficiently study natural hazards and renewable energy. The use of these tools will support novel methods and the use of powerful computing resources in ways that are not currently possible.\n\nMany scientific applications in the geosciences are increasingly reliant on \"ensemble-based\" methods to make scientific progress.  This is true for applications that are both net producers of data, as well as aggregate consumers of data.   In response to the growing importance and pervasiveness of ensemble-based applications and analysis, and to address the challenges of scale, simplicity and flexibility, the research team will develop the Ensemble Toolkit for Earth Sciences.  The Ensemble Toolkit will provide an important addition to the set of capabilities and tools that will enable the geosciences community to use high-performance computing resources more efficiently, effectively and in an extensible fashion. This project represents the co-design of Ensemble Toolkit for Earth Sciences and is a collective effort of an interdisciplinary team of cyberinfrastructure and domain scientists. It will also support the integration of the Ensemble Toolkit with a range of science applications, as well as its use in solving scientific problems of significant societal impact that are currently unable to utilize the collective capacity of supercomputers, campus clusters and clouds"], "researcher_paper_abstract_in_json_file": "Social media data have emerged as new sources for detecting and monitoring disaster events. Several recent studies have suggested that social media data streams can be used to mine actionable data for emergency response and relief operations. Such massive and rapidly changing data present several new grand challenges to archive and extract critical validated information for various disaster management activities. The volume, velocity, and variety of the data require advanced computing infrastructure technologies for big data management. Cloud computing is proposed as the ideal platform to address these computing challenges. This chapter discusses the opportunities and challenges associated with using social media to gain situational awareness during disasters and the feasibility of using cloud computing to build a resilient and real-time disaster management system. Several real case studies and applications are presented to illustrate how both social media and cloud-computing solutions are used for various natural hazards.", "paper_keywords": ["disaster management", "data mining", "disaster relief", "disaster coordination", "social media", "cloud computing"], "score_lsi": ["0.681177"]}
{"researcher_id": [1449], "researcher_name_in_nsf_list": "John A Stankovic", "researcher_paper_title_in_json_file": "Demo: AsthmaGuide: An Ecosystem for Asthma Monitoring and Advice", "projects_cnt": 1, "year": 2015, "paper_citation": 0, "score_lda": ["0.970704"], "field": ["Simulation", "Data mining", "Multimedia", "Natural environment"], "researcher_nsf_project_abstract": ["Recently there is an increasing availability of smart wearables including smart watches, bands, buttons and pendants. Many of these devices are part of human-in-the-loop Cyber Physical Systems (CPS). With future fundamental advances in the intersection of communications, control, and computation for these energy and resource limited devices, there is a great potential to revolutionize many CPS applications. Examples of possible applications include detecting and controlling hand washing to prevent transmission of infections or bacteria, monitoring and using interventions to keep factory workers safe, detecting activities in the home for monitoring the elderly, and improving rehabilitation of stroke victims via controlled exercises. However, to date, much of the work on wearables concentrates on only sensing, collecting and presenting data. For use in CPS it is necessary to consider the increased use of new sensing modalities, to apply feedback to close the control loop, and to focus on the fundamental issues of how both the environment and human behavior affect the cyber. In particular, since humans are intimately involved with wearables it is necessary to increase understanding of how human behaviors affect and can be affected by the control loops and how the systems can maintain safety.\n\nThis work develops generic underlying algorithms for processing smart wearable data rather than one-off solutions, it extends the understanding and control of wearable systems by addressing humans-in-the-loop behaviors, and it explicitly focuses on the impact of the environment and human behavior on the cyber. Novel ideas are proposed for each of these areas along with a structure for their integration. For example, the algorithmic approach to support more robust, accurate and efficient activity recognition using wearable devices is based on five fundamental concepts: (i) Direction Agnostic Modeling, (ii) Direction Aware Modeling, (iii) Spatial Reachability, (iv) Spatiotemporal Segmentation, and (v) Dynamic Space Time Warping. For dealing with humans-in the-loop behaviors, Model Predictive Control (MPC) is extended to semantic based MPC. This solves control problems that are not amenable to electromechanical laws and employs machine learning. Many CPS projects do not explicitly address how the uncertain world affects how the cyber must be developed in order to perform robustly and safely. A new *-aware software development paradigm focuses on physical-cyber CPS issues as central tenets and that serves as an integrating platform for all the proposed work. The *-aware paradigm focuses on how software must be made robust to handle the physical world, while meeting safety and adaptability requirements"], "researcher_paper_abstract_in_json_file": "AsthmaGuide is a smartphone and cloud based asthma system in which a smart phone is used as a hub for collecting a comprehensive collection of information. The data, including data over time, is then displayed in a cloud web application for both patients and healthcare providers to view. AsthmaGuide also provides an advice and alarm infrastructure based on the collected data and parameters set by healthcare providers. With these components, AsthmaGuide provides a comprehensive ecosystem that allows patients to be involved in their own health and also allows doctors to provide more effective day to day care. Using real asthma patient wheezing sounds we develop a new combination of classifiers that is 96% accurate at automatically detecting wheezing. This abstract provides an overview of the design and implementation of AsthmaGuide and provides empirical evidence that AsthmaGuide is 3% - 11% more accurate in detecting wheezing sounds than standard techniques.", "paper_keywords": ["environment", "advice", "asthma ecosystem"], "score_lsi": ["0.607561"]}
{"researcher_id": [13705], "researcher_name_in_nsf_list": "WALLACE  FOWLER", "researcher_paper_title_in_json_file": "Small satellite thermal design, test, and analysis", "projects_cnt": 1, "year": 2006, "paper_citation": 50, "score_lda": ["0.452476"], "field": ["Meteorology", "Simulation", "Engineering", "Electrical engineering"], "researcher_nsf_project_abstract": ["CONSISTENT WITH THE NASA''''S SCIENCE MISSION DIRECTORATE SCIENCE EDUCATION GOALS TO ENABLE STEM EDUCATION, IMPROVE U.S. SCIENTIFIC LITERACY, ADVANCE NATIONAL EDUCATION GOALS AND LEVERAGE EFFORTS THROUGH PARTNERSHIPS NASA''''S TEXAS SPACE GRANT CONSORTIUM"], "researcher_paper_abstract_in_json_file": "There has been a recent increase in emphasis on small satellites because of their low cost, short development times, relative simplicity, and cost efficiency. However, these satellites do have drawbacks. Their small size results in small surface areas which often translate into thermal and power constraints. A small satellite may not have enough surface area for radiators and/or solar panels. The radiators are used to release internal heat during hot environments, and solar panels create necessary power for the heaters during cold environments. Because of the surface area and power limitations, a passive thermal design was then selected for the F ormation A utonomy S pacecraft with T hrust, R elative Navigation, A ttitude, and C rosslink Program (FASTRAC) twin satellites, built by students at the University of Texas at Austin. Thermal cycling and thermal analysis were performed. The thermal cycling was done in Chamber-N at Johnson Space Center, Texas, using worst case hot and cold scenarios. The thermal analysis was conducted using Finite Elements (FE), and the results were compared to the test data and validated. FASTRAC is planned to be in a LEO orbit which ranges between 300km and 500km in altitude. The orbits were then simulated to determine the characteristics of the LEO orbits in which FASTRAC can survive.\u00a9 (2006) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.", "paper_keywords": ["solar cells", "thermal analysis", "finite element", "thermal cycling", "small satellite", "thermal design", "satellites", "cost efficiency", "nanosatellites", "development time", "orbital dynamics", "thermal vacuum test", "surface area", "johnson space center"], "score_lsi": ["0.0706552"]}
{"researcher_id": [3928], "researcher_name_in_nsf_list": "Wei  Ren", "researcher_paper_title_in_json_file": "Heterogeneous Distributed Average Tracking", "projects_cnt": 1, "year": 2016, "paper_citation": 0, "score_lda": ["0.99291"], "field": ["Mathematical optimization", "Simulation", "Computer Science", "Control theory"], "researcher_nsf_project_abstract": ["Distributed motion coordination, where multiple agents achieve collective motion patterns with only local information and interaction, has numerous applications. Despite the important role of cooperative optimization, existing results on distributed motion coordination seldom optimize a team objective function while those on distributed optimization (primarily discrete-time algorithms) do not explicitly account for continuous-time physical dynamics. Distributed continuous-time optimization is of great significance in enabling multiple physical agents to cooperatively achieve motion coordination and team optimization with only local information and interaction. Despite a few recent results on distributed continuous-time optimization, they are rather limited, with restrictive assumptions posed on convex objective functions and not addressing realistic challenges, namely, i) fully distributed design, ii) finite-time convergence, iii) time-varying objective functions, and iv) physical agent dynamics. The special assumptions limit the application domains while the realistic challenges are relevant in, respectively, i) real-world implementation of distributed algorithms, ii) time-critical missions, iii) applications demanding response to real-time changes, and iv) real physical systems. Despite their relevance and importance, each of the above issues is largely unexplored, not to mention a combination. The proposed research aims at addressing these realistic challenges. Numerous civilian, homeland security, and military applications involving multi-agent systems and fields related to optimization and networked systems will benefit from the proposed research.\n\nThe goal of this project is to address distributed continuous-time optimization for physical agents with local information and interaction under realistic challenges, coupled with general convex functions and directed graphs. The proposal consists five thrusts, namely, 1) fully distributed continuous-time optimization, 2) finite-time distributed continuous-time optimization, 3) distributed continuous-time optimization with time-varying objective functions, 4) distributed continuous-time optimization with physical agent dynamics, and 5) experimental validation. In Thrust 1, the PI will design and analyze novel self-adaptive fully distributed optimization algorithms, robust to topology changes and addition/removal of agents in the team, with state-dependent diminishing gains. In Thrust 2, the PI will design and analyze novel finite-time fully distributed optimization algorithms without/with constraints by combining distributed tracking and estimation with adaptive gains driven by a switching mechanism. In Thrust 3, the PI will tackle issues such as relaxed conditions on changing rates of time-varying objective functions, non-existence of Hessians, and incomplete knowledge of local objective functions. In Thrust 4, the PI will design and analyze novel distributed optimization algorithms accounting for Lagrange and more general unknown nonlinear dynamics. In Thrust 5, distributed control laws from Thrusts 1-4 will be experimentally validated on teams of autonomous robots. The project will solve many open problems in distributed control and optimization and significantly advance theory and applications in multi-agent systems."], "researcher_paper_abstract_in_json_file": "This paper addresses distributed average tracking for a group of heterogeneous physical agents consisting of single-integrator, double-integrator and Euler-Lagrange dynamics. Here, the goal is that each agent uses local information and local interaction to calculate the average of individual time-varying reference inputs, one per agent. Two nonsmooth algorithms are proposed to achieve the distributed average tracking goal. In our first proposed algorithm, each agent tracks the average of the reference inputs, where each agent is required to have access to only its own position and the relative positions between itself and its neighbors. To relax the restrictive assumption on admissible reference inputs, we propose the second algorithm. A filter is introduced for each agent to generate an estimation of the average of the reference inputs. Then, each agent tracks its own generated signal to achieve the average tracking goal in a distributed manner. Finally, numerical example is included for illustration.", "paper_keywords": "", "score_lsi": ["0.782293"]}
{"researcher_id": [12119], "researcher_name_in_nsf_list": "Yan  Yu", "researcher_paper_title_in_json_file": "Task Allocation for Event-Aware Spatiotemporal Sampling of Environmental Variables", "projects_cnt": 1, "year": 2004, "paper_citation": 0, "score_lda": ["0.779574"], "field": ["Real-time computing", "Simulation", "Geography", "Operations management"], "researcher_nsf_project_abstract": ["CBET - 1554078\nPI: Yu, Yan\n\nJanus particles are specialized particles whose surfaces have two distinct parts with different surface chemistry on each part.  This two-faced feature makes Janus particles especially useful in novel biomedical applications, such as programmed drug delivery, provided the Janus particles can enter cells to carry out their functions.  However, effects of the two-faced feature on cellular entry is not well understood.  The goals of this CAREER project is to elucidate physical principles governing the entry of Janus particles into cells. Results from this project will provide guidance on how to use surface anisotropy to control the entry of synthetic particles into cells, which has the potential to transform the way multifunctional drug delivery particles are designed. The project will also promote interdisciplinary sciences and education at the interface of chemistry, engineering and biology. A Biomaterials Ambassadors outreach program will be established to bring interdisciplinary sciences to K-12 students and teachers in rural communities in Indiana.  This new program will help address the urgent need of improving education in hard-to-reach areas. Visual methods of learning will also be developed to foster critical thinking in undergraduate education. \n\nThis CAREER project will focus on ligand-guided cell entry of Janus microparticles. The hypothesis underlying the project is that the surface anisotropy of Janus particles alters the mechanism of cellular entry by changing the interplay between competing forces. To test this hypothesis, dynamics of Janus spheres will be measured with a new single-particle tracking method, which will reveal particle-cell interactions in multi-dimensions. Cellular dynamics, including membrane deformation and actin protrusion, will be measured with high-resolution fluorescence microscopy. The competition between surface anisotropy and shape in controlling the cellular entry pathways will also be investigated with non-spherical Janus particles. The results will establish a direct and quantitative connection between the particle-level anisotropies and the cell-level responses with unprecedented spatial and temporal resolution."], "researcher_paper_abstract_in_json_file": "Monitoring of environmental phenomena with embedded networked sensing confronts the challenges of both unpredictable variability in the spatial distribution of phenomena, coupled with demands for a high spatial sampling rate in three dimensions. For example, low distortion mapping of critical solar radiation properties in forest environments may require two-dimensional spatial sampling rates of greater than 10 samples/m2 over transects exceeding 1000 m2 . Clearly, adequate sampling coverage of such a transect requires an impractically large number of sensing nodes. A new approach, Networked Infomechanical System (NIMS), has been introduced to combine autonomous-articulated and static sensor nodes enabling sufficient spatiotemporal sampling density over large transects to meet a general set of environmental mapping demands. This paper describes our work on a critical parts of NIMS, the Task Allocation module. We present our methodologies and the two basic greedy Task Allocation policies - based on time of the task arrival (Time policy) and distance from the robot to the task (Distance policy). We present results from NIMS deployed in a forest reserve and from a lab testbed. The results show that both policies are adequate for the task of spatiotemporal sampling, but also complement each other. Finally, we suggest the future direction of research that would both help us better quantify the performance of our system and create more complex policies combining time, distance, information gain, etc.", "paper_keywords": "", "score_lsi": ["0.317746"]}
{"researcher_id": [3251], "researcher_name_in_nsf_list": "Shuo  Wang", "researcher_paper_title_in_json_file": "CloudCraft: Cloud-based Data Management for MMORPGs", "projects_cnt": 1, "year": 2014, "paper_citation": 0, "score_lda": ["0.801329"], "field": ["Simulation", "Computer Science", "Data mining", "Distributed computing"], "researcher_nsf_project_abstract": ["When electrical current flows through conventional transformer windings, the current is not evenly distributed within the windings. High winding power losses are generated where the current has high concentration and the winding is not fully utilized where the current has low concentration. High current concentration generates high winding power loss which leads to reduced energy conversion efficiency and high temperature. High temperature reduces transformer lifetime, so more windings must be used in transformer design to reduce power loss. As a result, transformers are bulky and heavy which increases transformer cost and cannot meet today's demand for highly compact designs. This research pursues a fundamental solution to this problem. It has been preliminarily discovered by the investigator that the current distribution within transformer windings always reaches a minimum magnetic energy state. Based on this discovery, a novel transformer winding design technique is being developed using the relationship between the minimum magnetic energy state and power loss in the windings. The technique can make current much more evenly distributed within transformer windings than is possible with existing techniques. The technique can greatly reduce transformer winding power loss, improve transformer energy efficiency, improve transformer reliability, reduce transformer cost and improve transformer power density. The technique can also be applied to other magnetic components. Transformers and magnetic components are widely used in many electronics and electrical application areas such as consumer electronics, industry products and transportation systems. Thus this technique is expected to bring significant scientific and economic impacts to society. The education plan educates electrical engineering students and promotes diversity.\n\n\nThe goal of this research is to develop a novel transformer winding design technique to minimize power loss in transformer windings.   The researchers will investigate the minimum magnetic energy state in transformer windings, explore the relationship between the minimum magnetic energy state and winding current distribution and develop a technique to find desired winding current or magnetomotive force (MMF) profiles to minimize transformer winding power loss.  The relationship between the winding power loss and magnetic energy windings will be first investigated. The PI has previously found that skin and proximity effects which influence the current distribution within winding conductors and the current sharing among parallel winding conductors comply with the minimum magnetic energy theory. The relationship between the minimum magnetic energy state and the winding current distribution or sharing in high frequency transformer windings will be investigated. A technique to achieve the minimum winding power loss by minimizing the magnetic energy among windings will be explored. A method to solve for the desired MMF profile using the minimum magnetic energy model will be developed. A technique to implement the desired MMF to either series or parallel winding turns will be studied. The theory and techniques to be developed in the research activity can be applied to any transformers and to magnetic component design in many applications. Compared with conventional extensive finite element analysis (FEA) or experiment-based trial-and-error winding design methods, the research can reveal the fundamentals of the winding current distribution and sharing in high frequency transformers. The method should be capable of quickly and directly find the best transformer winding design with the lowest winding power loss. The method should also provide useful guidance in transformer winding design. The methodology will not only help designers understand the current distribution and sharing within transformer windings but also give designers capability to steer the currents within transformer windings based on the desired current sharing profiles. With this technique, the transformer's power losses in windings can be greatly reduced and its power density can be greatly improved. Preliminary research shows that compared with conventional fully interleaved winding structure, 33-41.5% winding power loss reduction should be achievable using the theory and techniques to be developed."], "researcher_paper_abstract_in_json_file": "Massively Multiplayer Online Role-Playing Games (MMORPGs) are very sophisticated applications, which have significantly grown in popularity since their early days in the mid-90s. Along with growing numbers of users the require- ments on these systems have reached a point where technical problems become a severe risk for the commercial success. Within the CloudCraft project we investi- gate how Cloud-based architectures and data management can help to solve some of the most critical problems regarding scalability and consistency. In this article, we describe an implemented working environment based on the Cassandra DBMS and some of the key findings outlining its advantages and shortcomings for the given application scenario.", "paper_keywords": "", "score_lsi": ["0.329897"]}
{"researcher_id": [3833], "researcher_name_in_nsf_list": "Panadda  Marayong", "researcher_paper_title_in_json_file": "Spatial motion constraints: theory and demonstrations for robot guidance using virtual fixtures", "projects_cnt": 1, "year": 2003, "paper_citation": 120, "score_lda": ["0.57353"], "field": ["Telerobotics", "Control engineering", "Motion control", "Computer vision", "Simulation", "Computer Science", "Engineering", "Artificial intelligence", "Virtual reality"], "researcher_nsf_project_abstract": ["This Major Research Instrumentation Award supports the acquisition of an immersive virtual reality (VR) environment with synchronized full-body motion tracking system to facilitate interdisciplinary research in human-machine interactions and enhanced teaching and student research training. This room-size virtual reality system provides multiple computer-generated displays that allow users to fully immerse and collaboratively interact with the simulated environment in real-time while the motion tracking system captures the user's physical movements. The VR system will enable several research studies across multiple disciplines to gain a better understanding of human-machine interactions and human motor skills and learning. This will lead to the development of technologies that enhance human mobility and function and foster collaborative operations between robots and humans. The research activities envisioned to be impacted by the acquisition of this system include those in rehabilitation, sports training, advanced manufacturing, and design of human-machine interfaces such as intelligent cockpits. The activities have a strong potential for technology transfer that can directly improve the quality of life for individuals in the community. Additionally, the instrumentation will provide an attractive resource for several outreach activities to motivate students to pursue a college career in STEM disciplines. \n\nThe instrument is a turnkey, well-integrated CAVE virtual reality environment with a four-wall projection system, eight-camera real-time motion capture system with finger tracking capability, and a control software suite for customized software development and additional hardware integration.  The instrumentation will enable fundamental research activities in several key areas.  Researchers will investigate interaction models between humans and robots in a dynamic environment to develop effective control strategies for human-robot collaboration.  Additional projects will study human perception, biomechanics of human movement under various stimuli, and development of assistive technologies and training protocols to enhance motor skills and learning. The instrumentation will also enable researchers to develop models and evaluation metrics for human interactions in complex environments that include multisensory feedback, perturbations, advanced modes of display, and multiple users. The VR system provides a cost-effective design solution that can be adapted, utilized, and evaluated to fit the needs of different projects that require a rich visual display that captures the dynamic variation in the real world."], "researcher_paper_abstract_in_json_file": "In this article, we describe and demonstrate control algorithms for general motion constraints. These constraints are designed to enhance the accuracy and speed of a user manipulating in an environment with the assistance of a cooperative or telerobotic system. Our method uses a basis of preferred directions, created off-line or in real-time using sensor data, to generate virtual fixtures that may constrain the user to a curve, surface, orientation, etc. in space. Open loop virtual fixtures seek only to maintain user motion along preferred directions, whereas closed loop fixtures additionally guide the user toward a point, line, or surface. This article demonstrates and compares the effects of open and closed loop fixtures in both autonomous and human-machine cases.", "paper_keywords": ["manipulators", "control algorithm", "motion control", "human machines spatial motion constraints robot guidance virtual fixtures telerobotic system cooperative systems closed loop fixtures open loop fixtures user manipulation compliance control autonomous machines", "real time", "virtual reality", "compliance control", "medical robotics", "cooperative systems", "surgery", "telerobotics", "compliance control motion control virtual reality medical robotics surgery telerobotics cooperative systems manipulators", "constraint theory fixtures surgery robot sensing systems surges humans microsurgery retina veins admittance"], "score_lsi": ["0.550898"]}
